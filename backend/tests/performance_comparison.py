#!/usr/bin/env python3
"""
æ€§èƒ½å¯¹æ¯”æµ‹è¯•è„šæœ¬

å¯¹æ¯” Tier6+Model å’Œ DS_TPU çš„è¯„ä¼°æ€§èƒ½
ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹é…ç½®ã€éƒ¨ç½²é…ç½®å’Œç¡¬ä»¶é…ç½®
"""

import sys
import time
import json
from pathlib import Path

# æ·»åŠ è·¯å¾„
tier6_backend = Path(__file__).parent.parent
ds_tpu_root = Path("/Users/lixiang/Documents/å·¥ä½œ/code/DS_TPU_1209")

sys.path.insert(0, str(tier6_backend))
sys.path.insert(0, str(ds_tpu_root))


def load_ds_tpu_config():
    """åŠ è½½ DS_TPU çš„æ¨¡å‹é…ç½®"""
    from config.config_loader import load_model_config

    model_config = load_model_config("deepseek-v3.2")
    return model_config


def create_tier6_config(ds_model_config, deployment_config):
    """å°† DS_TPU é…ç½®è½¬æ¢ä¸º Tier6+Model æ ¼å¼"""

    # æ¨¡å‹é…ç½®
    model_dict = {
        "model_name": ds_model_config.get("name", "DeepSeek-V3.2"),
        "model_type": "moe",  # DS-V3.2 æ˜¯ MoE æ¨¡å‹
        "hidden_size": ds_model_config["hidden_dim"],
        "num_layers": ds_model_config["n_layers"],
        "num_attention_heads": ds_model_config["n_heads"],
        "num_kv_heads": ds_model_config.get("n_kv_heads", ds_model_config["n_heads"]),
        "intermediate_size": ds_model_config["inter_dim"],
        "vocab_size": ds_model_config.get("vocab_size", 32000),
        "dtype": "bf16",
        "max_seq_length": 8192,
        "attention_type": "mla",
        "mla_config": {
            "qk_nope_head_dim": ds_model_config["qk_nope_head_dim"],
            "qk_rope_head_dim": ds_model_config["qk_rope_head_dim"],
            "v_head_dim": ds_model_config["v_head_dim"],
            "kv_lora_rank": ds_model_config["kv_lora_rank"],
            "q_lora_rank": ds_model_config["q_lora_rank"],
            "variant": "mla_absorb_v32",  # DeepSeek V3.2 ä½¿ç”¨ absorb ä¼˜åŒ–
        },
        "moe_config": {
            "num_experts": ds_model_config["n_routed_experts"],
            "num_experts_per_tok": ds_model_config["n_activated_experts"],
            "num_shared_experts": ds_model_config.get("n_shared_experts", 0),
            "expert_intermediate_size": ds_model_config["moe_inter_dim"],
            "first_k_dense_replace": ds_model_config.get("n_dense_layers", 0),
        }
    }

    # æ¨ç†é…ç½®
    # æ³¨æ„ï¼šTier6 ä¼šå…ˆ Prefill (input_seq_length ä¸ª token)ï¼Œå† Decode (output_seq_length ä¸ª token)
    # ä¸ºäº†å¯¹é½ DS_TPU çš„ Decode æ¨¡å¼ (context=8192)ï¼Œè®¾ç½®ï¼š
    inference_dict = {
        "batch_size": deployment_config["batch_size"],
        "input_seq_length": deployment_config["kv_len"],  # Prefill å¤„ç† 8192 ä¸ª tokenï¼ˆå»ºç«‹ KV cacheï¼‰
        "output_seq_length": 1,  # Decode ç”Ÿæˆ 1 ä¸ª token
        "max_seq_length": deployment_config["kv_len"],
    }

    # å¹¶è¡Œç­–ç•¥
    parallelism_dict = {
        "dp": deployment_config["dp"],
        "tp": deployment_config["tp"],
        "pp": 1,  # DS_TPU é»˜è®¤ä¸ç”¨ PP
        "ep": deployment_config["ep"],
        "sp": 1,
    }

    # ç¡¬ä»¶é…ç½®ï¼ˆä½¿ç”¨ SG2260E å‚æ•°ï¼‰
    hardware_dict = {
        "chip": {
            "chip_type": "SG2260E",
            "compute_tflops_fp16": 64,
            "memory_gb": 64,
            "memory_bandwidth_gbps": 273,
            "num_cores": deployment_config["tpu_cores"],
        },
        "node": {
            "chips_per_node": 8,
            "intra_node_bandwidth_gbps": 64,
            "intra_node_latency_us": 0.35,
        },
        "cluster": {
            "num_nodes": 1,
            "inter_node_bandwidth_gbps": 16,
            "inter_node_latency_us": 2,
        }
    }

    # æ‹“æ‰‘é…ç½®ï¼ˆç®€å•çš„å•èŠ‚ç‚¹æ‹“æ‰‘ï¼‰
    total_chips = deployment_config["dp"] * deployment_config["tp"] * deployment_config["ep"]
    topology_dict = {
        "pods": [
            {
                "id": "pod_0",
                "racks": [
                    {
                        "id": "rack_0",
                        "boards": [
                            {
                                "id": f"board_{i}",
                                "chips": [
                                    {
                                        "id": f"chip_{i * 8 + j}",
                                        "name": "SG2260E",
                                    }
                                    for j in range(min(8, total_chips - i * 8))
                                ]
                            }
                            for i in range((total_chips + 7) // 8)
                        ]
                    }
                ]
            }
        ],
        "connections": []
    }

    return topology_dict, model_dict, inference_dict, parallelism_dict, hardware_dict


def run_ds_tpu_benchmark(model_config, deployment_config, tpu_kwargs):
    """è¿è¡Œ DS_TPU è¯„ä¼°å¹¶è®¡æ—¶"""
    print("\n" + "="*80)
    print("ğŸš€ DS_TPU è¯„ä¼°å¼€å§‹")
    print("="*80)

    from top.simulator import TPUSimulator
    from config.deployment_config import DeploymentConfig

    # åˆ›å»ºéƒ¨ç½²é…ç½®å¯¹è±¡
    deploy_cfg = DeploymentConfig(
        batch_size=deployment_config["batch_size"],
        q_seq_len=deployment_config["q_len"],
        kv_seq_len=deployment_config["kv_len"],
        tp=deployment_config["tp"],
        dp=deployment_config["dp"],
        moe_tp=deployment_config["moe_tp"],
        ep=deployment_config["ep"],
        is_prefill=deployment_config["is_prefill"],
        enable_tp_sp=deployment_config["enable_tp_sp"],
        comm_protocol=deployment_config["comm_protocol"],
    )

    # è®¡æ—¶å¼€å§‹
    start_time = time.time()

    # è¿è¡Œæ¨¡æ‹Ÿ
    global_cache = {}
    simulator = TPUSimulator()
    results = simulator.run_simulation(
        model_cfg=model_config,
        tpu_kwargs=tpu_kwargs,
        deploy_cfg=deploy_cfg,
        model_version="v3.2",
        global_cache=global_cache
    )

    # è®¡æ—¶ç»“æŸ
    elapsed = time.time() - start_time

    print(f"\nâœ… DS_TPU è¯„ä¼°å®Œæˆ")
    print(f"â±ï¸  æ€»è€—æ—¶: {elapsed:.3f}s")

    perf = results.get("performance", {})
    print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    print(f"   - æ‰§è¡Œæ—¶é—´: {perf.get('total_elapse_us', 0):.2f} Î¼s")
    print(f"   - ååé‡: {perf.get('tps', 0):.2f} tokens/s")
    print(f"   - MFU: {perf.get('mfu', 0)*100:.2f}%")

    return elapsed, results


def run_tier6_benchmark(topology_dict, model_dict, inference_dict, parallelism_dict, hardware_dict):
    """è¿è¡Œ Tier6+Model è¯„ä¼°å¹¶è®¡æ—¶"""
    print("\n" + "="*80)
    print("ğŸš€ Tier6+Model è¯„ä¼°å¼€å§‹")
    print("="*80)

    from llm_simulator.core.simulator import run_simulation

    # è®¡æ—¶å¼€å§‹
    start_time = time.time()

    # è¿è¡Œæ¨¡æ‹Ÿï¼ˆå‚æ•°å¯¹é½ DS_TPUï¼‰
    results = run_simulation(
        topology_dict=topology_dict,
        model_dict=model_dict,
        inference_dict=inference_dict,
        parallelism_dict=parallelism_dict,
        hardware_dict=hardware_dict,
        enable_tile_search=True,  # âœ… å¯¹é½ DS_TPUï¼šå¼€å¯ tile æœç´¢
        enable_partition_search=True,  # âœ… å¯¹é½ DS_TPUï¼šå¼€å¯åˆ†åŒºæœç´¢
        max_simulated_tokens=1,  # âœ… å¯¹é½ DS_TPUï¼šåªæ¨¡æ‹Ÿ1ä¸ª decode token
    )

    # è®¡æ—¶ç»“æŸ
    elapsed = time.time() - start_time

    print(f"\nâœ… Tier6+Model è¯„ä¼°å®Œæˆ")
    print(f"â±ï¸  æ€»è€—æ—¶: {elapsed:.3f}s")

    stats = results.get("stats", {})
    print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    print(f"   - TTFT: {stats.get('ttft', 0):.2f} ms")
    print(f"   - Avg TPOT: {stats.get('avgTpot', 0):.2f} ms")
    print(f"   - MFU: {stats.get('dynamicMfu', 0)*100:.2f}%")

    return elapsed, results


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸ”¬ Tier6+Model vs DS_TPU æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*80)

    # é…ç½®å‚æ•°ï¼ˆå¯¹é½DS_TPUçš„é»˜è®¤é…ç½®ï¼‰
    deployment_config = {
        "batch_size": 48 * 32,
        "q_len": 1,  # Decode é˜¶æ®µ
        "kv_len": 8192,
        "tp": 1,
        "dp": 32,
        "moe_tp": 1,
        "ep": 32,
        "is_prefill": False,
        "enable_tp_sp": True,
        "comm_protocol": 1,
        "tpu_cores": 64,
    }

    tpu_kwargs = {"core": deployment_config["tpu_cores"]}

    print("\nğŸ“‹ é…ç½®å‚æ•°:")
    print(f"   - Batch Size: {deployment_config['batch_size']}")
    print(f"   - Seq Len: {deployment_config['q_len']} (q) / {deployment_config['kv_len']} (kv)")
    print(f"   - å¹¶è¡Œåº¦: TP={deployment_config['tp']}, DP={deployment_config['dp']}, EP={deployment_config['ep']}")
    print(f"   - TPU Cores: {deployment_config['tpu_cores']}")
    print(f"   - Prefill: {deployment_config['is_prefill']}")

    # åŠ è½½ DS_TPU é…ç½®
    print("\nğŸ“¥ åŠ è½½ DS_TPU æ¨¡å‹é…ç½®...")
    ds_model_config = load_ds_tpu_config()

    # è½¬æ¢ä¸º Tier6 é…ç½®
    print("ğŸ”„ è½¬æ¢ä¸º Tier6+Model é…ç½®æ ¼å¼...")
    topology_dict, model_dict, inference_dict, parallelism_dict, hardware_dict = create_tier6_config(
        ds_model_config, deployment_config
    )

    # è¿è¡Œ DS_TPU åŸºå‡†æµ‹è¯•
    try:
        ds_time, ds_results = run_ds_tpu_benchmark(ds_model_config, deployment_config, tpu_kwargs)
    except Exception as e:
        print(f"\nâŒ DS_TPU è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        ds_time = None
        ds_results = None

    # è¿è¡Œ Tier6+Model åŸºå‡†æµ‹è¯•
    try:
        tier6_time, tier6_results = run_tier6_benchmark(
            topology_dict, model_dict, inference_dict, parallelism_dict, hardware_dict
        )
    except Exception as e:
        print(f"\nâŒ Tier6+Model è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        tier6_time = None
        tier6_results = None

    # å¯¹æ¯”ç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("="*80)

    if ds_time and tier6_time:
        print(f"\nâ±ï¸  è€—æ—¶å¯¹æ¯”:")
        print(f"   DS_TPU:        {ds_time:.3f}s")
        print(f"   Tier6+Model:   {tier6_time:.3f}s")
        print(f"   å·®è·:          {tier6_time - ds_time:.3f}s ({tier6_time/ds_time:.2f}x)")

        if tier6_time > ds_time:
            print(f"\nâš ï¸  Tier6+Model æ¯” DS_TPU æ…¢ {(tier6_time/ds_time - 1)*100:.1f}%")
        else:
            print(f"\nâœ… Tier6+Model æ¯” DS_TPU å¿« {(1 - tier6_time/ds_time)*100:.1f}%")

    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_dir = Path(__file__).parent / "comparison_results"
    output_dir.mkdir(exist_ok=True)

    if ds_results:
        ds_output = output_dir / "ds_tpu_result.json"
        with open(ds_output, "w") as f:
            json.dump(ds_results, f, indent=2)
        print(f"\nğŸ’¾ DS_TPU ç»“æœå·²ä¿å­˜: {ds_output}")

    if tier6_results:
        tier6_output = output_dir / "tier6_result.json"
        with open(tier6_output, "w") as f:
            json.dump(tier6_results, f, indent=2)
        print(f"ğŸ’¾ Tier6+Model ç»“æœå·²ä¿å­˜: {tier6_output}")


if __name__ == "__main__":
    main()
