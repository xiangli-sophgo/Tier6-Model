#!/usr/bin/env python3
"""
çº¯ç®—å­è¯„ä¼°å¯¹æ¯”ï¼ˆä¸è¿è¡Œå®Œæ•´æ¨¡æ‹Ÿï¼‰

åªå¯¹æ¯”ï¼šåˆ›å»ºæ¨¡å‹ + è¯„ä¼°æ‰€æœ‰ç®—å­çš„æ—¶é—´
"""

import sys
import time
from pathlib import Path

tier6_backend = Path(__file__).parent.parent
ds_tpu_root = Path("/Users/lixiang/Documents/å·¥ä½œ/code/DS_TPU_1209")

sys.path.insert(0, str(tier6_backend))
sys.path.insert(0, str(ds_tpu_root))


def test_ds_tpu_evaluation_only():
    """DS_TPU: çº¯ç®—å­è¯„ä¼°"""
    from model.model_factories import model_factory
    from tpu.tpu_factories import tpu_factory
    from performance.analyzer import PerformanceAnalyzer
    from config.deployment_config import DeploymentConfig
    from config.config_loader import load_model_config

    print("\n" + "="*80)
    print("DS_TPU - çº¯ç®—å­è¯„ä¼°")
    print("="*80)

    model_cfg = load_model_config("deepseek-v3.2")
    deploy_cfg = DeploymentConfig(
        batch_size=1536, q_seq_len=1, kv_seq_len=8192,
        tp=1, dp=32, moe_tp=1, ep=32, is_prefill=False,
    )

    # è®¡æ—¶å¼€å§‹
    total_start = time.time()

    # åˆ›å»ºæ¨¡å‹
    model_start = time.time()
    model = model_factory.create_model(model_cfg, deploy_cfg.__dict__, "DeepSeek-V3.2")
    model_time = time.time() - model_start

    # åˆ›å»º TPU
    tpu = tpu_factory.create_tpu('v1', {'core': 64})

    # è¯„ä¼°ï¼ˆPerformanceAnalyzer ä¼šè‡ªåŠ¨è¯„ä¼°æ‰€æœ‰ç®—å­ï¼‰
    eval_start = time.time()
    analyzer = PerformanceAnalyzer(model, tpu, deploy_cfg, {})
    eval_time = time.time() - eval_start

    total_time = time.time() - total_start

    print(f"\nâ±ï¸  æ—¶é—´åˆ†è§£:")
    print(f"   æ¨¡å‹åˆ›å»º: {model_time*1000:.2f}ms")
    print(f"   ç®—å­è¯„ä¼°: {eval_time*1000:.2f}ms")
    print(f"   æ€»è€—æ—¶: {total_time*1000:.2f}ms ({total_time:.2f}s)")

    return {
        "model_time_s": model_time,
        "eval_time_s": eval_time,
        "total_time_s": total_time,
        "num_layers": len(model.layers),
    }


def test_tier6_evaluation_only():
    """Tier6: çº¯ç®—å­è¯„ä¼°ï¼ˆ61å±‚ï¼‰"""
    from llm_simulator.layers import MLAAbsorbv32Layer, MoELayer, MLPLayer
    from llm_simulator.evaluators import get_arch_preset, create_gemm_evaluator

    print("\n" + "="*80)
    print("Tier6+Model - çº¯ç®—å­è¯„ä¼° (61å±‚)")
    print("="*80)

    # é…ç½®
    mla_cfg = {
        'hidden_dim': 7168, 'num_heads': 128,
        'qk_nope_dim': 128, 'qk_rope_dim': 64, 'v_head_dim': 128,
        'kv_lora_rank': 512, 'q_lora_rank': 1536,
        'batch_size': 1536, 'seq_len': 1, 'kv_seq_len': 8192,
        'tp': 1, 'comm_protocol': 1,
    }

    dense_mlp_cfg = {
        'hidden_dim': 7168,
        'inter_dim': 18432,
        'batch_size': 1536,
        'seq_len': 1,
        'tp': 1,
        'comm_protocol': 1,
    }

    moe_cfg = {
        'hidden_dim': 7168,
        'inter_dim': 18432,
        'num_experts': 256,
        'num_experts_per_tok': 8,
        'expert_intermediate_size': 2048,
        'batch_size': 1536,
        'seq_len': 1,
        'tp': 1,
        'comm_protocol': 1,
    }

    # åˆ›å»ºè¯„ä¼°å™¨
    arch = get_arch_preset("SG2260E")
    gemm_eval = create_gemm_evaluator(arch, fast_mode=False, enable_partition_search=True)

    total_start = time.time()

    # åˆ›å»ºå±‚ï¼ˆ61å±‚ï¼š3 Dense + 58 MoEï¼Œéƒ½æœ‰ MLAï¼‰
    create_start = time.time()
    layers = []

    # å‰3å±‚ï¼šDense MLP + MLA
    for i in range(3):
        mla = MLAAbsorbv32Layer(f"mla_{i}", mla_cfg)
        mlp = MLPLayer(f"dense_mlp_{i}", dense_mlp_cfg)
        layers.append((mla, mlp))

    # å58å±‚ï¼šMoE + MLA
    for i in range(3, 61):
        mla = MLAAbsorbv32Layer(f"mla_{i}", mla_cfg)
        moe = MoELayer(f"moe_{i}", moe_cfg)
        layers.append((mla, moe))

    create_time = time.time() - create_start

    # è¯„ä¼°æ‰€æœ‰å±‚
    eval_start = time.time()
    for i, (attn, ffn) in enumerate(layers):
        # è¯„ä¼° Attention
        for op in attn.comp_ops:
            if op.operator_type == "MatMulOperator":
                result = gemm_eval.evaluate(
                    G=op.parallel_params.get("G", 1),
                    M=op.parallel_params.get("M", 1),
                    K=op.parallel_params.get("K", 1),
                    N=op.parallel_params.get("N", 1),
                    input_dtype='fp8', output_dtype='bf16',
                    use_multiprocess=True,
                )
                op.elapse = result.latency_us

        # è¯„ä¼° FFN
        for op in ffn.comp_ops:
            if op.operator_type == "MatMulOperator":
                result = gemm_eval.evaluate(
                    G=op.parallel_params.get("G", 1),
                    M=op.parallel_params.get("M", 1),
                    K=op.parallel_params.get("K", 1),
                    N=op.parallel_params.get("N", 1),
                    input_dtype='fp8', output_dtype='bf16',
                    use_multiprocess=True,
                )
                op.elapse = result.latency_us

        # æ¯10å±‚æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
        if (i + 1) % 10 == 0:
            print(f"   å·²è¯„ä¼° {i+1}/61 å±‚...")

    eval_time = time.time() - eval_start
    total_time = time.time() - total_start

    print(f"\nâ±ï¸  æ—¶é—´åˆ†è§£:")
    print(f"   åˆ›å»º61å±‚: {create_time*1000:.2f}ms")
    print(f"   è¯„ä¼°61å±‚: {eval_time*1000:.2f}ms")
    print(f"   æ€»è€—æ—¶: {total_time*1000:.2f}ms ({total_time:.2f}s)")

    # æ‰“å°ç¼“å­˜ç»Ÿè®¡
    gemm_eval.print_cache_stats()

    return {
        "create_time_s": create_time,
        "eval_time_s": eval_time,
        "total_time_s": total_time,
        "num_layers": 61,
    }


def main():
    print("="*80)
    print("ğŸ”¬ çº¯ç®—å­è¯„ä¼°æ€§èƒ½å¯¹æ¯”ï¼ˆä¸å«å®Œæ•´æ¨¡æ‹Ÿï¼‰")
    print("="*80)

    # æµ‹è¯• DS_TPU
    try:
        ds_result = test_ds_tpu_evaluation_only()
    except Exception as e:
        print(f"\nâŒ DS_TPU å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        ds_result = None

    # æµ‹è¯• Tier6
    try:
        tier6_result = test_tier6_evaluation_only()
    except Exception as e:
        print(f"\nâŒ Tier6 å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        tier6_result = None

    # å¯¹æ¯”
    if ds_result and tier6_result:
        print("\n" + "="*80)
        print("ğŸ“Š å¯¹æ¯”æ€»ç»“")
        print("="*80)

        print(f"\nã€DS_TPUã€‘")
        print(f"  æ¨¡å‹åˆ›å»º: {ds_result['model_time_s']:.2f}s")
        print(f"  ç®—å­è¯„ä¼°: {ds_result['eval_time_s']:.2f}s")
        print(f"  æ€»è€—æ—¶: {ds_result['total_time_s']:.2f}s")
        print(f"  å±‚æ•°: {ds_result['num_layers']}")

        print(f"\nã€Tier6ã€‘")
        print(f"  åˆ›å»º61å±‚: {tier6_result['create_time_s']:.2f}s")
        print(f"  è¯„ä¼°61å±‚: {tier6_result['eval_time_s']:.2f}s")
        print(f"  æ€»è€—æ—¶: {tier6_result['total_time_s']:.2f}s")
        print(f"  å±‚æ•°: {tier6_result['num_layers']}")

        print(f"\nã€é€Ÿåº¦æ¯”ã€‘")
        speedup = tier6_result['total_time_s'] / ds_result['total_time_s']
        print(f"  {speedup:.2f}x {'(Tier6æ…¢)' if speedup > 1 else '(Tier6å¿«)'}")

        print(f"\nã€ç»“è®ºã€‘")
        if speedup > 10:
            print(f"  âš ï¸  Tier6 æ¯” DS_TPU æ…¢ {speedup:.1f}å€")
            print(f"  ä¸»è¦ç“¶é¢ˆåœ¨ç®—å­è¯„ä¼°é˜¶æ®µ")
        elif speedup > 1:
            print(f"  âš ï¸  Tier6 æ¯” DS_TPU æ…¢ {(speedup-1)*100:.0f}%")
            print(f"  ç“¶é¢ˆåœ¨ç®—å­è¯„ä¼°é˜¶æ®µ")
        else:
            print(f"  âœ… Tier6 æ¯” DS_TPU å¿«")
            print(f"  ç“¶é¢ˆä¸åœ¨ç®—å­è¯„ä¼°ï¼Œè€Œåœ¨æ¨¡æ‹Ÿé€»è¾‘å…¶ä»–éƒ¨åˆ†")


if __name__ == "__main__":
    main()
