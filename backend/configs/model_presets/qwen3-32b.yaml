model_name: Qwen3-32B
model_type: dense
hidden_size: 5120
num_layers: 64
num_attention_heads: 64
num_kv_heads: 8
intermediate_size: 25600
vocab_size: 151936
weight_dtype: bf16
activation_dtype: bf16
max_seq_length: 131072
norm_type: rmsnorm
attention_type: gqa
