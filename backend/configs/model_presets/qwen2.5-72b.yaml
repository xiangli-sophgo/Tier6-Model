model_name: Qwen2.5-72B
model_type: dense
hidden_size: 8192
num_layers: 80
num_attention_heads: 64
num_kv_heads: 8
intermediate_size: 29568
vocab_size: 152064
weight_dtype: bf16
activation_dtype: bf16
max_seq_length: 131072
norm_type: rmsnorm
attention_type: gqa
