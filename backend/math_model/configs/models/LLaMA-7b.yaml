# LLaMA 7B 模型配置（Dense 模型）
name: "LLaMA-7B"

# 核心参数
max_seq_len: 2048
vocab_size: 32000
hidden_size: 4096
intermediate_size: 11008
num_layers: 32
num_attention_heads: 32
num_key_value_heads: 32
hidden_act: "silu"
norm_type: "rmsnorm"
rms_norm_eps: 1.0e-6

# RoPE 配置
RoPE:
  theta: 10000.0
