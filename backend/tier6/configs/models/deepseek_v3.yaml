# DeepSeek V3 模型配置
# 适用版本: V3 original, V3.2 (参数相同)

model:
  name: deepseek_v3
  type: llm
  family: deepseek
  version: v3.2  # v3 original 参数相同

  # 模型结构
  hidden_size: 7168
  num_layers: 61
  num_dense_layers: 3    # 前 3 层为 Dense
  num_moe_layers: 58     # 后 58 层为 MoE
  num_heads: 128
  vocab_size: 129280
  max_position_embeddings: 4096

  # 数据类型
  dtype: bf16

  # MoE 配置
  moe:
    num_routed_experts: 256
    num_shared_experts: 1
    num_activated_experts: 8
    num_expert_groups: 8
    num_limited_groups: 4
    intermediate_size: 2048  # 每个专家的 FFN 中间层
    route_scale: 2.5

  # MLA (Multi-head Latent Attention) 配置
  mla:
    q_lora_rank: 1536
    kv_lora_rank: 512
    qk_nope_head_dim: 128  # 不使用 RoPE 的 head 维度
    qk_rope_head_dim: 64   # 使用 RoPE 的 head 维度
    v_head_dim: 128

  # DSA (Dynamic Sparse Attention) 配置
  dsa:
    n_index_heads: 64
    index_head_dim: 128
    topk_index: 2048

  # NSA (Native Sparse Attention) 配置
  nsa:
    l: 32    # 层数相关参数
    d: 16    # 维度相关参数
    sl: 64   # 序列长度相关
    sn: 16   # 稀疏数量
    w: 512   # 窗口大小

  # RoPE (YARN 变体)
  rope:
    theta: 10000.0
    original_seq_len: 4096
    factor: 40
    beta_fast: 32
    beta_slow: 1
    mscale: 1.0

  # FFN Dense 层配置 (用于前 3 层)
  ffn:
    intermediate_size: 18432

# 运行时配置 (可被 scenario 覆盖)
runtime:
  max_batch_size: 8
  max_seq_len: 4096

# 派生参数（供参考）
# params_b: ~671  # 约 6710 亿参数 (含 MoE)
# active_params_b: ~37  # 激活参数约 370 亿
# weight_size_gb: ~1250  # bf16 (含所有专家)
