"""
éƒ¨ç½²æ–¹æ¡ˆè¯„ä¼°é€‚é…å™¨

å°†ç°æœ‰çš„æ¨¡æ‹Ÿå™¨é€»è¾‘åŒ…è£…æˆä»»åŠ¡ç®¡ç†å™¨éœ€è¦çš„æ¥å£
"""

import logging
import threading
from multiprocessing import cpu_count
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any, Optional, Callable, List
from ..core.simulator import run_simulation
from ..core.analyzer import PerformanceAnalyzer
from ..evaluators import get_arch_preset
from ..evaluators.cost_evaluator import evaluate_deployment_cost
from ..models import create_deepseek_v3, create_deepseek_v3_absorb, create_deepseek_v32

logger = logging.getLogger(__name__)


class TaskCancelledException(Exception):
    """ä»»åŠ¡å–æ¶ˆå¼‚å¸¸"""
    pass


# ============================================
# èŠ¯ç‰‡æ•°é‡è®¡ç®—å…¬å…±å‡½æ•°
# ============================================

def count_topology_chips(topology: dict) -> int:
    """ç»Ÿè®¡æ‹“æ‰‘ä¸­çš„èŠ¯ç‰‡æ€»æ•°

    Args:
        topology: grouped_pods æ ¼å¼æ‹“æ‰‘é…ç½®

    Returns:
        èŠ¯ç‰‡æ€»æ•°
    """
    from math_model.L0_entry.topology_format import count_chips
    return count_chips(topology)


def calculate_required_chips(parallelism: dict, model_config: dict) -> int:
    """è®¡ç®—å¹¶è¡Œç­–ç•¥æ‰€éœ€çš„èŠ¯ç‰‡æ•°

    Args:
        parallelism: å¹¶è¡Œç­–ç•¥é…ç½® (dp, tp, ep, ...)
        model_config: æ¨¡å‹é…ç½®

    Returns:
        æ‰€éœ€èŠ¯ç‰‡æ•°

    Note:
        - MoE æ¨¡å‹ï¼šDP Ã— TPï¼ˆå› ä¸º MoE çº¦æŸ DPÃ—TP = MoE_TPÃ—EPï¼‰
        - é MoE æ¨¡å‹ï¼šDP Ã— TP Ã— EP
        - PP æš‚æœªå¯ç”¨ï¼Œä¸å‚ä¸è®¡ç®—
    """
    dp = parallelism.get("dp", 1)
    tp = parallelism.get("tp", 1)
    ep = parallelism.get("ep", 1)

    is_moe = model_config.get("moe_config") is not None
    if is_moe:
        return dp * tp
    else:
        return dp * tp * ep


def evaluate_deployment(
    topology: dict,
    model_config: dict,
    inference_config: dict,
    search_mode: str,
    manual_parallelism: Optional[dict] = None,
    search_constraints: Optional[dict] = None,
    progress_callback: Optional[Callable[[int, int, str], None]] = None,
    result_callback: Optional[Callable[[dict], None]] = None,
    cancel_check: Optional[Callable[[], bool]] = None,
    enable_tile_search: bool = True,
    enable_partition_search: bool = False,
    max_simulated_tokens: int = 4,
    max_workers: int = 1,
) -> dict:
    """
    è¯„ä¼°éƒ¨ç½²æ–¹æ¡ˆ

    Args:
        topology: å®Œæ•´æ‹“æ‰‘é…ç½®ï¼ˆåŒ…å« pods/racks/boards/chips/connections + protocol_config + network_config + chip_latency_configï¼‰
        model_config: æ¨¡å‹é…ç½®
        inference_config: æ¨ç†é…ç½®
        search_mode: 'manual' æˆ– 'auto'
        manual_parallelism: æ‰‹åŠ¨æ¨¡å¼çš„å¹¶è¡Œç­–ç•¥
        search_constraints: è‡ªåŠ¨æ¨¡å¼çš„æœç´¢çº¦æŸ
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, message)
        result_callback: ç»“æœå›è°ƒå‡½æ•° (è‡ªåŠ¨æ¨¡å¼ä¸‹è¾¹è¯„ä¼°è¾¹ä¿å­˜)
        cancel_check: å–æ¶ˆæ£€æŸ¥å‡½æ•°ï¼Œè¿”å› True è¡¨ç¤ºä»»åŠ¡è¢«å–æ¶ˆ

    Returns:
        è¯„ä¼°ç»“æœå­—å…¸ï¼ŒåŒ…å«:
        - top_k_plans: å¯è¡Œæ–¹æ¡ˆåˆ—è¡¨
        - infeasible_plans: ä¸å¯è¡Œæ–¹æ¡ˆåˆ—è¡¨
        - search_stats: æœç´¢ç»Ÿè®¡

    Raises:
        TaskCancelledException: ä»»åŠ¡è¢«å–æ¶ˆæ—¶æŠ›å‡º
    """

    # è®¡ç®—æ‹“æ‰‘ä¸­çš„æ€»èŠ¯ç‰‡æ•°
    total_chips = count_topology_chips(topology)

    logger.info(f"å¼€å§‹è¯„ä¼°: mode={search_mode}, chips={total_chips}")

    if search_mode == 'manual':
        return _evaluate_manual_mode(
            topology,
            model_config,
            inference_config,
            manual_parallelism,
            total_chips,
            progress_callback,
            cancel_check,
            enable_tile_search,
            enable_partition_search,
            max_simulated_tokens
        )
    else:
        return _evaluate_auto_mode(
            topology,
            model_config,
            inference_config,
            search_constraints,
            total_chips,
            progress_callback,
            result_callback,
            cancel_check,
            enable_tile_search,
            enable_partition_search,
            max_simulated_tokens,
            max_workers
        )


def _evaluate_manual_mode(
    topology: dict,
    model_config: dict,
    inference_config: dict,
    manual_parallelism: dict,
    total_chips: int,
    progress_callback: Optional[Callable[[int, int, str], None]] = None,
    cancel_check: Optional[Callable[[], bool]] = None,
    enable_tile_search: bool = True,
    enable_partition_search: bool = False,
    max_simulated_tokens: int = 4,
) -> dict:
    """æ‰‹åŠ¨æ¨¡å¼è¯„ä¼°ï¼ˆå¸¦ç»†ç²’åº¦è¿›åº¦ï¼‰"""

    # æ£€æŸ¥å–æ¶ˆ
    if cancel_check and cancel_check():
        logger.info("ä»»åŠ¡åœ¨å¼€å§‹å‰è¢«å–æ¶ˆ")
        raise TaskCancelledException("ä»»åŠ¡å·²å–æ¶ˆ")

    if progress_callback:
        progress_callback(0, 100, "å¼€å§‹æ‰‹åŠ¨æ¨¡å¼è¯„ä¼°...")

    # æå–å¹¶è¡Œç­–ç•¥ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼šä¸ä½¿ç”¨é»˜è®¤å€¼ï¼‰
    required_parallelism_fields = ["dp", "tp", "pp", "ep"]
    missing_fields = [f for f in required_parallelism_fields if f not in manual_parallelism]
    if missing_fields:
        raise ValueError(f"æ‰‹åŠ¨å¹¶è¡Œç­–ç•¥ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}")

    dp = manual_parallelism["dp"]
    tp = manual_parallelism["tp"]
    pp = manual_parallelism["pp"]
    ep = manual_parallelism["ep"]
    sp = manual_parallelism.get("sp", 1)  # sp æ˜¯å¯é€‰çš„ï¼Œé»˜è®¤ä¸º 1

    # è®¡ç®—å®é™…ä½¿ç”¨çš„èŠ¯ç‰‡æ•°
    required_chips = calculate_required_chips(manual_parallelism, model_config)

    if progress_callback:
        progress_callback(5, 100, "æ£€æŸ¥èŠ¯ç‰‡æ•°é‡...")

    # æ£€æŸ¥èŠ¯ç‰‡æ•°é‡æ˜¯å¦è¶³å¤Ÿ
    if required_chips > total_chips:
        result = _create_infeasible_result(
            manual_parallelism,
            required_chips,
            f"éœ€è¦ {required_chips} ä¸ªèŠ¯ç‰‡ï¼Œä½†æ‹“æ‰‘ä¸­åªæœ‰ {total_chips} ä¸ª"
        )

        if progress_callback:
            progress_callback(100, 100, "è¯„ä¼°å®Œæˆï¼ˆä¸å¯è¡Œï¼‰")

        return {
            "top_k_plans": [],
            "infeasible_plans": [result],
            "search_stats": {
                "total_plans": 1,
                "feasible_plans": 0,
                "infeasible_plans": 1,
            }
        }

    # ä»æ‹“æ‰‘é…ç½®ä¸­æå–ç¡¬ä»¶é…ç½®
    if progress_callback:
        progress_callback(8, 100, "æå–ç¡¬ä»¶é…ç½®...")
    hardware_config = _extract_hardware_config(topology)

    # è°ƒç”¨çœŸå®çš„æ¨¡æ‹Ÿå™¨ï¼ˆå¸¦ç»†ç²’åº¦è¿›åº¦å›è°ƒï¼‰
    try:
        # åˆ›å»ºå†…éƒ¨è¿›åº¦å›è°ƒï¼Œå°†æ¨¡æ‹Ÿå™¨çš„ 0-100% æ˜ å°„åˆ°å¤–éƒ¨çš„ 10-95%
        def sim_progress_callback(percent: float, message: str):
            if progress_callback:
                # æ˜ å°„: æ¨¡æ‹Ÿå™¨çš„ 0-100% -> å¤–éƒ¨çš„ 10-95%
                external_progress = 10 + percent * 0.85
                progress_callback(int(external_progress), 100, message)

        if progress_callback:
            progress_callback(10, 100, "è¿è¡Œæ¨¡æ‹Ÿå™¨...")

        # æ‰‹åŠ¨æ¨¡å¼ä½¿ç”¨ä¿å®ˆçš„è¿›ç¨‹æ•°ï¼Œé¿å…å¤šä»»åŠ¡ä¸²è¡Œæ‰§è¡Œæ—¶è¿‡åº¦ç«äº‰
        # å…¬å¼: max(2, cpu_count() // 4)
        # ä¾‹å¦‚: 16æ ¸CPU -> 4è¿›ç¨‹, 8æ ¸CPU -> 2è¿›ç¨‹
        manual_gemm_processes = max(2, cpu_count() // 4)
        logger.info(f"æ‰‹åŠ¨æ¨¡å¼ GEMM è¿›ç¨‹æ•°: {manual_gemm_processes} (CPUæ ¸æ•°: {cpu_count()})")

        sim_result = run_simulation(
            topology_dict=topology,
            model_dict=model_config,
            inference_dict=inference_config,
            parallelism_dict=manual_parallelism,
            hardware_dict=hardware_config,
            progress_callback=sim_progress_callback,
            enable_tile_search=enable_tile_search,
            enable_partition_search=enable_partition_search,
            max_simulated_tokens=max_simulated_tokens,
            max_gemm_processes=manual_gemm_processes,
        )

        if progress_callback:
            progress_callback(95, 100, "è½¬æ¢ç»“æœæ ¼å¼...")

        # è½¬æ¢ä¸º DS_TPU æ ¼å¼
        result = _transform_to_ds_tpu_format(
            sim_result=sim_result,
            parallelism=manual_parallelism,
            chips=required_chips,
            model_config=model_config,
            inference_config=inference_config,
            topology=topology,
        )

        if progress_callback:
            progress_callback(100, 100, "è¯„ä¼°å®Œæˆ")

        return {
            "top_k_plans": [result],
            "infeasible_plans": [],
            "search_stats": {
                "total_plans": 1,
                "feasible_plans": 1,
                "infeasible_plans": 0,
            }
        }

    except Exception as e:
        logger.error(f"æ¨¡æ‹Ÿå™¨è¿è¡Œå¤±è´¥: {e}")
        result = _create_infeasible_result(
            manual_parallelism,
            required_chips,
            f"æ¨¡æ‹Ÿå™¨è¿è¡Œå¤±è´¥: {str(e)}"
        )
        return {
            "top_k_plans": [],
            "infeasible_plans": [result],
            "search_stats": {
                "total_plans": 1,
                "feasible_plans": 0,
                "infeasible_plans": 1,
            }
        }


def _evaluate_auto_mode(
    topology: dict,
    model_config: dict,
    inference_config: dict,
    search_constraints: dict,
    total_chips: int,
    progress_callback: Optional[Callable[[int, int, str], None]] = None,
    result_callback: Optional[Callable[[dict], None]] = None,
    cancel_check: Optional[Callable[[], bool]] = None,
    enable_tile_search: bool = True,
    enable_partition_search: bool = False,
    max_simulated_tokens: int = 4,
    max_workers: int = 1,
) -> dict:
    """è‡ªåŠ¨æ¨¡å¼è¯„ä¼°ï¼ˆæœç´¢æœ€ä¼˜æ–¹æ¡ˆï¼Œæ”¯æŒå¹¶è¡Œè¯„ä¼°ï¼‰

    è¿›åº¦è®¡ç®—æ–¹å¼ï¼š
    - æ€»è¿›åº¦ = (å·²å®Œæˆæ–¹æ¡ˆæ•° / æ€»æ–¹æ¡ˆæ•°) * 100% + (å½“å‰æ–¹æ¡ˆå†…éƒ¨è¿›åº¦ / æ€»æ–¹æ¡ˆæ•°)
    - ä¾‹å¦‚ï¼š10 ä¸ªæ–¹æ¡ˆï¼Œç¬¬ 3 ä¸ªæ–¹æ¡ˆæ‰§è¡Œåˆ° 50%
      æ€»è¿›åº¦ = (2/10)*100% + (50%/10) = 20% + 5% = 25%

    Args:
        max_workers: å¹¶è¡Œè¯„ä¼°çš„æœ€å¤§ worker æ•°é‡ï¼ˆ1 è¡¨ç¤ºä¸²è¡Œï¼‰
    """

    # æ£€æŸ¥å–æ¶ˆ
    if cancel_check and cancel_check():
        logger.info("ä»»åŠ¡åœ¨å¼€å§‹å‰è¢«å–æ¶ˆ")
        raise TaskCancelledException("ä»»åŠ¡å·²å–æ¶ˆ")

    max_chips = search_constraints.get("max_chips", total_chips)
    # æ³¨æ„ï¼šå·²ç§»é™¤ top_k é™åˆ¶ï¼Œç°åœ¨è¿”å›æ‰€æœ‰å¯è¡Œæ–¹æ¡ˆ

    if progress_callback:
        progress_callback(0, 100, "å‡†å¤‡è¯„ä¼°...")

    # ä»æ‹“æ‰‘é…ç½®ä¸­æå–ç¡¬ä»¶é…ç½®
    hardware_config = _extract_hardware_config(topology)

    # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å¹¶è¡Œç»„åˆ
    candidates = _generate_parallelism_candidates(
        max_chips,
        model_config,
        hardware_config
    )

    total_candidates = len(candidates)
    logger.info(f"è‡ªåŠ¨æ¨¡å¼: ç”Ÿæˆ {total_candidates} ä¸ªå€™é€‰æ–¹æ¡ˆ")

    # åˆå§‹åŒ–å­ä»»åŠ¡è¿›åº¦è·Ÿè¸ªï¼ˆæ‰€æœ‰æ–¹æ¡ˆåˆå§‹ä¸º pendingï¼‰
    sub_tasks = []
    for i, candidate in enumerate(candidates):
        sub_tasks.append({
            "candidate_index": i,
            "parallelism": candidate,
            "status": "pending",
            "progress": 0,
            "chips": calculate_required_chips(candidate, model_config),
        })

    # å®šä¹‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ¨é€å­ä»»åŠ¡çŠ¶æ€
    def update_sub_task(index: int, status: str, progress: int):
        sub_tasks[index]["status"] = status
        sub_tasks[index]["progress"] = progress

    if progress_callback:
        progress_callback(2, 100, f"å¼€å§‹è¯„ä¼° {total_candidates} ä¸ªæ–¹æ¡ˆï¼ˆå¹¶è¡Œåº¦={max_workers}ï¼‰...")

    # è®¡ç®—æ¯ä¸ªæ–¹æ¡ˆçš„ GEMM è¿›ç¨‹æ•°ï¼ˆåŠ¨æ€è°ƒæ•´é¿å…ç³»ç»Ÿè¿‡è½½ï¼‰
    # å…¬å¼ï¼šmax(1, cpu_count() // (2 * max_workers))
    # ä¾‹å¦‚ï¼š16æ ¸CPUï¼Œmax_workers=4ï¼Œåˆ™æ¯ä¸ªæ–¹æ¡ˆç”¨ 16//(2*4)=2 ä¸ªè¿›ç¨‹
    max_gemm_processes = max(1, cpu_count() // (2 * max_workers))
    logger.info(f"æ¯ä¸ªè¯„ä¼°ä»»åŠ¡ä½¿ç”¨ {max_gemm_processes} ä¸ª GEMM è¿›ç¨‹ï¼ˆæ€» worker={max_workers}ï¼ŒCPUæ ¸={cpu_count()}ï¼‰")

    # è¯„ä¼°æ‰€æœ‰å€™é€‰æ–¹æ¡ˆ
    feasible_plans = []
    infeasible_plans = []

    # é¢„ç•™ 2% ç»™å‡†å¤‡å·¥ä½œï¼Œ98% ç»™æ–¹æ¡ˆè¯„ä¼°
    progress_per_plan = 98.0 / total_candidates if total_candidates > 0 else 98.0

    # çº¿ç¨‹å®‰å…¨çš„è¿›åº¦æ›´æ–°é”
    progress_lock = threading.Lock()

    # è·Ÿè¸ªå·²å®Œæˆæ–¹æ¡ˆæ•°
    completed_count = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹

    # åˆ›å»ºå†…éƒ¨è¿›åº¦å›è°ƒï¼Œå°†æ–¹æ¡ˆçš„ 0-100% æ˜ å°„åˆ°è¯¥æ–¹æ¡ˆçš„è¿›åº¦ä»½é¢
    def make_inner_callback(plan_index: int):
        def inner_callback(inner_percent: float, message: str):
            if progress_callback:
                with progress_lock:
                    # æ›´æ–°å­ä»»åŠ¡è¿›åº¦
                    update_sub_task(plan_index, "running", int(inner_percent))
                    # è®¡ç®—æ€»è¿›åº¦ï¼šå·²å®Œæˆæ–¹æ¡ˆè¿›åº¦ + å½“å‰æ–¹æ¡ˆå†…éƒ¨è¿›åº¦
                    base_progress = 2 + completed_count[0] * progress_per_plan
                    current_plan_progress = (inner_percent / 100.0) * progress_per_plan
                    total_progress = base_progress + current_plan_progress
                    # å°è¯•ä¼ é€’ sub_tasks ä¿¡æ¯ç»™å‰ç«¯ï¼ˆä½¿ç”¨ try-except å¤„ç†ä¸åŒç­¾åï¼‰
                    try:
                        progress_callback(int(total_progress), 100, f"[{completed_count[0] + 1}/{total_candidates}] {message}", sub_tasks=sub_tasks)
                    except TypeError:
                        # å¦‚æœ progress_callback ä¸æ”¯æŒ sub_tasks å‚æ•°ï¼Œåªä¼ é€’åŸºæœ¬å‚æ•°
                        progress_callback(int(total_progress), 100, f"[{completed_count[0] + 1}/{total_candidates}] {message}")
        return inner_callback

    # è¯„ä¼°å•ä¸ªæ–¹æ¡ˆçš„workerå‡½æ•°
    def evaluate_plan_worker(plan_index: int, candidate: dict):
        """Workerå‡½æ•°ï¼šè¯„ä¼°å•ä¸ªæ–¹æ¡ˆ"""
        try:
            # æ£€æŸ¥å–æ¶ˆ
            if cancel_check and cancel_check():
                logger.info(f"ä»»åŠ¡åœ¨è¯„ä¼°æ–¹æ¡ˆ {plan_index + 1} å‰è¢«å–æ¶ˆ")
                raise TaskCancelledException("ä»»åŠ¡å·²å–æ¶ˆ")

            # æ ‡è®°å½“å‰æ–¹æ¡ˆä¸º running
            with progress_lock:
                update_sub_task(plan_index, "running", 0)
                # æŠ¥å‘Šä»»åŠ¡å¼€å§‹çŠ¶æ€
                if progress_callback:
                    base_progress = 2 + completed_count[0] * progress_per_plan
                    try:
                        progress_callback(int(base_progress), 100, f"è¯„ä¼°æ–¹æ¡ˆ {plan_index + 1}/{total_candidates}...", sub_tasks=sub_tasks)
                    except TypeError:
                        progress_callback(int(base_progress), 100, f"è¯„ä¼°æ–¹æ¡ˆ {plan_index + 1}/{total_candidates}...")

            inner_progress = make_inner_callback(plan_index)

            result = _evaluate_single_plan(
                parallelism=candidate,
                total_chips=total_chips,
                model_config=model_config,
                inference_config=inference_config,
                topology=topology,
                hardware_config=hardware_config,
                progress_callback=inner_progress,
                cancel_check=cancel_check,
                enable_tile_search=enable_tile_search,
                enable_partition_search=enable_partition_search,
                max_simulated_tokens=max_simulated_tokens,
                max_gemm_processes=max_gemm_processes,  # ä¼ é€’åŠ¨æ€è°ƒæ•´åçš„ GEMM è¿›ç¨‹æ•°
            )

            return (plan_index, result, None)

        except Exception as e:
            logger.error(f"è¯„ä¼°æ–¹æ¡ˆ {plan_index + 1} å¤±è´¥: {e}")
            return (plan_index, None, str(e))

    # æ ¹æ® max_workers é€‰æ‹©æ‰§è¡Œæ¨¡å¼
    if max_workers == 1:
        # ä¸²è¡Œæ¨¡å¼ï¼ˆä¿æŒåŸæœ‰è¡Œä¸ºï¼‰
        for i, candidate in enumerate(candidates):
            plan_index, result, error = evaluate_plan_worker(i, candidate)

            if error:
                with progress_lock:
                    update_sub_task(plan_index, "failed", 0)
                completed_count[0] += 1
                continue

            if result and result.get("is_feasible", False):
                with progress_lock:
                    update_sub_task(plan_index, "completed", 100)
                    feasible_plans.append(result)
                # æ¯å®Œæˆä¸€ä¸ªå¯è¡Œæ–¹æ¡ˆå°±ç«‹å³ä¿å­˜
                if result_callback:
                    try:
                        result_callback(result)
                    except Exception as e:
                        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            else:
                with progress_lock:
                    update_sub_task(plan_index, "failed", 100)
                    if result:
                        infeasible_plans.append(result)

            completed_count[0] += 1

    else:
        # å¹¶è¡Œæ¨¡å¼
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(evaluate_plan_worker, i, candidate): i
                for i, candidate in enumerate(candidates)
            }

            # æŒ‰å®Œæˆé¡ºåºå¤„ç†ç»“æœ
            for future in as_completed(future_to_index):
                # æ£€æŸ¥å–æ¶ˆ
                if cancel_check and cancel_check():
                    logger.info("ä»»åŠ¡è¢«å–æ¶ˆï¼Œåœæ­¢è¯„ä¼°")
                    executor.shutdown(wait=False, cancel_futures=True)
                    raise TaskCancelledException("ä»»åŠ¡å·²å–æ¶ˆ")

                plan_index, result, error = future.result()

                if error:
                    with progress_lock:
                        update_sub_task(plan_index, "failed", 0)
                    completed_count[0] += 1
                    continue

                if result and result.get("is_feasible", False):
                    with progress_lock:
                        update_sub_task(plan_index, "completed", 100)
                        feasible_plans.append(result)
                    # æ¯å®Œæˆä¸€ä¸ªå¯è¡Œæ–¹æ¡ˆå°±ç«‹å³ä¿å­˜
                    if result_callback:
                        try:
                            result_callback(result)
                        except Exception as e:
                            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
                else:
                    with progress_lock:
                        update_sub_task(plan_index, "failed", 100)
                        if result:
                            infeasible_plans.append(result)

                completed_count[0] += 1

    # æŒ‰å¾—åˆ†æ’åºï¼ˆè¿”å›æ‰€æœ‰å¯è¡Œæ–¹æ¡ˆï¼Œä¸å†æˆªæ–­ä¸º top_kï¼‰
    feasible_plans.sort(key=lambda x: x.get("score", 0), reverse=True)

    if progress_callback:
        progress_callback(100, 100, f"è¯„ä¼°å®Œæˆï¼Œæ‰¾åˆ° {len(feasible_plans)} ä¸ªå¯è¡Œæ–¹æ¡ˆ")

    return {
        "top_k_plans": feasible_plans,  # è¿”å›æ‰€æœ‰å¯è¡Œæ–¹æ¡ˆï¼ŒæŒ‰åˆ†æ•°æ’åº
        "infeasible_plans": infeasible_plans,
        "search_stats": {
            "total_plans": total_candidates,
            "feasible_plans": len(feasible_plans),
            "infeasible_plans": len(infeasible_plans),
        }
    }


def _generate_parallelism_candidates(
    max_chips: int,
    model_config: dict,
    hardware_config: dict
) -> List[dict]:
    """ç”Ÿæˆå¹¶è¡Œç­–ç•¥å€™é€‰

    èŠ¯ç‰‡æ•°è®¡ç®—è§„åˆ™ï¼š
    - MoE æ¨¡å‹ï¼šchips = DP Ã— TPï¼ˆå› ä¸º DPÃ—TP = MoE_TPÃ—EP çº¦æŸï¼‰
    - é MoE æ¨¡å‹ï¼šchips = DP Ã— TP Ã— EP
    - PP æš‚ä¸å¯ç”¨ï¼Œé»˜è®¤ä¸º1ï¼Œä¸å½±å“èŠ¯ç‰‡æ•°è®¡ç®—
    """

    candidates = []
    is_moe = model_config.get("moe_config") is not None

    # TODO: å®ç°æ™ºèƒ½çš„å€™é€‰ç”Ÿæˆç®—æ³•
    # åº”è¯¥æ ¹æ®ä»¥ä¸‹å› ç´ ç”Ÿæˆåˆæ³•çš„å› å­åˆ†è§£ï¼š
    # 1. æ¨¡å‹å¤§å°å’Œå±‚æ•°ï¼ˆå†³å®š PP çš„å¯èƒ½æ€§ï¼‰
    # 2. æ³¨æ„åŠ›å¤´æ•°ï¼ˆå†³å®š TP çš„çº¦æŸï¼‰
    # 3. ä¸“å®¶æ•°é‡ï¼ˆå†³å®š EP çš„éœ€æ±‚ï¼Œå¦‚æœæ˜¯ MoE æ¨¡å‹ï¼‰
    # 4. å¯ç”¨èŠ¯ç‰‡æ•°ï¼ˆmax_chipsï¼‰
    # 5. æ‹“æ‰‘ç»“æ„ï¼ˆåŒæ¿/åŒæœº/è·¨æœºçš„å¸¦å®½å·®å¼‚ï¼‰

    # å€™é€‰ç”Ÿæˆç­–ç•¥
    if is_moe:
        # MoE æ¨¡å‹ï¼šå®é™…èŠ¯ç‰‡æ•° = DP Ã— TP
        # EP ç”± MoE çº¦æŸè‡ªåŠ¨ç¡®å®šï¼Œè¿™é‡Œä¹Ÿæšä¸¾ä¸åŒçš„ EP
        for tp in [1, 2, 4, 8, 16, 32]:
            if tp > max_chips:
                continue
            for ep in [1, 2, 4, 8, 16, 32]:
                # è®¡ç®— DPï¼Œä½¿å¾— DP * TP <= max_chips
                dp = max_chips // tp
                if dp > 0:
                    # MoE çº¦æŸï¼šDP * TP = MoE_TP * EP
                    # è®¾ç½® moe_tp = (DP * TP) / EP
                    if (dp * tp) % ep == 0:
                        moe_tp = (dp * tp) // ep
                        candidate = {
                            "dp": dp,
                            "tp": tp,
                            "pp": 1,  # PP æš‚ä¸å¯ç”¨
                            "ep": ep,
                            "sp": 1,
                            "moe_tp": moe_tp,
                        }
                        candidates.append(candidate)
                        logger.debug(f"ç”Ÿæˆ MoE å€™é€‰: DP={dp}, TP={tp}, EP={ep}, MoE_TP={moe_tp}, èŠ¯ç‰‡æ•°={dp*tp}")
    else:
        # é MoE æ¨¡å‹ï¼šå®é™…èŠ¯ç‰‡æ•° = DP Ã— TP Ã— EP
        for tp in [1, 2, 4, 8, 16, 32]:
            if tp > max_chips:
                continue
            for ep in [1, 2, 4, 8, 16, 32]:
                if tp * ep > max_chips:
                    continue
                # è®¡ç®— DPï¼Œä½¿å¾— DP * TP * EP <= max_chips
                dp = max_chips // (tp * ep)
                if dp > 0:
                    candidates.append({
                        "dp": dp,
                        "tp": tp,
                        "pp": 1,  # PP æš‚ä¸å¯ç”¨
                        "ep": ep,
                        "sp": 1,
                    })

    logger.info(f"ç”Ÿæˆ {len(candidates)} ä¸ªå€™é€‰æ–¹æ¡ˆ (is_moe={is_moe}, max_chips={max_chips})")

    # æ‰“å°å‰5ä¸ªå€™é€‰æ–¹æ¡ˆç”¨äºè°ƒè¯•
    if len(candidates) > 0:
        logger.info(f"å‰5ä¸ªå€™é€‰æ–¹æ¡ˆç¤ºä¾‹:")
        for i, c in enumerate(candidates[:5]):
            logger.info(f"  æ–¹æ¡ˆ{i+1}: DP={c['dp']}, TP={c['tp']}, EP={c['ep']}, MoE_TP={c.get('moe_tp', 'N/A')}, èŠ¯ç‰‡æ•°={calculate_required_chips(c, model_config)}")

    # æ‰“å°å€™é€‰æ–¹æ¡ˆçš„ç»Ÿè®¡ä¿¡æ¯
    if len(candidates) > 0:
        ep_values = set(c['ep'] for c in candidates)
        moe_tp_values = set(c.get('moe_tp') for c in candidates if 'moe_tp' in c)
        logger.info(f"å€™é€‰æ–¹æ¡ˆç»Ÿè®¡: æ€»æ•°={len(candidates)}, EPå–å€¼={sorted(ep_values)}, MoE_TPå–å€¼={sorted(moe_tp_values) if moe_tp_values else 'N/A'}")

    return candidates


def _evaluate_single_plan(
    parallelism: dict,
    total_chips: int,
    model_config: dict,
    inference_config: dict,
    topology: dict,
    hardware_config: Optional[dict] = None,
    progress_callback: Optional[Callable[[float, str], None]] = None,
    cancel_check: Optional[Callable[[], bool]] = None,
    enable_tile_search: bool = True,
    enable_partition_search: bool = False,
    max_simulated_tokens: int = 4,
    max_gemm_processes: Optional[int] = None,
) -> dict:
    """è¯„ä¼°å•ä¸ªæ–¹æ¡ˆ

    Args:
        parallelism: å¹¶è¡Œç­–ç•¥é…ç½®
        total_chips: å¯ç”¨èŠ¯ç‰‡æ€»æ•°
        model_config: æ¨¡å‹é…ç½®
        inference_config: æ¨ç†é…ç½®
        topology: æ‹“æ‰‘é…ç½®
        hardware_config: ç¡¬ä»¶é…ç½®ï¼ˆå¯é€‰ï¼Œæœªæä¾›æ—¶è‡ªåŠ¨ä» topology æå–ï¼‰
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (percent: float, message: str) -> None
            percent: 0-100 çš„è¿›åº¦ç™¾åˆ†æ¯”
            message: è¿›åº¦æè¿°ä¿¡æ¯
        cancel_check: å–æ¶ˆæ£€æŸ¥å‡½æ•°
        enable_tile_search: æ˜¯å¦å¯ç”¨ tile æœç´¢
        enable_partition_search: æ˜¯å¦å¯ç”¨åˆ†åŒºæœç´¢
        max_simulated_tokens: æœ€å¤§æ¨¡æ‹Ÿ token æ•°
        max_gemm_processes: GEMM è¿›ç¨‹æ•°é™åˆ¶
    """
    # æ£€æŸ¥å–æ¶ˆ
    if cancel_check and cancel_check():
        logger.info("å•ä¸ªæ–¹æ¡ˆè¯„ä¼°è¢«å–æ¶ˆ")
        raise TaskCancelledException("ä»»åŠ¡å·²å–æ¶ˆ")

    # ç›´æ¥è®¿é—®å­—æ®µï¼Œç¼ºå¤±æ—¶ä¼šç«‹å³ KeyError
    dp = parallelism["dp"]
    tp = parallelism["tp"]
    pp = parallelism["pp"]
    ep = parallelism["ep"]

    # è®¡ç®—å®é™…ä½¿ç”¨çš„èŠ¯ç‰‡æ•°
    required_chips = calculate_required_chips(parallelism, model_config)

    # æŠ¥å‘Šæ£€æŸ¥è¿›åº¦
    if progress_callback:
        progress_callback(5, "æ£€æŸ¥èŠ¯ç‰‡æ•°é‡")

    if required_chips > total_chips:
        if progress_callback:
            progress_callback(100, "èŠ¯ç‰‡æ•°é‡ä¸è¶³")
        return _create_infeasible_result(
            parallelism,
            required_chips,
            f"éœ€è¦ {required_chips} ä¸ªèŠ¯ç‰‡ï¼Œè¶…å‡º {total_chips}"
        )

    # å¦‚æœæœªæä¾›ç¡¬ä»¶é…ç½®ï¼Œä»æ‹“æ‰‘ä¸­æå–
    if hardware_config is None:
        hardware_config = _extract_hardware_config(topology)

    # è°ƒç”¨çœŸå®çš„æ¨¡æ‹Ÿå™¨
    try:
        if progress_callback:
            progress_callback(10, "å¯åŠ¨æ¨¡æ‹Ÿå™¨")

        sim_result = run_simulation(
            topology_dict=topology,
            model_dict=model_config,
            inference_dict=inference_config,
            parallelism_dict=parallelism,
            hardware_dict=hardware_config,
            progress_callback=progress_callback,
            enable_tile_search=enable_tile_search,
            max_simulated_tokens=max_simulated_tokens,
            max_gemm_processes=max_gemm_processes,
        )

        if progress_callback:
            progress_callback(95, "è½¬æ¢ç»“æœæ ¼å¼")

        # è½¬æ¢ä¸º DS_TPU æ ¼å¼
        result = _transform_to_ds_tpu_format(
            sim_result=sim_result,
            parallelism=parallelism,
            chips=required_chips,
            model_config=model_config,
            inference_config=inference_config,
            topology=topology,
        )

        if progress_callback:
            progress_callback(100, "è¯„ä¼°å®Œæˆ")

        return result

    except Exception as e:
        logger.error(f"è¯„ä¼°æ–¹æ¡ˆå¤±è´¥ {parallelism}: {e}")
        if progress_callback:
            progress_callback(100, f"æ¨¡æ‹Ÿå¤±è´¥: {str(e)}")
        return _create_infeasible_result(
            parallelism,
            required_chips,
            f"æ¨¡æ‹Ÿå¤±è´¥: {str(e)}"
        )


def _create_infeasible_result(parallelism: dict, chips: int, reason: str) -> dict:
    """åˆ›å»ºä¸å¯è¡Œæ–¹æ¡ˆç»“æœ"""
    return {
        "parallelism": parallelism,
        "chips": chips,
        "is_feasible": False,
        "infeasible_reason": reason,
        "total_elapse_us": 0,
        "total_elapse_ms": 0,
        "comm_elapse_us": 0,
        "tps": 0,
        "tps_per_batch": 0,
        "tps_per_chip": 0,
        "mfu": 0,
        "flops": 0,
        "dram_occupy": 0,
        "score": 0,
    }


def _create_model_instance(model_config: dict, inference_config: dict, parallelism: dict):
    """æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ˆä¸ä½¿ç”¨é»˜è®¤å€¼ï¼‰"""
    # å‡†å¤‡æ¨¡å‹é…ç½®ï¼ˆåˆå¹¶ model_config, inference_config, parallelismï¼‰
    # ç›´æ¥è®¿é—®å­—æ®µï¼Œç¼ºå¤±æ—¶ä¼šç«‹å³ KeyError
    full_config = {
        **model_config,
        "batch_size": inference_config["batch_size"],
        "seq_len": inference_config["input_seq_length"],
        "kv_seq_len": inference_config["input_seq_length"],
        "is_prefill": False,  # Decode é˜¶æ®µ
        "tp": parallelism["tp"],
        "dp": parallelism["dp"],
        "moe_tp": parallelism.get("moe_tp", 1),  # moe_tp æ˜¯å¯é€‰çš„ï¼Œé MoE æ¨¡å‹æ²¡æœ‰
        "ep": parallelism.get("ep", 1),  # ep ä¹Ÿæ˜¯å¯é€‰çš„
        "comm_protocol": 1,
    }

    # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºå®ä¾‹
    model_name = model_config.get("model_name", "").lower()
    mla_config = model_config.get("mla_config")

    if "deepseek" in model_name or mla_config:
        # åˆ¤æ–­ MLA å˜ä½“
        if mla_config:
            # æœ‰ MLA é…ç½®ï¼Œé€‰æ‹©å¯¹åº”çš„å·¥å‚å‡½æ•°
            # ç®€åŒ–ï¼šé»˜è®¤ä½¿ç”¨ absorb å˜ä½“ï¼ˆDecode é˜¶æ®µï¼‰
            return create_deepseek_v3_absorb(**full_config)
        else:
            # æ ‡å‡† DeepSeek V3
            return create_deepseek_v3(**full_config)
    else:
        # å…¶ä»–æ¨¡å‹ç±»å‹ï¼Œä½¿ç”¨åŸºç¡€å·¥å‚
        logger.warning(f"Unknown model type: {model_name}, using deepseek_v3 as fallback")
        return create_deepseek_v3(**full_config)


def _transform_to_ds_tpu_format(
    sim_result: dict,
    parallelism: dict,
    chips: int,
    model_config: dict,
    inference_config: dict,
    topology: dict,
) -> dict:
    """å°† Tier6-Model æ¨¡æ‹Ÿç»“æœè½¬æ¢ä¸º DS_TPU æ ¼å¼ï¼ˆåŒ…å«è¯¦ç»†çš„ layers ä¿¡æ¯ï¼‰"""

    stats = sim_result.get("stats", {})

    # è°ƒè¯•ï¼šæ£€æŸ¥ linkTrafficStats æ˜¯å¦å­˜åœ¨
    import logging
    logger = logging.getLogger(__name__)
    link_traffic_stats = stats.get("linkTrafficStats")
    logger.info(f"ğŸ” [DEBUG] linkTrafficStats in stats: {link_traffic_stats is not None}, count: {len(link_traffic_stats) if link_traffic_stats else 0}")

    # æå–åŸºç¡€æ€§èƒ½æŒ‡æ ‡
    avg_tpot = stats.get("avgTpot", 0)  # å¾®ç§’/token
    ttft = stats.get("ttft", 0)  # å¾®ç§’
    mfu = stats.get("dynamicMfu", 0)
    mbu = stats.get("dynamicMbu", 0)

    # Decode é˜¶æ®µé€šä¿¡æ—¶é—´
    decode_stats = stats.get("decode", {})
    comm_time_us = decode_stats.get("commTime", 0)

    # è®¡ç®—ååé‡æŒ‡æ ‡
    total_elapse_us = avg_tpot if avg_tpot > 0 else 1.0
    total_elapse_ms = total_elapse_us / 1000.0

    # tokens/s = 1,000,000 us/s / (us/token)
    tps = 1_000_000.0 / total_elapse_us if total_elapse_us > 0 else 0

    batch_size = inference_config.get("batch_size", 1)
    tps_per_batch = tps / batch_size if batch_size > 0 else 0

    # ä¿®æ­£ tps_per_chip è®¡ç®—ï¼šé™¤ä»¥å®é™…ä½¿ç”¨çš„èŠ¯ç‰‡æ•°
    actual_chips = calculate_required_chips(parallelism, model_config)
    tps_per_chip = tps / actual_chips if actual_chips > 0 else 0

    # ä»æ‹“æ‰‘é…ç½®ä¸­æå–ç¡¬ä»¶é…ç½®ï¼ˆåªè°ƒç”¨ä¸€æ¬¡ï¼Œåç»­å¤ç”¨ï¼‰
    hardware_config = _extract_hardware_config(topology)
    # æ–°æ ¼å¼: é¡¶å±‚ chips å­—å…¸
    chips_dict = hardware_config.get("chips", {})
    if chips_dict:
        first_chip_name = next(iter(chips_dict))
        chip_hw = chips_dict[first_chip_name]
        chip_type = chip_hw.get("name", "SG2260E")
    else:
        chip_type = "SG2260E"

    # ä½¿ç”¨ PerformanceAnalyzer è·å–è¯¦ç»†çš„ layers ä¿¡æ¯
    layers_info = {}
    flops = 0
    dram_occupy = 0

    try:
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = _create_model_instance(model_config, inference_config, parallelism)

        # è·å–æ¶æ„é¢„è®¾
        arch = get_arch_preset(chip_type)

        # åˆ›å»º PerformanceAnalyzer
        analyzer = PerformanceAnalyzer(model, arch, global_cache={})

        # è·å–æ€§èƒ½åˆ†æç»“æœ
        analysis = model.analyze_performance(
            batch_size=batch_size,
            seq_len=inference_config.get("input_seq_length", 1),
            tpu_flops=getattr(arch, 'flops', 0)
        )

        # æå– layers ä¿¡æ¯å’ŒæŒ‡æ ‡
        layers_info = analysis.get("layers", {})
        flops = analysis.get("total_flops", 0)
        dram_occupy = analysis.get("dram_occupy", 0)

        logger.info(f"PerformanceAnalyzer æˆåŠŸç”Ÿæˆ {len(layers_info)} å±‚çš„è¯¦ç»†ä¿¡æ¯")

    except Exception as e:
        logger.warning(f"PerformanceAnalyzer è¿è¡Œå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ä¼°ç®—: {e}")

        # å›é€€åˆ°ç®€åŒ–ä¼°ç®—
        hidden_size = model_config.get("hidden_size", 0)
        num_layers = model_config.get("num_layers", 0)
        intermediate_size = model_config.get("intermediate_size", 0)

        flops_per_token = num_layers * (
            4 * hidden_size * hidden_size +
            2 * hidden_size * intermediate_size * 2
        )
        flops = flops_per_token * batch_size

        bytes_per_param = 2
        model_params = num_layers * (
            4 * hidden_size * hidden_size +
            2 * hidden_size * intermediate_size
        )
        seq_len = inference_config.get("input_seq_length", 0) + inference_config.get("output_seq_length", 0)
        kv_cache_params = 2 * num_layers * hidden_size * seq_len * batch_size
        dram_occupy = (model_params + kv_cache_params) * bytes_per_param

    # TODO: ç»¼åˆå¾—åˆ†è®¡ç®—å…¬å¼éœ€è¦é‡æ–°è®¾è®¡ï¼Œæš‚æ—¶è®¾ç½®ä¸º 0
    # åŸå…¬å¼ï¼šscore = tps_per_chip * mfuï¼ˆä¸å¤ªåˆç†ï¼Œåç»­è®¨è®ºä¿®æ”¹ï¼‰
    score = 0

    # æˆæœ¬è¯„ä¼°ï¼ˆå¤ç”¨å‰é¢æå–çš„ hardware_config å’Œ chip_typeï¼‰
    cost_result = {}
    try:

        # è·å–æ¨¡å‹å‚æ•°é‡
        num_parameters = model_config.get("num_parameters", 0)
        if num_parameters == 0:
            # å¦‚æœæ²¡æœ‰ç›´æ¥æä¾› num_parametersï¼Œå°è¯•ä»å…¶ä»–å‚æ•°ä¼°ç®—
            hidden_size = model_config.get("hidden_size", 0)
            num_layers = model_config.get("num_layers", 0)
            intermediate_size = model_config.get("intermediate_size", 0)
            vocab_size = model_config.get("vocab_size", 0)
            if all([hidden_size, num_layers, intermediate_size]):
                # ç®€åŒ–ä¼°ç®—ï¼šAttention + FFN å‚æ•°
                num_parameters = num_layers * (
                    4 * hidden_size * hidden_size +  # QKV + O
                    3 * hidden_size * intermediate_size  # Gate + Up + Down
                ) + vocab_size * hidden_size  # Embedding + LM Head

        # è®¡ç®—æˆæœ¬ï¼ˆä»…å½“æœ‰è¶³å¤Ÿä¿¡æ¯æ—¶ï¼‰
        if num_parameters > 0 and avg_tpot > 0 and tps > 0:
            cost_result = evaluate_deployment_cost(
                chips=chips,
                chip_type=chip_type,
                num_parameters=num_parameters,
                tp=parallelism.get("tp", 1),
                tpot_ms=avg_tpot / 1000.0,  # å¾®ç§’è½¬æ¯«ç§’
                tps=tps,
                bytes_per_param=2  # FP16/BF16
            )
            logger.info(f"æˆæœ¬è¯„ä¼°å®Œæˆ: æ€»æˆæœ¬=${cost_result.get('total_cost', 0):,.2f}")
        else:
            logger.warning(f"è·³è¿‡æˆæœ¬è®¡ç®—: num_parameters={num_parameters}, avg_tpot={avg_tpot}, tps={tps}")
    except Exception as e:
        logger.warning(f"æˆæœ¬è¯„ä¼°å¤±è´¥ï¼Œè·³è¿‡: {e}")
        cost_result = {}

    # è®°å½•æ—¥å¿—ä»¥ä¾¿è°ƒè¯•
    logger.info(f"ä¿å­˜ç»“æœ: DP={parallelism['dp']}, TP={parallelism['tp']}, PP={parallelism.get('pp', 1)}, EP={parallelism.get('ep', 1)}, MoE_TP={parallelism.get('moe_tp', 'N/A')}, èŠ¯ç‰‡æ•°={chips}")

    return {
        "parallelism": parallelism,
        "chips": chips,
        "is_feasible": True,
        "total_elapse_us": total_elapse_us,
        "total_elapse_ms": total_elapse_ms,
        "comm_elapse_us": comm_time_us,
        "tps": tps,
        "tps_per_batch": tps_per_batch,
        "tps_per_chip": tps_per_chip,
        "ttft": ttft / 1000.0,  # è½¬æ¢ä¸ºæ¯«ç§’
        "tpot": avg_tpot / 1000.0,  # è½¬æ¢ä¸ºæ¯«ç§’
        "mfu": mfu,
        "mbu": mbu,
        "flops": flops,
        "dram_occupy": dram_occupy,
        "score": score,
        "layers": layers_info,  # æ·»åŠ è¯¦ç»†çš„ layers ä¿¡æ¯
        "cost": cost_result,  # æˆæœ¬è¯„ä¼°ç»“æœ
        "stats": stats,  # å®Œæ•´çš„ç»Ÿè®¡æ•°æ®ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰
        "gantt_chart": sim_result.get("ganttChart"),  # ç”˜ç‰¹å›¾æ•°æ®ï¼ˆç”¨äºå‰ç«¯å¯è§†åŒ–ï¼‰
    }


def _extract_hardware_config(topology: dict) -> dict:
    """ä»æ‹“æ‰‘é…ç½®ä¸­æå–ç¡¬ä»¶é…ç½®

    æ”¯æŒå¤šç§é…ç½®æ ¼å¼ï¼š
    1. hardware_config: ç›´æ¥åµŒå…¥çš„ç¡¬ä»¶é…ç½®
    2. chips: é¡¶å±‚èŠ¯ç‰‡é…ç½®å­—å…¸ + interconnect: äº’è”é…ç½®
    3. pods/racks/boards/chips: å±•å¼€åçš„æ‹“æ‰‘ç»“æ„

    Args:
        topology: å®Œæ•´æ‹“æ‰‘é…ç½®

    Returns:
        ç¡¬ä»¶é…ç½®å­—å…¸ï¼ŒåŒ…å« chip, node, cluster é…ç½®

    Raises:
        ValueError: å¦‚æœæ— æ³•ä»æ‹“æ‰‘ä¸­æå–èŠ¯ç‰‡é…ç½®
    """
    # æ ¼å¼1ï¼šç›´æ¥åŒ…å« hardware_config
    if "hardware_config" in topology:
        return topology["hardware_config"]

    # æ ¼å¼2ï¼šé…ç½®æ–‡ä»¶æ ¼å¼ é¡¶å±‚ chips + interconnect (æ–°æ ¼å¼)
    if "chips" in topology:
        chips_dict = topology["chips"]

        if chips_dict:
            result = {
                "chips": chips_dict,
                "interconnect": topology.get("interconnect", {}),
            }
            return result

    # æ ¼å¼3ï¼šå±•å¼€åçš„ pods/racks/boards/chips ç»“æ„ -> è½¬æ¢ä¸ºæ–°æ ¼å¼
    pods = topology.get("pods", [])
    if pods:
        for pod in pods:
            racks = pod.get("racks", [])
            if racks:
                for rack in racks:
                    boards = rack.get("boards", [])
                    if boards:
                        for board in boards:
                            chips = board.get("chips", [])
                            if chips:
                                chip = chips[0]
                                chip_name = chip.get("name", "SG2260E")
                                # æ„å»ºæ–°æ ¼å¼çš„é¡¶å±‚ chips ç»“æ„
                                return {
                                    "chips": {
                                        chip_name: {
                                            "name": chip_name,
                                            "num_cores": chip.get("num_cores", 64),
                                            "compute_tflops_fp8": chip.get("compute_tflops_fp8", 0),
                                            "compute_tflops_bf16": chip.get("compute_tflops_bf16", 0),
                                            "memory_capacity_gb": chip.get("memory_capacity_gb", chip.get("memory_gb", 80)),
                                            "memory_bandwidth_gbps": chip.get("memory_bandwidth_gbps", 3000),
                                            "memory_bandwidth_utilization": chip.get("memory_bandwidth_utilization", 0.85),
                                            "lmem_capacity_mb": chip.get("lmem_capacity_mb", 0),
                                            "lmem_bandwidth_gbps": chip.get("lmem_bandwidth_gbps", 0),
                                            "cube_m": chip.get("cube_m"),
                                            "cube_k": chip.get("cube_k"),
                                            "cube_n": chip.get("cube_n"),
                                            "sram_size_kb": chip.get("sram_size_kb"),
                                            "sram_utilization": chip.get("sram_utilization"),
                                            "lane_num": chip.get("lane_num"),
                                            "align_bytes": chip.get("align_bytes"),
                                            "compute_dma_overlap_rate": chip.get("compute_dma_overlap_rate"),
                                        }
                                    },
                                    "interconnect": {},
                                }

    # å¦‚æœæ‰¾ä¸åˆ°èŠ¯ç‰‡é…ç½®ï¼ŒæŠ›å‡ºé”™è¯¯
    raise ValueError("æ— æ³•ä»æ‹“æ‰‘é…ç½®ä¸­æå–èŠ¯ç‰‡ç¡¬ä»¶é…ç½®ï¼Œè¯·ç¡®ä¿æ‹“æ‰‘ä¸­åŒ…å«æœ‰æ•ˆçš„èŠ¯ç‰‡æ•°æ®æˆ–é¡¶å±‚ chips é…ç½®")
