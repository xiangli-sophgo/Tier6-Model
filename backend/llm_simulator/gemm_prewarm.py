"""
GEMM è¯„ä¼°å™¨ç¦»çº¿é¢„è°ƒä¼˜

åœ¨æ¨¡æ‹Ÿå™¨åˆå§‹åŒ–æ—¶é¢„å…ˆè¯„ä¼°å¸¸è§çš„ GEMM å½¢çŠ¶ï¼Œé¿å…è¿è¡Œæ—¶é‡å¤æœç´¢ã€‚
"""

import logging
from typing import List, Tuple, Optional
from .evaluators import GEMMEvaluator

logger = logging.getLogger(__name__)


def generate_transformer_gemm_shapes(
    hidden_size: int,
    intermediate_size: int,
    num_attention_heads: int,
    num_kv_heads: int,
    batch_sizes: List[int],
    seq_lengths: List[int],
    mla_config: Optional[dict] = None,
    moe_config: Optional[dict] = None,
) -> List[Tuple[int, int, int, int]]:
    """
    ç”Ÿæˆ Transformer ä¸­æ‰€æœ‰å¯èƒ½çš„ GEMM å½¢çŠ¶

    Args:
        hidden_size: éšè—å±‚å¤§å°
        intermediate_size: FFN ä¸­é—´å±‚å¤§å°
        num_attention_heads: æ³¨æ„åŠ›å¤´æ•°é‡
        num_kv_heads: KV å¤´æ•°é‡ï¼ˆGQAï¼‰
        batch_sizes: æ‰¹æ¬¡å¤§å°åˆ—è¡¨ï¼ˆé€šå¸¸ [1, 2, 4, 8, ...]ï¼‰
        seq_lengths: åºåˆ—é•¿åº¦åˆ—è¡¨ï¼ˆPrefill: [128, 256, 512, 1024, 2048], Decode: [1]ï¼‰
        mla_config: MLA é…ç½®ï¼ˆå¯é€‰ï¼‰
        moe_config: MoE é…ç½®ï¼ˆå¯é€‰ï¼‰

    Returns:
        List of (G, M, K, N) å…ƒç»„
    """
    shapes = []
    head_dim = hidden_size // num_attention_heads

    for batch_size in batch_sizes:
        for seq_len in seq_lengths:
            M = batch_size * seq_len

            # ========== æ ‡å‡† Attention ==========
            if not mla_config:
                # QKV æŠ•å½±: [M, hidden] Ã— [hidden, 3*hidden] (for Q, K, V together)
                # æˆ–è€…åˆ†å¼€: Q=[M, hidden]Ã—[hidden, hidden], K/V=[M, hidden]Ã—[hidden, kv_hidden]
                shapes.append((1, M, hidden_size, hidden_size))  # Q projection
                shapes.append((1, M, hidden_size, num_kv_heads * head_dim))  # K projection (GQA)
                shapes.append((1, M, hidden_size, num_kv_heads * head_dim))  # V projection (GQA)

                # Attention Score: [batch, num_heads, seq_len, head_dim] Ã— [batch, num_heads, head_dim, seq_len]
                # â†’ Batched GEMM: G=batch*num_heads, M=seq_len, K=head_dim, N=seq_len
                # è¿™ä¸ªåœ¨ FlashAttention ä¸­é€šå¸¸èåˆï¼Œä¸å•ç‹¬è®¡ç®—

                # Attention Output: [M, hidden] Ã— [hidden, hidden]
                shapes.append((1, M, hidden_size, hidden_size))

            # ========== MLA (DeepSeek V3) ==========
            else:
                kv_lora_rank = mla_config.get("kv_lora_rank", 512)
                q_lora_rank = mla_config.get("q_lora_rank", 1536)
                qk_nope_head_dim = mla_config.get("qk_nope_head_dim", 128)
                qk_rope_head_dim = mla_config.get("qk_rope_head_dim", 64)
                v_head_dim = mla_config.get("v_head_dim", 128)

                # Q path: W_DQ, W_UQ, W_QR
                shapes.append((1, M, hidden_size, q_lora_rank))  # W_DQ
                shapes.append((1, M, q_lora_rank, num_attention_heads * (qk_nope_head_dim + qk_rope_head_dim)))  # W_UQ

                # KV path: W_DKV, W_UK, W_UV
                shapes.append((1, M, hidden_size, kv_lora_rank))  # W_DKV
                shapes.append((1, M, kv_lora_rank, num_attention_heads * qk_nope_head_dim))  # W_UK
                shapes.append((1, M, kv_lora_rank, num_attention_heads * v_head_dim))  # W_UV

                # Output: W_O
                shapes.append((1, M, num_attention_heads * v_head_dim, hidden_size))

            # ========== FFN ==========
            if not moe_config:
                # Standard FFN: gate, up, down
                shapes.append((1, M, hidden_size, intermediate_size))  # gate
                shapes.append((1, M, hidden_size, intermediate_size))  # up
                shapes.append((1, M, intermediate_size, hidden_size))  # down
            else:
                # MoE FFN
                num_experts = moe_config.get("num_experts", 64)
                expert_intermediate = moe_config.get("expert_intermediate_size", intermediate_size)

                # Router: [M, hidden] Ã— [hidden, num_experts]
                shapes.append((1, M, hidden_size, num_experts))

                # Expert FFN (æ¯ä¸ª expert çš„å½¢çŠ¶ç›¸åŒï¼Œåªè®¡ç®—ä¸€æ¬¡)
                shapes.append((1, M, hidden_size, expert_intermediate))  # expert gate
                shapes.append((1, M, hidden_size, expert_intermediate))  # expert up
                shapes.append((1, M, expert_intermediate, hidden_size))  # expert down

                # Shared expert (å¦‚æœæœ‰)
                if moe_config.get("num_shared_experts", 0) > 0:
                    shapes.append((1, M, hidden_size, intermediate_size))
                    shapes.append((1, M, intermediate_size, hidden_size))

    # å»é‡
    shapes = list(set(shapes))
    return shapes


def prewarm_gemm_evaluator(
    evaluator: GEMMEvaluator,
    hidden_size: int,
    intermediate_size: int,
    num_attention_heads: int,
    num_kv_heads: int,
    batch_size: int,
    input_seq_length: int,
    output_seq_length: int,
    mla_config: Optional[dict] = None,
    moe_config: Optional[dict] = None,
) -> int:
    """
    é¢„çƒ­ GEMM è¯„ä¼°å™¨ï¼Œé¢„å…ˆè¯„ä¼°å¸¸è§çš„ GEMM å½¢çŠ¶

    Args:
        evaluator: GEMM è¯„ä¼°å™¨å®ä¾‹
        ... å…¶ä»–å‚æ•°ä¸ generate_transformer_gemm_shapes ç›¸åŒ

    Returns:
        é¢„çƒ­çš„ GEMM å½¢çŠ¶æ•°é‡
    """
    import time

    logger.info("ğŸ”¥ å¼€å§‹ GEMM è¯„ä¼°å™¨é¢„çƒ­...")
    start = time.time()

    # ç”Ÿæˆå¸¸è§çš„æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦ç»„åˆ
    batch_sizes = [batch_size]  # åªé¢„çƒ­å½“å‰æ‰¹æ¬¡å¤§å°

    # Prefill å’Œ Decode çš„åºåˆ—é•¿åº¦
    seq_lengths = [
        input_seq_length,  # Prefill
        1,                 # Decode (æ¯æ¬¡ç”Ÿæˆ 1 ä¸ª token)
    ]

    # ç”Ÿæˆæ‰€æœ‰ GEMM å½¢çŠ¶
    shapes = generate_transformer_gemm_shapes(
        hidden_size=hidden_size,
        intermediate_size=intermediate_size,
        num_attention_heads=num_attention_heads,
        num_kv_heads=num_kv_heads,
        batch_sizes=batch_sizes,
        seq_lengths=seq_lengths,
        mla_config=mla_config,
        moe_config=moe_config,
    )

    logger.info(f"   ç”Ÿæˆ {len(shapes)} ä¸ª GEMM å½¢çŠ¶å¾…é¢„çƒ­")

    # é¢„çƒ­è¯„ä¼°
    dtype = "bf16"  # é»˜è®¤ä½¿ç”¨ bf16
    for i, (G, M, K, N) in enumerate(shapes):
        try:
            # è°ƒç”¨ evaluate ä¼šè‡ªåŠ¨ç¼“å­˜ç»“æœ
            evaluator.evaluate(
                G=G, M=M, K=K, N=N,
                input_dtype=dtype,
                output_dtype=dtype,
                use_multiprocess=False,  # é¢„çƒ­æ—¶ç¦ç”¨å¤šè¿›ç¨‹ï¼ˆé¿å…å¯åŠ¨å¼€é”€ï¼‰
            )

            if (i + 1) % 10 == 0:
                logger.info(f"   é¢„çƒ­è¿›åº¦: {i+1}/{len(shapes)}")

        except Exception as e:
            logger.warning(f"   é¢„çƒ­ GEMM ({G}, {M}, {K}, {N}) å¤±è´¥: {e}")

    elapsed = time.time() - start
    logger.info(f"âœ… GEMM é¢„çƒ­å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}sï¼Œç¼“å­˜ {len(shapes)} ä¸ªé…ç½®")

    return len(shapes)


def get_cache_stats(evaluator: GEMMEvaluator) -> dict:
    """
    è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

    Returns:
        ç¼“å­˜ç»Ÿè®¡å­—å…¸
    """
    return {
        "cached_configs": len(evaluator._cache),
        "cache_keys": list(evaluator._cache.keys())[:5],  # æ˜¾ç¤ºå‰ 5 ä¸ª
    }
