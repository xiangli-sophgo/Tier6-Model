"""
GEMM è¯„ä¼°å™¨ç¦»çº¿é¢„è°ƒä¼˜

åœ¨æ¨¡æ‹Ÿå™¨åˆå§‹åŒ–æ—¶é¢„å…ˆè¯„ä¼°å¸¸è§çš„ GEMM å½¢çŠ¶ï¼Œé¿å…è¿è¡Œæ—¶é‡å¤æœç´¢ã€‚
"""

import logging
from typing import List, Tuple, Optional
from ..evaluators import GEMMEvaluator

logger = logging.getLogger(__name__)


def generate_transformer_gemm_shapes(
    hidden_size: int,
    intermediate_size: int,
    num_attention_heads: int,
    num_kv_heads: int,
    batch_sizes: List[int],
    seq_lengths: List[int],
    tp: int = 1,  # â­ æ–°å¢: å¼ é‡å¹¶è¡Œåº¦
    mla_config: Optional[dict] = None,
    moe_config: Optional[dict] = None,
) -> List[Tuple[int, int, int, int]]:
    """
    ç”Ÿæˆ Transformer ä¸­æ‰€æœ‰å¯èƒ½çš„ GEMM å½¢çŠ¶

    Args:
        hidden_size: éšè—å±‚å¤§å°
        intermediate_size: FFN ä¸­é—´å±‚å¤§å°
        num_attention_heads: æ³¨æ„åŠ›å¤´æ•°é‡
        num_kv_heads: KV å¤´æ•°é‡ï¼ˆGQAï¼‰
        batch_sizes: æ‰¹æ¬¡å¤§å°åˆ—è¡¨ï¼ˆé€šå¸¸ [1, 2, 4, 8, ...]ï¼‰
        seq_lengths: åºåˆ—é•¿åº¦åˆ—è¡¨ï¼ˆPrefill: [128, 256, 512, 1024, 2048], Decode: [1]ï¼‰
        tp: å¼ é‡å¹¶è¡Œåº¦ï¼ˆé»˜è®¤1ï¼Œæ— å¹¶è¡Œï¼‰
        mla_config: MLA é…ç½®ï¼ˆå¯é€‰ï¼‰
        moe_config: MoE é…ç½®ï¼ˆå¯é€‰ï¼‰

    Returns:
        List of (G, M, K, N) å…ƒç»„
    """
    shapes = []
    head_dim = hidden_size // num_attention_heads

    # â­ TPåˆ†ç‰‡åçš„ç»´åº¦
    heads_per_tp = num_attention_heads // tp
    kv_heads_per_tp = num_kv_heads // tp
    hidden_per_tp = hidden_size // tp
    intermediate_per_tp = intermediate_size // tp

    for batch_size in batch_sizes:
        for seq_len in seq_lengths:
            M = batch_size * seq_len

            # ========== æ ‡å‡† Attention ==========
            if not mla_config:
                # â­ TPåçš„QKVæŠ•å½±å½¢çŠ¶
                # QKVåˆå¹¶æŠ•å½±: qkv_dim = (heads_per_tp + 2 * kv_heads_per_tp) * head_dim
                qkv_dim = (heads_per_tp + 2 * kv_heads_per_tp) * head_dim
                shapes.append((1, M, hidden_size, qkv_dim))  # QKV projection (TPåˆ†ç‰‡)

                # â­ TPåçš„OutputæŠ•å½±å½¢çŠ¶
                # Input: heads_per_tp * head_dim, Output: hidden_size (å…¨é‡ï¼Œåç»­AllReduce)
                shapes.append((1, M, heads_per_tp * head_dim, hidden_size))

            # ========== MLA (DeepSeek V3) ==========
            else:
                kv_lora_rank = mla_config.get("kv_lora_rank", 512)
                q_lora_rank = mla_config.get("q_lora_rank", 1536)
                qk_nope_head_dim = mla_config.get("qk_nope_head_dim", 128)
                qk_rope_head_dim = mla_config.get("qk_rope_head_dim", 64)
                v_head_dim = mla_config.get("v_head_dim", 128)

                # Q path: W_DQ, W_UQ, W_QR
                shapes.append((1, M, hidden_size, q_lora_rank))  # W_DQ
                shapes.append((1, M, q_lora_rank, num_attention_heads * (qk_nope_head_dim + qk_rope_head_dim)))  # W_UQ

                # KV path: W_DKV, W_UK, W_UV
                shapes.append((1, M, hidden_size, kv_lora_rank))  # W_DKV
                shapes.append((1, M, kv_lora_rank, num_attention_heads * qk_nope_head_dim))  # W_UK
                shapes.append((1, M, kv_lora_rank, num_attention_heads * v_head_dim))  # W_UV

                # Output: W_O
                shapes.append((1, M, num_attention_heads * v_head_dim, hidden_size))

            # ========== FFN ==========
            if not moe_config:
                # â­ TPåçš„FFNå½¢çŠ¶
                # Gate/UpæŠ•å½±: hidden_size -> intermediate_per_tp
                shapes.append((1, M, hidden_size, intermediate_per_tp))  # gate
                shapes.append((1, M, hidden_size, intermediate_per_tp))  # up
                # DownæŠ•å½±: intermediate_per_tp -> hidden_size
                shapes.append((1, M, intermediate_per_tp, hidden_size))  # down
            else:
                # MoE FFN
                num_experts = moe_config.get("num_experts", 64)
                expert_intermediate = moe_config.get("expert_intermediate_size", intermediate_size)

                # Router: [M, hidden] Ã— [hidden, num_experts]
                shapes.append((1, M, hidden_size, num_experts))

                # Expert FFN (æ¯ä¸ª expert çš„å½¢çŠ¶ç›¸åŒï¼Œåªè®¡ç®—ä¸€æ¬¡)
                shapes.append((1, M, hidden_size, expert_intermediate))  # expert gate
                shapes.append((1, M, hidden_size, expert_intermediate))  # expert up
                shapes.append((1, M, expert_intermediate, hidden_size))  # expert down

                # Shared expert (å¦‚æœæœ‰)
                if moe_config.get("num_shared_experts", 0) > 0:
                    shapes.append((1, M, hidden_size, intermediate_size))
                    shapes.append((1, M, intermediate_size, hidden_size))

    # å»é‡
    shapes = list(set(shapes))
    return shapes


def prewarm_gemm_evaluator(
    evaluator: GEMMEvaluator,
    hidden_size: int,
    intermediate_size: int,
    num_attention_heads: int,
    num_kv_heads: int,
    batch_size: int,
    input_seq_length: int,
    output_seq_length: int,
    tp: int = 1,  # â­ æ–°å¢: å¼ é‡å¹¶è¡Œåº¦
    mla_config: Optional[dict] = None,
    moe_config: Optional[dict] = None,
    progress_callback: Optional[callable] = None,  # è¿›åº¦å›è°ƒ
) -> int:
    """
    é¢„çƒ­ GEMM è¯„ä¼°å™¨ï¼Œé¢„å…ˆè¯„ä¼°å¸¸è§çš„ GEMM å½¢çŠ¶

    Args:
        evaluator: GEMM è¯„ä¼°å™¨å®ä¾‹
        ... å…¶ä»–å‚æ•°ä¸ generate_transformer_gemm_shapes ç›¸åŒ

    Returns:
        é¢„çƒ­çš„ GEMM å½¢çŠ¶æ•°é‡
    """
    import time

    logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    logger.info("ğŸ”¥ GEMM è¯„ä¼°å™¨é¢„çƒ­")
    logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    start = time.time()

    # ç”Ÿæˆå¸¸è§çš„æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦ç»„åˆ
    batch_sizes = [batch_size]  # åªé¢„çƒ­å½“å‰æ‰¹æ¬¡å¤§å°

    # ğŸš€ ä¼˜åŒ–ç­–ç•¥ï¼šåªé¢„çƒ­ Decode é˜¶æ®µï¼ˆæœ€é¢‘ç¹ä½¿ç”¨ï¼‰
    # Prefill å½¢çŠ¶ä¼šåœ¨é¦–æ¬¡è¿è¡Œæ—¶æŒ‰éœ€è¯„ä¼°å¹¶ç¼“å­˜
    # è¿™æ ·å¯ä»¥å°†é¢„çƒ­å½¢çŠ¶æ•°é‡å‡å°‘ 50%ï¼ŒåŠ é€Ÿå¯åŠ¨
    seq_lengths = [
        1,  # Decode (æ¯æ¬¡ç”Ÿæˆ 1 ä¸ª tokenï¼Œæœ€é¢‘ç¹ä½¿ç”¨)
    ]

    # å¦‚æœ input_seq_length å¾ˆå°ï¼ˆ<= 512ï¼‰ï¼Œä¹Ÿé¢„çƒ­ Prefill
    # å› ä¸ºå°åºåˆ—é•¿åº¦æœç´¢å¾ˆå¿«
    if input_seq_length <= 512:
        seq_lengths.append(input_seq_length)

    # ç”Ÿæˆæ‰€æœ‰ GEMM å½¢çŠ¶
    shapes = generate_transformer_gemm_shapes(
        hidden_size=hidden_size,
        intermediate_size=intermediate_size,
        num_attention_heads=num_attention_heads,
        num_kv_heads=num_kv_heads,
        batch_sizes=batch_sizes,
        seq_lengths=seq_lengths,
        tp=tp,  # â­ ä¼ é€’TPå‚æ•°
        mla_config=mla_config,
        moe_config=moe_config,
    )

    logger.info(f"   æ¨¡å‹é…ç½®: hidden={hidden_size}, intermediate={intermediate_size}")
    logger.info(f"   æ‰¹æ¬¡å¤§å°: {batch_size}, é¢„çƒ­åºåˆ—é•¿åº¦: {seq_lengths}")
    logger.info(f"   å¹¶è¡Œç­–ç•¥: TP={tp}")  # â­ æ˜¾ç¤ºTPé…ç½®
    logger.info(f"   ç”Ÿæˆ {len(shapes)} ä¸ª GEMM å½¢çŠ¶å¾…é¢„çƒ­")
    logger.info(f"   æœç´¢æ¨¡å¼: å¤šè¿›ç¨‹å¹¶è¡Œæœç´¢ (åŠ é€Ÿ 15-20x)")

    # é¢„çƒ­è¯„ä¼°
    dtype = "bf16"  # é»˜è®¤ä½¿ç”¨ bf16
    prewarm_times = []

    for i, (G, M, K, N) in enumerate(shapes):
        try:
            shape_start = time.time()

            # è°ƒç”¨ evaluate ä¼šè‡ªåŠ¨ç¼“å­˜ç»“æœ
            # ğŸš€ å¯ç”¨å¤šè¿›ç¨‹åŠ é€Ÿæœç´¢ï¼ˆ24æ ¸ vs 1æ ¸ï¼Œæé€Ÿ 15-20xï¼‰
            evaluator.evaluate(
                G=G, M=M, K=K, N=N,
                input_dtype=dtype,
                output_dtype=dtype,
                use_multiprocess=True,  # âœ… å¯ç”¨å¤šè¿›ç¨‹å¹¶è¡Œæœç´¢
            )

            shape_time = (time.time() - shape_start) * 1000
            prewarm_times.append(shape_time)

            # æ¯5ä¸ªæˆ–åœ¨æœ€åæ‰“å°è¿›åº¦
            if (i + 1) % 5 == 0 or (i + 1) == len(shapes):
                avg_time = sum(prewarm_times[-5:]) / min(5, len(prewarm_times[-5:]))
                logger.info(f"   è¿›åº¦: {i+1}/{len(shapes)} (å¹³å‡ {avg_time:.1f}ms/å½¢çŠ¶)")

                # è°ƒç”¨è¿›åº¦å›è°ƒï¼ˆé¢„çƒ­å  10% çš„æ€»è¿›åº¦ï¼‰
                if progress_callback:
                    prewarm_progress = (i + 1) / len(shapes) * 10  # 0-10%
                    progress_callback(prewarm_progress, f"GEMM é¢„çƒ­ {i+1}/{len(shapes)}")

        except Exception as e:
            logger.warning(f"   âš ï¸  é¢„çƒ­å¤±è´¥ GEMM({G},{M},{K},{N}): {e}")

    elapsed = time.time() - start
    avg_prewarm_time = sum(prewarm_times) / len(prewarm_times) if prewarm_times else 0

    logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    logger.info(f"âœ… é¢„çƒ­å®Œæˆ")
    logger.info(f"   æ€»è€—æ—¶: {elapsed:.2f}s")
    logger.info(f"   å·²ç¼“å­˜: {len(shapes)} ä¸ªé…ç½®")
    logger.info(f"   å¹³å‡è€—æ—¶: {avg_prewarm_time:.1f}ms/å½¢çŠ¶")
    if prewarm_times:
        logger.info(f"   æœ€æ…¢å½¢çŠ¶: {max(prewarm_times):.1f}ms")
    logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

    return len(shapes)


def get_cache_stats(evaluator: GEMMEvaluator) -> dict:
    """
    è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

    Returns:
        ç¼“å­˜ç»Ÿè®¡å­—å…¸
    """
    return {
        "cached_configs": len(evaluator._cache),
        "cache_keys": list(evaluator._cache.keys())[:5],  # æ˜¾ç¤ºå‰ 5 ä¸ª
    }
