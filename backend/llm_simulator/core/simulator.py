"""
LLM æ¨ç†æ¨¡æ‹Ÿå™¨æ ¸å¿ƒ

å®ç°åŸºäºæ‹“æ‰‘çš„ GPU/åŠ é€Ÿå™¨ä¾§ç²¾ç»†æ¨¡æ‹Ÿï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®æ¬è¿é˜¶æ®µï¼ˆPCIeä¼ è¾“ã€HBMå­˜å‚¨ã€æƒé‡åŠ è½½ï¼‰
- æ¨ç†è®¡ç®—é˜¶æ®µï¼ˆç»†åŒ–ä¸ºAttention/FFN/LayerNormå­æ“ä½œï¼‰
- ç»“æœæ”¶é›†é˜¶æ®µï¼ˆHBMè¯»å–ã€PCIeå›ä¼ ï¼‰
"""

from __future__ import annotations

import time
from typing import Any, Optional
from dataclasses import dataclass, field

from ..config import (
    LLMModelConfig,
    InferenceConfig,
    ParallelismStrategy,
    HierarchicalTopology,
    ChipConfig,
    SimulationResult,
    SimulationStats,
    PhaseTimeStats,
    GanttTaskType,
    InferencePhase,
    get_bytes_per_element,
    MLAConfig,
    MoEConfig,
    # éªŒè¯å‡½æ•°
    validate_mla_config,
    validate_moe_config,
    validate_model_config,
    validate_hardware_config,
    validate_parallelism_config,
)


@dataclass
class RuntimeHardwareParams:
    """è¿è¡Œæ—¶ç¡¬ä»¶å‚æ•°ï¼ˆä»æ‹“æ‰‘é…ç½®æˆ–ç¡¬ä»¶é…ç½®ä¸­æå–ï¼‰

    è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ•°æ®ç±»ï¼Œç”¨äºå­˜å‚¨æ¨¡æ‹Ÿå™¨è¿è¡Œæ—¶éœ€è¦çš„ç¡¬ä»¶å‚æ•°ã€‚
    å®ƒä¸ä»£è¡¨å®Œæ•´çš„ç¡¬ä»¶é…ç½®ï¼Œåªæ˜¯æ¨¡æ‹Ÿå™¨éœ€è¦çš„å‚æ•°é›†åˆã€‚
    """
    # èŠ¯ç‰‡å‚æ•°
    chip_type: str = "Unknown"
    num_cores: int = 1
    compute_tflops_fp8: float = 0.0
    compute_tflops_bf16: float = 0.0
    memory_capacity_gb: float = 0.0
    memory_bandwidth_gbps: float = 0.0
    memory_bandwidth_utilization: float = 0.85
    lmem_capacity_mb: float = 0.0
    lmem_bandwidth_gbps: float = 0.0
    # å¾®æ¶æ„å‚æ•°ï¼ˆå¯é€‰ï¼‰
    cube_m: Optional[int] = None
    cube_k: Optional[int] = None
    cube_n: Optional[int] = None
    sram_size_kb: Optional[float] = None
    sram_utilization: Optional[float] = None
    lane_num: Optional[int] = None
    align_bytes: Optional[int] = None
    compute_dma_overlap_rate: Optional[float] = None
    # äº’è”å‚æ•°ï¼ˆé»˜è®¤å€¼ï¼Œä¼šè¢«æ‹“æ‰‘é…ç½®è¦†ç›–ï¼‰
    c2c_bandwidth_gbps: float = 0.0
    c2c_latency_us: float = 0.0
    b2b_bandwidth_gbps: float = 450.0  # Board-to-Board
    b2b_latency_us: float = 0.35
    r2r_bandwidth_gbps: float = 200.0  # Rack-to-Rack
    r2r_latency_us: float = 2.0
    p2p_bandwidth_gbps: float = 100.0  # Pod-to-Pod
    p2p_latency_us: float = 5.0
from .topology import TopologyParser
from .gantt import GanttChartBuilder, convert_to_frontend_format

# æ–°è¯„ä¼°å™¨ç³»ç»Ÿ
from ..evaluators import (
    get_arch_preset,
    AcceleratorMicroArch,
    GEMMEvaluator,
    FA2Evaluator,
    AllReduceEval,
    AllGatherEval,
    create_gemm_evaluator,
    ReduceScatterEval,
)
from .analyzer import PerformanceAnalyzer
from ..layers import (
    MLALayer,
    MLAv32Layer,
    MLAAbsorbLayer,
    MLAAbsorbv32Layer,
    MHALayer,
    MLPLayer,
    MoELayer,
)
from ..operators.base import ComputeOpType, CommOpType


@dataclass
class SimulationConfig:
    """æ¨¡æ‹Ÿé…ç½®"""

    max_simulated_tokens: int = 16
    enable_data_transfer: bool = True
    enable_detailed_ops: bool = True
    enable_kv_cache: bool = True
    enable_overlap: bool = True
    # æ–°å¢: Kernel Fusion å’Œ MLA ä¼˜åŒ–
    enable_fusion: bool = True  # å¯ç”¨ Kernel Fusion ä¼˜åŒ–
    enable_comm_overlap: bool = True  # å¯ç”¨è®¡ç®—-é€šä¿¡é‡å 
    enable_tbo: bool = True  # å¯ç”¨ TBO (Tensor-Bus Overlap) é‡å ä¼˜åŒ– (MoEä¸“ç”¨) â­ æ–°å¢
    # è®­ç»ƒæ¨¡å¼é…ç½®
    enable_training_mode: bool = False  # å¯ç”¨è®­ç»ƒæ¨¡å¼ï¼ˆæ¨¡æ‹ŸDPæ¢¯åº¦åŒæ­¥ï¼‰
    enable_dp_gradient_sync: bool = False  # å¯ç”¨DPæ¢¯åº¦åŒæ­¥æ¨¡æ‹Ÿ
    gradient_accumulation_steps: int = 1  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    # æ–°è¯„ä¼°å™¨ç³»ç»Ÿé…ç½®
    use_precise_evaluator: bool = True  # ä½¿ç”¨ç²¾ç¡®è¯„ä¼°å™¨ï¼ˆåŸºäºç¡¬ä»¶å»ºæ¨¡ï¼‰
    evaluation_granularity: str = "fine"  # è¯„ä¼°ç²’åº¦: coarseï¼ˆç²—ç²’åº¦ï¼‰æˆ– fineï¼ˆç»†ç²’åº¦ï¼‰
    enable_gemm_prewarm: bool = False  # ğŸš€ ç¦ç”¨é¢„çƒ­ï¼Œæ”¹ç”¨æ‡’åŠ è½½ç­–ç•¥ï¼ˆæŒ‰éœ€æœç´¢+å…¨å±€ç¼“å­˜ï¼‰
    # æ³¨æ„: mla_variant å·²ç§»è‡³ model.mla_config.variantï¼Œä»æ¨¡å‹é…ç½®è¯»å–


@dataclass
class ChipState:
    """èŠ¯ç‰‡çŠ¶æ€"""

    chip_id: str
    pp_stage: int
    tp_rank: int
    current_time: float = 0.0
    compute_idle_at: float = 0.0
    network_idle_at: float = 0.0


class LLMInferenceSimulator:
    """LLM æ¨ç†æ¨¡æ‹Ÿå™¨"""

    def __init__(
        self,
        topology_dict: dict[str, Any],
        model: LLMModelConfig,
        inference: InferenceConfig,
        parallelism: ParallelismStrategy,
        hardware: RuntimeHardwareParams,
        config: SimulationConfig | None = None,
        comm_latency_config: dict[str, float] | None = None,
        progress_callback: callable | None = None,
        enable_tile_search: bool = True,
        enable_partition_search: bool = False,
        max_gemm_processes: Optional[int] = None,
        moe_tp: int | None = None,
    ):
        """
        åˆå§‹åŒ–æ¨¡æ‹Ÿå™¨

        Args:
            topology_dict: å‰ç«¯æ‹“æ‰‘é…ç½®ï¼ˆåŒ…å«åµŒå…¥çš„ç¡¬ä»¶å‚æ•°ï¼‰
            model: æ¨¡å‹é…ç½®
            inference: æ¨ç†é…ç½®
            parallelism: å¹¶è¡Œç­–ç•¥
            hardware: è¿è¡Œæ—¶ç¡¬ä»¶å‚æ•°
            config: æ¨¡æ‹Ÿé…ç½®
            comm_latency_config: é€šä¿¡å»¶è¿Ÿé…ç½® (å‰ç«¯ä¼ é€’çš„ç»Ÿä¸€é…ç½®ï¼Œè¦†ç›–é¢„è®¾å€¼)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (percent: float, message: str) -> None
        """
        self.model = model
        self.inference = inference
        self.parallelism = parallelism
        self.hardware = hardware
        self.config = config or SimulationConfig()
        self.comm_latency_config = comm_latency_config
        self.progress_callback = progress_callback
        self.moe_tp = moe_tp  # MoE å¼ é‡å¹¶è¡Œåº¦ï¼ˆç”¨äº MoE å±‚è®¡ç®—ï¼‰

        # åˆå§‹åŒ–æ–°è¯„ä¼°å™¨ç³»ç»Ÿ
        if self.config.use_precise_evaluator:
            # æ ¹æ®ç¡¬ä»¶ç±»å‹é€‰æ‹©èŠ¯ç‰‡æ¶æ„é¢„è®¾
            chip_type = hardware.chip_type
            import logging
            logger = logging.getLogger(__name__)
            logger.info(f"ğŸ”§ èŠ¯ç‰‡ç±»å‹: {chip_type}")
            try:
                self.arch = get_arch_preset(chip_type)
                logger.info(f"âœ… ä½¿ç”¨æ¶æ„é¢„è®¾: {self.arch.name}")
            except (KeyError, ValueError) as e:
                # å¦‚æœæ²¡æœ‰é¢„è®¾ï¼Œä½¿ç”¨é»˜è®¤ SG2260E
                logger.warning(f"æœªæ‰¾åˆ° {chip_type} çš„æ¶æ„é¢„è®¾ ({e})ï¼Œä½¿ç”¨ SG2260E")
                self.arch = get_arch_preset("SG2260E")

            # ä½¿ç”¨å‰ç«¯ä¼ é€’çš„é€šä¿¡å»¶è¿Ÿé…ç½®è¦†ç›–é¢„è®¾å€¼
            if comm_latency_config:
                # è¦†ç›–èŠ¯ç‰‡å»¶è¿Ÿé…ç½®
                from ..evaluators.arch_config import CommunicationLatency

                self.arch.comm_latency = CommunicationLatency(
                    chip_to_chip_us=comm_latency_config.get("chip_to_chip_us", self.arch.comm_latency.chip_to_chip_us),
                    memory_read_latency_us=comm_latency_config.get("memory_read_latency_us", self.arch.comm_latency.memory_read_latency_us),
                    memory_write_latency_us=comm_latency_config.get("memory_write_latency_us", self.arch.comm_latency.memory_write_latency_us),
                    noc_latency_us=comm_latency_config.get("noc_latency_us", self.arch.comm_latency.noc_latency_us),
                    die_to_die_latency_us=comm_latency_config.get("die_to_die_latency_us", self.arch.comm_latency.die_to_die_latency_us),
                )

            # åˆ›å»ºåè®®é…ç½®å’Œç½‘ç»œåŸºç¡€è®¾æ–½é…ç½®å¯¹è±¡ (ä¾›é€šä¿¡è¯„ä¼°å™¨ä½¿ç”¨)
            from ..config import ProtocolConfig, NetworkInfraConfig

            if comm_latency_config:
                self.protocol_cfg = ProtocolConfig(
                    rtt_tp_us=comm_latency_config.get("rtt_tp_us", 0.35),
                    rtt_ep_us=comm_latency_config.get("rtt_ep_us", 0.85),
                    bandwidth_utilization=comm_latency_config.get("bandwidth_utilization", 0.95),
                    sync_latency_us=comm_latency_config.get("sync_latency_us", 0.0),
                )
                self.network_cfg = NetworkInfraConfig(
                    switch_delay_us=comm_latency_config.get("switch_delay_us", 1.0),
                    cable_delay_us=comm_latency_config.get("cable_delay_us", 0.025),
                )
            else:
                self.protocol_cfg = ProtocolConfig()
                self.network_cfg = NetworkInfraConfig()

            # åˆ›å»º GEMM è¯„ä¼°å™¨ï¼ˆå…¨å±€å•ä¾‹ï¼Œè·¨å±‚å¤ç”¨ï¼‰
            # fast_mode=True æ—¶ä½¿ç”¨å›ºå®štileï¼ˆå…³é—­tileæœç´¢ï¼‰ï¼Œæ˜¾è‘—æå‡è¯„ä¼°é€Ÿåº¦
            # enable_partition_search=False æ—¶ä½¿ç”¨å›ºå®šåˆ†åŒºï¼ˆå…³é—­åˆ†åŒºæœç´¢ï¼‰ï¼Œé€Ÿåº¦æå‡100å€
            fast_mode = not enable_tile_search
            import logging

            logger = logging.getLogger(__name__)
            logger.info(f"ğŸ”§ åˆ›å»º GEMM è¯„ä¼°å™¨: enable_tile_search={enable_tile_search}, enable_partition_search={enable_partition_search}, fast_mode={fast_mode}, max_gemm_processes={max_gemm_processes}")
            self.gemm_evaluator = create_gemm_evaluator(self.arch, fast_mode=fast_mode, enable_partition_search=enable_partition_search, max_gemm_processes=max_gemm_processes)
            evaluator_type = self.gemm_evaluator.__class__.__name__
            logger.info(f"âœ… ä½¿ç”¨ GEMM è¯„ä¼°å™¨: {evaluator_type}")

            # ğŸš€ æ‡’åŠ è½½ç­–ç•¥ï¼šä¸é¢„çƒ­ï¼Œè¿è¡Œæ—¶æŒ‰éœ€æœç´¢ï¼ˆå¯¹é½ DS_TPUï¼‰
            # ä¼˜åŠ¿ï¼š
            # - å¯åŠ¨æ—¶é—´ä» 17åˆ†é’Ÿ â†’ 0ç§’
            # - åªæœç´¢å®é™…ç”¨åˆ°çš„å½¢çŠ¶ï¼ˆé¿å…æµªè´¹ï¼‰
            # - å¤šè¿›ç¨‹å¹¶è¡Œæœç´¢ + å…¨å±€ç¼“å­˜å¤ç”¨
            if self.config.enable_gemm_prewarm:
                import logging

                logger = logging.getLogger(__name__)
                logger.info("ğŸš€ GEMM æ‡’åŠ è½½æ¨¡å¼ï¼šé¢„çƒ­å·²ç¦ç”¨ï¼Œå°†æŒ‰éœ€æœç´¢å¹¶ç¼“å­˜")
                # æ³¨ï¼šå¦‚éœ€å¯ç”¨é¢„çƒ­ï¼Œè¯·åœ¨ SimulationConfig ä¸­è®¾ç½® enable_gemm_prewarm=True

            # å…¨å±€è¯„ä¼°ç¼“å­˜ï¼ˆè·¨å±‚å¤ç”¨ï¼‰
            self.eval_cache: dict = {}
        else:
            self.arch = None
            self.gemm_evaluator = None
            self.eval_cache = None
            self.protocol_cfg = None
            self.network_cfg = None

        # è§£ææ‹“æ‰‘ï¼ˆç¡¬ä»¶å‚æ•°ç°åœ¨åµŒå…¥åœ¨æ‹“æ‰‘é…ç½®ä¸­ï¼‰
        self.topo_parser = TopologyParser(topology_dict)
        # éªŒè¯æ‹“æ‰‘ä¸­çš„ç¡¬ä»¶å‚æ•°æ˜¯å¦å®Œæ•´
        self.topo_parser.validate_hardware_params()
        self.interconnect = self.topo_parser.build_interconnect_graph()
        is_moe = model.moe_config is not None
        self.group_assignment = self.topo_parser.map_parallelism(parallelism, is_moe=is_moe)

        # è·å– TP ç»„çš„é“¾è·¯å‚æ•°
        if self.group_assignment.tp_groups and len(self.group_assignment.tp_groups[0]) > 1:
            self.tp_bandwidth, self.tp_latency = self.topo_parser.get_link_params_for_group(self.group_assignment.tp_groups[0], "allreduce")
        else:
            self.tp_bandwidth = hardware.b2b_bandwidth_gbps
            self.tp_latency = hardware.b2b_latency_us

        # è·å– PP ç»„çš„é“¾è·¯å‚æ•°
        if self.group_assignment.pp_groups and len(self.group_assignment.pp_groups[0]) > 1:
            self.pp_bandwidth, self.pp_latency = self.topo_parser.get_link_params_for_group(self.group_assignment.pp_groups[0], "p2p")
        else:
            self.pp_bandwidth = hardware.r2r_bandwidth_gbps
            self.pp_latency = hardware.r2r_latency_us

        # è·å– EP ç»„çš„é“¾è·¯å‚æ•° (MoE Expert Parallelism)
        if self.group_assignment.ep_groups and len(self.group_assignment.ep_groups[0]) > 1:
            self.ep_bandwidth, self.ep_latency = self.topo_parser.get_link_params_for_group(self.group_assignment.ep_groups[0], "alltoall")
        else:
            # é»˜è®¤ä½¿ç”¨ Board å†…å¸¦å®½ (EP é€šå¸¸åœ¨ Board å†…)
            self.ep_bandwidth = hardware.b2b_bandwidth_gbps
            self.ep_latency = hardware.b2b_latency_us

        # ç”˜ç‰¹å›¾æ„å»ºå™¨
        self.gantt_builder = GanttChartBuilder(parallelism)

        # èŠ¯ç‰‡çŠ¶æ€
        self.chip_states: dict[str, ChipState] = {}
        self._init_chip_states()

        # ç»Ÿè®¡
        self.prefill_stats = PhaseTimeStats()
        self.decode_stats = PhaseTimeStats()

        # é“¾è·¯æµé‡ç´¯åŠ å™¨: (source_chip, target_chip) -> {traffic_mb, bandwidth_gbps, latency_us, ...}
        self._link_traffic_accumulator: dict[tuple[str, str], dict[str, Any]] = {}

    def _init_chip_states(self):
        """åˆå§‹åŒ–èŠ¯ç‰‡çŠ¶æ€"""
        for assignment in self.group_assignment.assignments:
            self.chip_states[assignment.chip_id] = ChipState(
                chip_id=assignment.chip_id,
                pp_stage=assignment.pp_rank,
                tp_rank=assignment.tp_rank,
            )

    def _accumulate_link_traffic(
        self,
        source_chip: str,
        target_chip: str,
        traffic_mb: float,
        task_id: str,
        task_type: GanttTaskType,
        bandwidth_gbps: float,
        latency_us: float,
        link_type: str,
    ):
        """ç´¯åŠ é“¾è·¯æµé‡

        Args:
            source_chip: æºèŠ¯ç‰‡ID
            target_chip: ç›®æ ‡èŠ¯ç‰‡ID
            traffic_mb: æµé‡ï¼ˆMBï¼‰
            task_id: ä»»åŠ¡ID
            task_type: ä»»åŠ¡ç±»å‹
            bandwidth_gbps: é“¾è·¯å¸¦å®½ï¼ˆGbpsï¼‰
            latency_us: é“¾è·¯å»¶è¿Ÿï¼ˆå¾®ç§’ï¼‰
            link_type: é“¾è·¯ç±»å‹ï¼ˆc2c/b2b/r2r/p2pï¼‰
        """
        # ä½¿ç”¨æœ‰åºçš„é”®ï¼ˆæŒ‰å­—å…¸åºï¼‰ï¼Œé¿å…é‡å¤è®¡æ•°
        sorted_chips = sorted([source_chip, target_chip])
        key: tuple[str, str] = (sorted_chips[0], sorted_chips[1])

        if key not in self._link_traffic_accumulator:
            self._link_traffic_accumulator[key] = {
                'source': key[0],
                'target': key[1],
                'traffic_mb': 0.0,
                'bandwidth_gbps': bandwidth_gbps,
                'latency_us': latency_us,
                'link_type': link_type,
                'contributing_tasks': [],
                'task_type_breakdown': {}
            }

        acc = self._link_traffic_accumulator[key]
        acc['traffic_mb'] += traffic_mb
        acc['contributing_tasks'].append(task_id)

        task_type_str = task_type.value if isinstance(task_type, GanttTaskType) else str(task_type)
        acc['task_type_breakdown'][task_type_str] = \
            acc['task_type_breakdown'].get(task_type_str, 0.0) + traffic_mb

    def _accumulate_pp_comm_traffic(
        self,
        from_stage: int,
        to_stage: int,
        num_tokens: int,
        task_id: str,
        task_type: GanttTaskType,
    ):
        """ç´¯åŠ  PP é€šä¿¡æµé‡

        Args:
            from_stage: æº PP stage
            to_stage: ç›®æ ‡ PP stage
            num_tokens: Token æ•°é‡
            task_id: ä»»åŠ¡ID
            task_type: ä»»åŠ¡ç±»å‹
        """
        # è®¡ç®—æ•°æ®é‡
        bytes_per_elem = get_bytes_per_element(self.model.dtype)
        data_size_bytes = self.inference.batch_size * num_tokens * self.model.hidden_size * bytes_per_elem
        traffic_mb = data_size_bytes / (1024 ** 2)

        # è·å–æºå’Œç›®æ ‡ stage çš„èŠ¯ç‰‡åˆ—è¡¨
        if from_stage >= len(self.group_assignment.pp_groups) or to_stage >= len(self.group_assignment.pp_groups):
            return

        from_chips = self.group_assignment.pp_groups[from_stage]
        to_chips = self.group_assignment.pp_groups[to_stage]

        # ç´¯åŠ æ¯å¯¹èŠ¯ç‰‡ä¹‹é—´çš„æµé‡
        for from_chip in from_chips:
            for to_chip in to_chips:
                self._accumulate_link_traffic(
                    source_chip=from_chip,
                    target_chip=to_chip,
                    traffic_mb=traffic_mb / (len(from_chips) * len(to_chips)),  # å¹³å‡åˆ†é…
                    task_id=task_id,
                    task_type=task_type,
                    bandwidth_gbps=self.pp_bandwidth,
                    latency_us=self.pp_latency,
                    link_type='pp',
                )

    def _accumulate_tp_comm_traffic(
        self,
        chip_id: str,
        data_size_gb: float,
        task_id: str,
        task_type: GanttTaskType,
    ):
        """ç´¯åŠ  TP é€šä¿¡æµé‡ï¼ˆAllReduceï¼‰

        Args:
            chip_id: å½“å‰èŠ¯ç‰‡IDï¼ˆç”¨äºæŸ¥æ‰¾å…¶æ‰€å±çš„ TP ç»„ï¼‰
            data_size_gb: æ•°æ®é‡ï¼ˆGBï¼‰
            task_id: ä»»åŠ¡ID
            task_type: ä»»åŠ¡ç±»å‹
        """
        import logging
        logger = logging.getLogger(__name__)

        # æŸ¥æ‰¾èŠ¯ç‰‡æ‰€å±çš„ TP ç»„
        tp_chips = None
        tp_group_idx = -1
        for idx, group in enumerate(self.group_assignment.tp_groups):
            if chip_id in group:
                tp_chips = group
                tp_group_idx = idx
                break

        if tp_chips is None or len(tp_chips) <= 1:
            logger.debug(f"èŠ¯ç‰‡ {chip_id} æ—  TP é€šä¿¡ï¼ˆTP ç»„å¤§å° <= 1ï¼‰")
            return

        logger.debug(f"èŠ¯ç‰‡ {chip_id} å±äº TP ç»„ {tp_group_idx}ï¼Œç»„å¤§å°: {len(tp_chips)}")

        # Ring AllReduce: æ¯ä¸ªèŠ¯ç‰‡ä¸ç›¸é‚»èŠ¯ç‰‡é€šä¿¡
        # ç®€åŒ–ï¼šç´¯åŠ ç¯ä¸Šæ‰€æœ‰ç›¸é‚»èŠ¯ç‰‡å¯¹çš„æµé‡
        traffic_mb = data_size_gb * 1024  # GB -> MB
        tp = len(tp_chips)

        # Ring AllReduce ä¸­ï¼Œæ¯æ¡é“¾è·¯ä¼ è¾“ (N-1)/N çš„æ•°æ®é‡ï¼ˆä¸¤ä¸ªæ–¹å‘ï¼‰
        per_link_traffic = traffic_mb * 2 * (tp - 1) / tp / tp

        for i in range(len(tp_chips)):
            next_i = (i + 1) % len(tp_chips)
            self._accumulate_link_traffic(
                source_chip=tp_chips[i],
                target_chip=tp_chips[next_i],
                traffic_mb=per_link_traffic,
                task_id=task_id,
                task_type=task_type,
                bandwidth_gbps=self.tp_bandwidth,
                latency_us=self.tp_latency,
                link_type='tp',
            )

    def _generate_link_traffic_stats(self) -> list:
        """ç”Ÿæˆé“¾è·¯æµé‡ç»Ÿè®¡

        Returns:
            LinkTrafficStats åˆ—è¡¨
        """
        from ..config.types import LinkTrafficStats
        import logging
        logger = logging.getLogger(__name__)

        stats = []

        # è·å–ä»¿çœŸæ€»æ—¶é•¿ï¼ˆä» gantt ä»»åŠ¡ä¸­è®¡ç®—ï¼‰
        if not self.gantt_builder.tasks:
            logger.warning("ğŸ“Š é“¾è·¯æµé‡ç»Ÿè®¡: æ—  Gantt ä»»åŠ¡æ•°æ®")
            return stats

        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        logger.info(f"ğŸ“Š é“¾è·¯æµé‡ç´¯åŠ å™¨: {len(self._link_traffic_accumulator)} æ¡é“¾è·¯")

        total_time_us = max(task.end for task in self.gantt_builder.tasks)
        total_time_s = total_time_us / 1_000_000

        for (source, target), acc in self._link_traffic_accumulator.items():
            # è®¡ç®—åˆ©ç”¨ç‡ = å®é™…æµé‡ / (å¸¦å®½ Ã— æ—¶é—´)
            # å¸¦å®½å•ä½: Gbps -> MBps éœ€è¦ä¹˜ä»¥ 1000 / 8 = 125
            bandwidth_mbps = acc['bandwidth_gbps'] * 125
            max_capacity_mb = bandwidth_mbps * total_time_s
            utilization = (acc['traffic_mb'] / max_capacity_mb) * 100 if max_capacity_mb > 0 else 0

            stats.append(LinkTrafficStats(
                source=acc['source'],
                target=acc['target'],
                traffic_mb=acc['traffic_mb'],
                bandwidth_gbps=acc['bandwidth_gbps'],
                latency_us=acc['latency_us'],
                utilization_percent=min(utilization, 100),
                link_type=acc['link_type'],
                contributing_tasks=acc['contributing_tasks'],
                task_type_breakdown=acc['task_type_breakdown']
            ))

        # æŒ‰æµé‡å¤§å°æ’åº
        stats.sort(key=lambda s: s.traffic_mb, reverse=True)
        return stats

    def _map_compute_op_to_task_type(self, op_type: ComputeOpType, op_name: str = "") -> GanttTaskType:
        """å°†è®¡ç®—ç®—å­ç±»å‹æ˜ å°„åˆ° Gantt ä»»åŠ¡ç±»å‹"""
        if op_type == ComputeOpType.MATMUL:
            # æ ¹æ®ç®—å­åç§°ç»†åˆ†
            if "qkv" in op_name or "q_a" in op_name or "q_b" in op_name or "kv_a" in op_name:
                return GanttTaskType.ATTENTION_QKV
            elif "o_proj" in op_name:
                return GanttTaskType.ATTENTION_OUTPUT
            elif "gate" in op_name:
                return GanttTaskType.FFN_GATE
            elif "up" in op_name:
                return GanttTaskType.FFN_UP
            elif "down" in op_name:
                return GanttTaskType.FFN_DOWN
            else:
                return GanttTaskType.COMPUTE
        elif op_type in (ComputeOpType.MHA, ComputeOpType.MQA, ComputeOpType.FA2):
            return GanttTaskType.ATTENTION_SCORE
        elif op_type == ComputeOpType.RMSNORM:
            return GanttTaskType.LAYERNORM
        elif op_type == ComputeOpType.SOFTMAX:
            return GanttTaskType.ATTENTION_SOFTMAX
        else:
            return GanttTaskType.COMPUTE

    def _map_comm_op_to_task_type(self, comm_kind: str) -> GanttTaskType:
        """å°†é€šä¿¡ç®—å­ç±»å‹æ˜ å°„åˆ° Gantt ä»»åŠ¡ç±»å‹"""
        if comm_kind == "allreduce":
            return GanttTaskType.TP_COMM
        elif comm_kind == "allgather":
            return GanttTaskType.SP_ALLGATHER
        elif comm_kind == "reducescatter":
            return GanttTaskType.SP_REDUCE_SCATTER
        elif comm_kind == "dispatch":
            return GanttTaskType.EP_DISPATCH
        elif comm_kind == "combine":
            return GanttTaskType.EP_COMBINE
        else:
            return GanttTaskType.TP_COMM

    def _build_layer_for_evaluation(self, layer_index: int, num_tokens: int, context_length: int, phase: InferencePhase):
        """
        ä¸ºæŒ‡å®šå±‚æ„å»ºç®—å­å¹¶è¯„ä¼°

        å®Œæ•´æ„å»ºTransformerå±‚ = Attention + FFN (å¯¹é½DS_TPU_1209)

        Args:
            layer_index: å±‚ç´¢å¼•
            num_tokens: å½“å‰å¤„ç†çš„ token æ•°é‡
            context_length: KV cache é•¿åº¦
            phase: æ¨ç†é˜¶æ®µ

        Returns:
            è¯„ä¼°åçš„å±‚å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰ç®—å­çš„æ€§èƒ½æ•°æ®
        """
        from ..layers.base import BaseLayer

        # åˆ¤æ–­å±‚ç±»å‹
        use_mla = self.model.attention_type == "mla" and self.model.mla_config is not None

        # åˆ¤æ–­æ˜¯å¦ä¸º MoE å±‚
        is_moe = self.model.model_type == "moe" and self.model.moe_config is not None and layer_index >= self.model.moe_config.first_k_dense_replace

        # æ„å»ºå±‚é…ç½®
        layer_config = {
            "hidden_dim": self.model.hidden_size,
            "batch_size": self.inference.batch_size,
            "seq_len": num_tokens,
            "kv_seq_len": context_length,
            "tp": self.parallelism.tp,
            "comm_protocol": 1,  # é»˜è®¤åè®®
        }

        # ========== 1. æ„å»ºAttentionå±‚ ==========
        if use_mla and self.model.mla_config:
            # MLA å±‚é…ç½®
            mla = self.model.mla_config
            layer_config.update(
                {
                    "num_heads": self.model.num_attention_heads,
                    "qk_nope_dim": mla.qk_nope_head_dim,
                    "qk_rope_dim": mla.qk_rope_head_dim,
                    "v_head_dim": mla.v_head_dim,
                    "kv_lora_rank": mla.kv_lora_rank,
                    "q_lora_rank": mla.q_lora_rank,
                }
            )

            # ä»æ¨¡å‹é…ç½®è¯»å– MLA å˜ä½“ï¼ˆè€Œéæ¨¡æ‹Ÿé…ç½®ï¼‰
            mla_variant = mla.variant
            if mla_variant == "mla_v32":
                attention_layer = MLAv32Layer(name=f"layer_{layer_index}_mla", config=layer_config)
            elif mla_variant == "mla_absorb":
                attention_layer = MLAAbsorbLayer(name=f"layer_{layer_index}_mla", config=layer_config)
            elif mla_variant == "mla_absorb_v32":
                attention_layer = MLAAbsorbv32Layer(name=f"layer_{layer_index}_mla", config=layer_config)
            else:
                attention_layer = MLALayer(name=f"layer_{layer_index}_mla", config=layer_config)
        else:
            # æ ‡å‡† MHA å±‚
            layer_config.update(
                {
                    "num_heads": self.model.num_attention_heads,
                    "num_kv_heads": self.model.num_kv_heads,
                    "head_dim": self.model.hidden_size // self.model.num_attention_heads,
                }
            )
            attention_layer = MHALayer(name=f"layer_{layer_index}_mha", config=layer_config)

        # ========== 2. æ„å»ºFFNå±‚ ==========
        ffn_config = {
            "hidden_dim": self.model.hidden_size,
            "inter_dim": self.model.intermediate_size,
            "batch_size": self.inference.batch_size,
            "seq_len": num_tokens,
            "tp": self.parallelism.tp,
            "dp": self.parallelism.dp,
            "ep": self.parallelism.ep,
            "comm_protocol": 1,
        }

        if is_moe:
            # MoEå±‚ - éœ€è¦é¢å¤–çš„ moe_tp å‚æ•°
            # ä»æ‹“æ‰‘é…ç½®ä¸­è·å– moe_tpï¼Œå¦‚æœæ²¡æœ‰åˆ™æ ¹æ® MoE çº¦æŸè®¡ç®—
            # MoE çº¦æŸ: DP Ã— TP = MoE_TP Ã— EP
            moe_tp = self.moe_tp
            if moe_tp is None:
                # æ ¹æ®çº¦æŸè®¡ç®—: moe_tp = (dp * tp) / ep
                moe_tp = (self.parallelism.dp * self.parallelism.tp) // self.parallelism.ep if self.parallelism.ep > 0 else 1

            ffn_config.update(
                {
                    "num_experts": self.model.moe_config.num_experts,
                    "num_experts_per_tok": self.model.moe_config.num_experts_per_tok,
                    "expert_intermediate_size": self.model.moe_config.expert_intermediate_size,
                    "moe_tp": moe_tp,
                }
            )
            ffn_layer = MoELayer(name=f"layer_{layer_index}_moe", config=ffn_config)
        else:
            # æ ‡å‡†MLPå±‚
            ffn_layer = MLPLayer(name=f"layer_{layer_index}_mlp", config=ffn_config)

        # ========== 3. åˆå¹¶Attentionå’ŒFFNçš„ç®—å­ ==========
        # åˆ›å»ºç»„åˆå±‚ï¼ŒåŒ…å«å®Œæ•´çš„Transformerå±‚
        combined_layer = BaseLayer(name=f"layer_{layer_index}", layer_type="TransformerLayer")

        # æ·»åŠ Attentionçš„æ‰€æœ‰ç®—å­
        for op in attention_layer.comp_ops:
            combined_layer.add_operator(op)
        for op in attention_layer.comm_ops:
            combined_layer.add_operator(op)

        # æ·»åŠ FFNçš„æ‰€æœ‰ç®—å­
        for op in ffn_layer.comp_ops:
            combined_layer.add_operator(op)
        for op in ffn_layer.comm_ops:
            combined_layer.add_operator(op)

        # ========== 4. è¯„ä¼°æ‰€æœ‰ç®—å­ ==========
        if self.config.use_precise_evaluator and self.arch is not None:
            self._evaluate_layer_operators(combined_layer)

        return combined_layer

    def _evaluate_layer_operators(self, layer):
        """ç›´æ¥è¯„ä¼°å±‚ä¸­çš„æ‰€æœ‰ç®—å­"""
        # å¯¼å…¥è¯„ä¼°å™¨ï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–ï¼‰
        from ..evaluators import (
            GEMMEvaluator,
            FA2Evaluator,
            RMSNormEvaluator,
            AllReduceEval,
            AllGatherEval,
            ReduceScatterEval,
        )

        # ğŸ”‘ ä½¿ç”¨å…¨å±€è¯„ä¼°å™¨ï¼ˆå¤ç”¨ç¼“å­˜ï¼‰
        gemm_eval = self.gemm_evaluator
        fa2_eval = FA2Evaluator(self.arch)
        rmsnorm_eval = RMSNormEvaluator(self.arch)
        # é€šä¿¡è¯„ä¼°å™¨ä½¿ç”¨å‰ç«¯ä¼ é€’çš„é…ç½®
        allreduce_eval = AllReduceEval(self.arch, self.protocol_cfg, self.network_cfg)
        allgather_eval = AllGatherEval(self.arch, self.protocol_cfg, self.network_cfg)
        reducescatter_eval = ReduceScatterEval(self.arch, self.protocol_cfg, self.network_cfg)

        # è¯„ä¼°æ‰€æœ‰è®¡ç®—ç®—å­
        import logging

        logger = logging.getLogger(__name__)

        total_ops = len(layer.comp_ops)
        cached_ops = 0
        evaluated_ops = 0

        for op_idx, op in enumerate(layer.comp_ops):
            cache_key = op.get_cache_key()

            # æ£€æŸ¥ç¼“å­˜
            if cache_key in self.eval_cache:
                op.apply_result(self.eval_cache[cache_key])
                cached_ops += 1
                continue

            # æŠ¥å‘Šè¯¦ç»†è¿›åº¦ï¼ˆæ¯10ä¸ªç®—å­æˆ–æœ€åä¸€ä¸ªï¼‰
            # if (op_idx + 1) % 10 == 0 or (op_idx + 1) == total_ops:
            # logger.info(f"      è¯„ä¼°ç®—å­ {op_idx + 1}/{total_ops} (ç¼“å­˜å‘½ä¸­: {cached_ops}, å·²è¯„ä¼°: {evaluated_ops})")

            # è¯„ä¼°ç®—å­
            if op.operator_type == "MatMulOperator":
                result = gemm_eval.evaluate(
                    G=op.parallel_params.get("G", 1),
                    M=op.parallel_params.get("M", 1),
                    K=op.parallel_params.get("K", 1),
                    N=op.parallel_params.get("N", 1),
                    input_dtype=op.parallel_params.get("input_dtype", "bf16"),
                    output_dtype=op.parallel_params.get("output_dtype", "bf16"),
                    use_multiprocess=True,  # ğŸš€ è¿è¡Œæ—¶å¯ç”¨å¤šè¿›ç¨‹æœç´¢
                )
                op.elapse = result.latency_us
                op.comp_elapse = result.compute_time_us
                op.dma_elapse = result.memory_time_us
                op.dram_traffic = result.dram_traffic_bytes
                op.urate = result.effective_utilization

            elif op.operator_type == "FA2Operator":
                result = fa2_eval.evaluate(
                    B=op.parallel_params.get("B", 1),
                    QS=op.parallel_params.get("QS", 1),
                    KS=op.parallel_params.get("KS", 1),
                    QD=op.parallel_params.get("QD", 1),
                    VD=op.parallel_params.get("VD", 1),
                )
                op.elapse = result.latency_us
                op.comp_elapse = result.compute_time_us
                op.dma_elapse = result.memory_time_us
                op.dram_traffic = result.dram_traffic_bytes
                op.urate = result.effective_utilization

            elif op.operator_type == "MHAOperator":
                # MHA ä½¿ç”¨ FA2 è¯„ä¼°å™¨ï¼Œç­‰æ•ˆ B = B * H
                B = op.parallel_params.get("B", 1)
                H = op.parallel_params.get("H", 1)
                result = fa2_eval.evaluate(
                    B=B * H,
                    QS=op.parallel_params.get("QS", 1),
                    KS=op.parallel_params.get("KS", 1),
                    QD=op.parallel_params.get("QD", 1),
                    VD=op.parallel_params.get("VD", 1),
                )
                op.elapse = result.latency_us
                op.comp_elapse = result.compute_time_us
                op.dma_elapse = result.memory_time_us
                op.dram_traffic = result.dram_traffic_bytes
                op.urate = result.effective_utilization

            elif op.operator_type == "MQAOperator":
                # MQA ä¹Ÿä½¿ç”¨ FA2 è¯„ä¼°å™¨
                result = fa2_eval.evaluate(
                    B=op.parallel_params.get("B", 1),
                    QS=op.parallel_params.get("QS", 1),
                    KS=op.parallel_params.get("KS", 1),
                    QD=op.parallel_params.get("QD", 1),
                    VD=op.parallel_params.get("VD", 1),
                )
                op.elapse = result.latency_us
                op.comp_elapse = result.compute_time_us
                op.dma_elapse = result.memory_time_us
                op.dram_traffic = result.dram_traffic_bytes
                op.urate = result.effective_utilization

            elif op.operator_type == "RMSNormOperator":
                result = rmsnorm_eval.evaluate(
                    batch_size=op.parallel_params.get("batch_size", 1),
                    hidden_dim=op.parallel_params.get("hidden_dim", 1),
                    has_scale=op.parallel_params.get("has_scale", True),
                    has_bias=op.parallel_params.get("has_bias", False),
                )
                # RMSNorm ä¸»è¦å—å¸¦å®½é™åˆ¶
                data_bytes = op.parallel_params.get("batch_size", 1) * op.parallel_params.get("hidden_dim", 1) * 2 * 2
                op.elapse = (data_bytes / self.arch.dram_bandwidth_bytes) * 1e6
                op.comp_elapse = op.elapse * 0.1
                op.dma_elapse = op.elapse * 0.9
                op.dram_traffic = data_bytes
                op.urate = result.utilization

            # ç¼“å­˜ç»“æœ
            self.eval_cache[cache_key] = {
                "elapse": op.elapse,
                "comp_elapse": op.comp_elapse,
                "dma_elapse": op.dma_elapse,
                "dram_traffic": op.dram_traffic,
                "urate": op.urate,
            }

        # è¯„ä¼°æ‰€æœ‰é€šä¿¡ç®—å­
        for op in layer.comm_ops:
            cache_key = op.get_cache_key()

            if cache_key in self.eval_cache:
                op.apply_result(self.eval_cache[cache_key])
                continue

            # è¯„ä¼°é€šä¿¡ç®—å­
            tp = op.parallel_params.get("tp", 1)
            comm_size = op.parallel_params.get("comm_size", 0)
            comm_protocol = op.parallel_params.get("comm_protocol", 1)

            if op.comm_kind == "allreduce":
                result = allreduce_eval.evaluate(tp, comm_size, comm_protocol)
                op.comm_elapse = result.latency_us
            elif op.comm_kind == "allgather":
                result = allgather_eval.evaluate(tp, comm_size, comm_protocol)
                op.comm_elapse = result.latency_us
            elif op.comm_kind == "reducescatter":
                result = reducescatter_eval.evaluate(tp, comm_size, comm_protocol)
                op.comm_elapse = result.latency_us
            else:
                # é»˜è®¤ä½¿ç”¨ç®€å•çš„å¸¦å®½æ¨¡å‹
                op.comm_elapse = (comm_size / self.tp_bandwidth) * 1e6

            # ç¼“å­˜ç»“æœ
            self.eval_cache[cache_key] = {"comm_elapse": op.comm_elapse}

    def _report_progress(self, percent: float, message: str):
        """æŠ¥å‘Šè¿›åº¦"""
        import sys

        print(f"[DEBUG SIMULATOR] _report_progress: percent={percent}, message={message}", flush=True)
        sys.stdout.flush()
        if self.progress_callback:
            try:
                self.progress_callback(percent, message)
            except Exception as e:
                print(f"[DEBUG SIMULATOR] callback error: {e}", flush=True)
                pass  # å¿½ç•¥å›è°ƒé”™è¯¯

    def simulate(self) -> SimulationResult:
        """
        è¿è¡Œå®Œæ•´æ¨¡æ‹Ÿ

        Returns:
            æ¨¡æ‹Ÿç»“æœ
        """
        import logging

        logger = logging.getLogger(__name__)

        wall_start = time.time()
        current_time = 0.0

        # è¿›åº¦åˆ’åˆ†:
        # 0-10%: H2D æ•°æ®ä¼ è¾“
        # 10-50%: Prefill æ¨ç† (æŒ‰å±‚ç»†åˆ†)
        # 50-90%: Decode æ¨ç† (æŒ‰ token ç»†åˆ†)
        # 90-100%: D2H + Gantt + ç»Ÿè®¡

        # é˜¶æ®µ1: æ•°æ®æ¬è¿ (H2D)
        self._report_progress(0, "H2D æ•°æ®ä¼ è¾“...")
        phase_start = time.time()
        if self.config.enable_data_transfer:
            current_time = self._simulate_data_transfer_h2d(current_time)
        h2d_wall_time = (time.time() - phase_start) * 1000
        self._report_progress(10, "H2D å®Œæˆ")

        # é˜¶æ®µ2: Prefill æ¨ç† (10-50%)
        phase_start = time.time()
        prefill_end_time = self._simulate_prefill(current_time, report_progress=True)
        phase_transition = prefill_end_time
        prefill_wall_time = (time.time() - phase_start) * 1000

        # é˜¶æ®µ3: Decode æ¨ç† (50-90%)
        phase_start = time.time()
        decode_end_time = self._simulate_decode(prefill_end_time, report_progress=True)
        decode_wall_time = (time.time() - phase_start) * 1000
        num_tokens = min(self.config.max_simulated_tokens, self.inference.output_seq_length)

        # é˜¶æ®µ4: æ•°æ®æ”¶é›† (D2H)
        self._report_progress(90, "D2H æ•°æ®ä¼ è¾“...")
        phase_start = time.time()
        if self.config.enable_data_transfer:
            final_time = self._simulate_data_transfer_d2h(decode_end_time)
        else:
            final_time = decode_end_time
        d2h_wall_time = (time.time() - phase_start) * 1000

        # æ„å»ºç”˜ç‰¹å›¾
        self._report_progress(93, "æ„å»º Gantt å›¾...")
        phase_start = time.time()
        gantt_data = self.gantt_builder.build(phase_transition=phase_transition)
        gantt_wall_time = (time.time() - phase_start) * 1000

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self._report_progress(96, "è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
        phase_start = time.time()
        stats = self._compute_stats(final_time)
        stats_wall_time = (time.time() - phase_start) * 1000

        total_wall_time = (time.time() - wall_start) * 1000

        # ğŸ“Š æ‰“å° GEMM ç¼“å­˜ç»Ÿè®¡ï¼ˆå¦‚æœä½¿ç”¨äº†ç²¾ç¡®è¯„ä¼°å™¨ï¼‰
        if self.config.use_precise_evaluator and hasattr(self, "gemm_evaluator"):
            logger.info("")  # ç©ºè¡Œåˆ†éš”
            self.gemm_evaluator.print_cache_stats()

        # ğŸ“Š æ‰“å°æ€§èƒ½æ‘˜è¦

        # è®¡ç®—å„é˜¶æ®µæ—¶é—´å æ¯”
        stages = [
            ("H2Dæ•°æ®ä¼ è¾“", h2d_wall_time),
            ("Prefillæ¨ç†", prefill_wall_time),
            ("Decodeæ¨ç†", decode_wall_time),
            ("D2Hæ•°æ®ä¼ è¾“", d2h_wall_time),
            ("Ganttå›¾æ„å»º", gantt_wall_time),
            ("ç»Ÿè®¡è®¡ç®—", stats_wall_time),
        ]

        for stage_name, stage_time in stages:
            percent = (stage_time / total_wall_time * 100) if total_wall_time > 0 else 0
            logger.info(f"   {stage_name:12s}: {stage_time:7.2f}ms ({percent:5.1f}%)")

        logger.info(f"   {'â”€' * 35}")
        logger.info(f"   {'æ€»è®¡':12s}: {total_wall_time:7.2f}ms")

        # è¯†åˆ«ç“¶é¢ˆ
        max_stage = max(stages, key=lambda x: x[1])
        if max_stage[1] > 0:
            logger.info(f"   ğŸ¯ æœ€æ…¢é˜¶æ®µ: {max_stage[0]} ({max_stage[1]:.2f}ms)")

        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

        # ç”Ÿæˆé“¾è·¯æµé‡ç»Ÿè®¡
        link_traffic_stats = self._generate_link_traffic_stats()
        if link_traffic_stats:
            logger.info(f"ğŸ“Š é“¾è·¯æµé‡ç»Ÿè®¡: {len(link_traffic_stats)} æ¡é“¾è·¯")

        return SimulationResult(
            gantt_chart=gantt_data,
            stats=stats,
            link_traffic_stats=link_traffic_stats,
            timestamp=time.time(),
        )

    def _simulate_data_transfer_h2d(self, start_time: float) -> float:
        """æ¨¡æ‹Ÿ Host to Device æ•°æ®ä¼ è¾“"""
        # è®¡ç®—è¾“å…¥æ•°æ®å¤§å°
        bytes_per_elem = get_bytes_per_element(self.model.dtype)
        input_size_gb = (self.inference.batch_size * self.inference.input_seq_length * self.model.hidden_size * bytes_per_elem) / (1024**3)

        # æ•°æ®ä¼ è¾“å»¶è¿Ÿ (ä½¿ç”¨ C2C å¸¦å®½ï¼Œç®€åŒ– Host-Device ä¼ è¾“)
        # å®é™… PCIe å¸¦å®½çº¦ 32-64 GB/sï¼Œä½†å¯¹ LLM æ¨ç†å½±å“å¾ˆå°ï¼Œä½¿ç”¨ C2C å¸¦å®½ç®€åŒ–
        transfer_bw_gbps = self.hardware.c2c_bandwidth_gbps
        transfer_latency_us = self.hardware.c2c_latency_us
        transfer_latency = (input_size_gb / transfer_bw_gbps) * 1000 + transfer_latency_us / 1000  # è½¬æ¢ä¸º ms

        # ä¸ºç¬¬ä¸€ä¸ª PP stage çš„æ‰€æœ‰èŠ¯ç‰‡æ·»åŠ ä¼ è¾“ä»»åŠ¡
        for chip_id, state in self.chip_states.items():
            if state.pp_stage == 0:
                self.gantt_builder.add_task(
                    name="H2D Transfer",
                    start=start_time,
                    end=start_time + transfer_latency,
                    task_type=GanttTaskType.PCIE_H2D,
                    phase=InferencePhase.PREFILL,
                    chip_id=chip_id,
                    pp_stage=0,
                )
                state.compute_idle_at = start_time + transfer_latency

        return start_time + transfer_latency

    def _simulate_data_transfer_d2h(self, start_time: float) -> float:
        """æ¨¡æ‹Ÿ Device to Host æ•°æ®ä¼ è¾“"""
        # è®¡ç®—è¾“å‡ºæ•°æ®å¤§å° (logits)
        bytes_per_elem = get_bytes_per_element(self.model.dtype)
        output_size_gb = (self.inference.batch_size * self.model.vocab_size * bytes_per_elem) / (1024**3)

        # æ•°æ®ä¼ è¾“å»¶è¿Ÿ (ä½¿ç”¨ C2C å¸¦å®½ï¼Œç®€åŒ– Device-Host ä¼ è¾“)
        transfer_bw_gbps = self.hardware.c2c_bandwidth_gbps
        transfer_latency_us = self.hardware.c2c_latency_us
        transfer_latency = (output_size_gb / transfer_bw_gbps) * 1000 + transfer_latency_us / 1000  # è½¬æ¢ä¸º ms

        # ä¸ºæœ€åä¸€ä¸ª PP stage çš„æ‰€æœ‰èŠ¯ç‰‡æ·»åŠ ä¼ è¾“ä»»åŠ¡
        last_stage = self.parallelism.pp - 1
        for chip_id, state in self.chip_states.items():
            if state.pp_stage == last_stage:
                self.gantt_builder.add_task(
                    name="D2H Transfer",
                    start=start_time,
                    end=start_time + transfer_latency,
                    task_type=GanttTaskType.PCIE_D2H,
                    phase=InferencePhase.DECODE,
                    chip_id=chip_id,
                    pp_stage=last_stage,
                )

        return start_time + transfer_latency

    def _simulate_prefill(self, start_time: float, report_progress: bool = False) -> float:
        """æ¨¡æ‹Ÿ Prefill é˜¶æ®µ

        Args:
            start_time: å¼€å§‹æ—¶é—´
            report_progress: æ˜¯å¦æŠ¥å‘Šè¿›åº¦ï¼ˆé»˜è®¤ Falseï¼‰

        Returns:
            Prefill ç»“æŸæ—¶é—´
        """
        import logging
        logger = logging.getLogger(__name__)

        num_tokens = self.inference.input_seq_length
        context_length = self.inference.input_seq_length
        num_layers = self.model.num_layers

        # æ¯ä¸ª PP stage å¤„ç†çš„å±‚æ•°ï¼ˆè‡³å°‘ä¸º 1ï¼Œé˜²æ­¢é™¤é›¶ï¼‰
        layers_per_stage = max(1, num_layers // self.parallelism.pp)

        # ä¸ºæ¯ä¸ª PP stage æ¨¡æ‹Ÿ
        stage_times = [start_time] * self.parallelism.pp

        if report_progress:
            logger.info(f"â”â” å¼€å§‹ Prefill é˜¶æ®µï¼šå…± {num_layers} å±‚ â”â”")

        for layer in range(num_layers):
            layer_wall_start = time.time() if report_progress else None

            # æŠ¥å‘Šè¿›åº¦: 10% + (layer / num_layers) * 40%
            if report_progress:
                progress = 10 + (layer / num_layers) * 40
                layer_progress_msg = f"Prefill Layer {layer + 1}/{num_layers}"
                self._report_progress(progress, layer_progress_msg)
                logger.info(f"")
                logger.info(f"  ğŸ”¹ å¼€å§‹è¯„ä¼° Layer {layer + 1}/{num_layers} (è¿›åº¦: {progress:.1f}%)")

            pp_stage = layer // layers_per_stage
            if pp_stage >= self.parallelism.pp:
                pp_stage = self.parallelism.pp - 1

            layer_in_stage = layer % layers_per_stage

            # è·å–è¯¥ stage çš„ç¬¬ä¸€ä¸ªèŠ¯ç‰‡
            chip_id = self._get_chip_for_stage(pp_stage)
            current_time = stage_times[pp_stage]

            # PP å‰å‘ä¼ é€’ç­‰å¾…ä¸Šä¸€ä¸ª stage
            if pp_stage > 0 and layer_in_stage == 0:
                prev_stage_end = stage_times[pp_stage - 1]
                if prev_stage_end > current_time:
                    # æ·»åŠ æ°”æ³¡
                    bubble_duration = prev_stage_end - current_time
                    self.gantt_builder.add_bubble(
                        start=current_time,
                        duration=bubble_duration,
                        phase=InferencePhase.PREFILL,
                        chip_id=chip_id,
                        pp_stage=pp_stage,
                    )
                    current_time = prev_stage_end

                    # PP P2P é€šä¿¡
                    pp_comm_latency = self._calc_pp_comm_latency(num_tokens)
                    self.gantt_builder.add_comm_task(
                        task_type=GanttTaskType.PP_COMM,
                        start=current_time,
                        duration=pp_comm_latency,
                        phase=InferencePhase.PREFILL,
                        chip_id=chip_id,
                        pp_stage=pp_stage,
                        layer_index=layer,
                    )

                    # ç´¯åŠ  PP é€šä¿¡æµé‡
                    task_id = f"pp_comm_prefill_layer{layer}_stage{pp_stage}"
                    self._accumulate_pp_comm_traffic(
                        from_stage=pp_stage - 1,
                        to_stage=pp_stage,
                        num_tokens=num_tokens,
                        task_id=task_id,
                        task_type=GanttTaskType.PP_COMM,
                    )

                    current_time += pp_comm_latency

            # æ¨¡æ‹Ÿå•å±‚
            current_time = self._simulate_single_layer(
                current_time=current_time,
                layer_index=layer,
                num_tokens=num_tokens,
                context_length=context_length,
                phase=InferencePhase.PREFILL,
                chip_id=chip_id,
                pp_stage=pp_stage,
            )

            stage_times[pp_stage] = current_time

            # æ‰“å°å±‚è¯„ä¼°å¢™ä¸Šæ—¶é—´
            if report_progress and layer_wall_start is not None:
                layer_wall_time = (time.time() - layer_wall_start) * 1000
                logger.info(f"  âœ… Layer {layer + 1}/{num_layers} å®Œæˆï¼Œå¢™ä¸Šæ—¶é—´: {layer_wall_time:.2f}ms")

        # è¿”å›æœ€åä¸€ä¸ª stage çš„ç»“æŸæ—¶é—´
        prefill_end = max(stage_times)

        # æ›´æ–°ç»Ÿè®¡
        self.prefill_stats.total_time = prefill_end - start_time

        if report_progress:
            self._report_progress(50, "Prefill å®Œæˆ")

        return prefill_end

    def _simulate_decode(self, start_time: float, report_progress: bool = False) -> float:
        """æ¨¡æ‹Ÿ Decode é˜¶æ®µ

        Args:
            start_time: å¼€å§‹æ—¶é—´
            report_progress: æ˜¯å¦æŠ¥å‘Šè¿›åº¦ï¼ˆé»˜è®¤ Falseï¼‰

        Returns:
            Decode ç»“æŸæ—¶é—´
        """
        import logging
        logger = logging.getLogger(__name__)

        current_time = start_time
        num_tokens_to_simulate = min(self.config.max_simulated_tokens, self.inference.output_seq_length)

        # æ¯ä¸ª PP stage å¤„ç†çš„å±‚æ•°ï¼ˆè‡³å°‘ä¸º 1ï¼Œé˜²æ­¢é™¤é›¶ï¼‰
        layers_per_stage = max(1, self.model.num_layers // self.parallelism.pp)

        for token_idx in range(num_tokens_to_simulate):
            # æŠ¥å‘Šè¿›åº¦: 50% + (token_idx / num_tokens) * 40%
            if report_progress:
                progress = 50 + (token_idx / num_tokens_to_simulate) * 40
                self._report_progress(progress, f"Decode Token {token_idx + 1}/{num_tokens_to_simulate}")

            token_wall_start = time.time()
            context_length = self.inference.input_seq_length + token_idx + 1
            stage_times = [current_time] * self.parallelism.pp

            for layer in range(self.model.num_layers):
                pp_stage = layer // layers_per_stage
                if pp_stage >= self.parallelism.pp:
                    pp_stage = self.parallelism.pp - 1

                layer_in_stage = layer % layers_per_stage
                chip_id = self._get_chip_for_stage(pp_stage)
                layer_start = stage_times[pp_stage]

                # PP ç­‰å¾…
                if pp_stage > 0 and layer_in_stage == 0:
                    prev_end = stage_times[pp_stage - 1]
                    if prev_end > layer_start:
                        bubble = prev_end - layer_start
                        self.gantt_builder.add_bubble(
                            start=layer_start,
                            duration=bubble,
                            phase=InferencePhase.DECODE,
                            chip_id=chip_id,
                            pp_stage=pp_stage,
                        )
                        layer_start = prev_end

                        pp_comm = self._calc_pp_comm_latency(1)
                        self.gantt_builder.add_comm_task(
                            task_type=GanttTaskType.PP_COMM,
                            start=layer_start,
                            duration=pp_comm,
                            phase=InferencePhase.DECODE,
                            chip_id=chip_id,
                            pp_stage=pp_stage,
                            layer_index=layer,
                            token_index=token_idx,
                        )

                        # ç´¯åŠ  PP é€šä¿¡æµé‡
                        task_id = f"pp_comm_decode_token{token_idx}_layer{layer}_stage{pp_stage}"
                        self._accumulate_pp_comm_traffic(
                            from_stage=pp_stage - 1,
                            to_stage=pp_stage,
                            num_tokens=1,
                            task_id=task_id,
                            task_type=GanttTaskType.PP_COMM,
                        )

                        layer_start += pp_comm

                # æ¨¡æ‹Ÿå•å±‚ (Decode: 1 token)
                layer_end = self._simulate_single_layer(
                    current_time=layer_start,
                    layer_index=layer,
                    num_tokens=1,
                    context_length=context_length,
                    phase=InferencePhase.DECODE,
                    chip_id=chip_id,
                    pp_stage=pp_stage,
                    token_index=token_idx,
                )

                stage_times[pp_stage] = layer_end

            current_time = max(stage_times)

            # ğŸ“Š æ¯ä¸ªtokençš„æ€§èƒ½æ—¥å¿—
            token_wall_time = (time.time() - token_wall_start) * 1000
            logger.info(f"    ğŸ”¹ Token {token_idx}/{num_tokens_to_simulate}: å¢™ä¸Šæ—¶é—´ {token_wall_time:.2f}ms, éå†äº† {self.model.num_layers} å±‚")

        # æ›´æ–°ç»Ÿè®¡
        self.decode_stats.total_time = current_time - start_time

        if report_progress:
            self._report_progress(90, "Decode å®Œæˆ")

        return current_time

    def _simulate_single_layer(
        self,
        current_time: float,
        layer_index: int,
        num_tokens: int,
        context_length: int,
        phase: InferencePhase,
        chip_id: str,
        pp_stage: int,
        token_index: int | None = None,
    ) -> float:
        """æ¨¡æ‹Ÿå•å±‚ Transformer"""

        # ä½¿ç”¨æ–°çš„ç²¾ç¡®è¯„ä¼°å™¨
        if self.config.use_precise_evaluator and self.arch is not None:
            return self._simulate_single_layer_precise(current_time, layer_index, num_tokens, context_length, phase, chip_id, pp_stage, token_index)

        # å›é€€åˆ°ç®€åŒ–æ¨¡æ‹Ÿï¼ˆç²—ç²’åº¦ï¼‰
        return self._simulate_single_layer_coarse(current_time, layer_index, num_tokens, context_length, phase, chip_id, pp_stage, token_index)

    def _simulate_single_layer_precise(
        self,
        current_time: float,
        layer_index: int,
        num_tokens: int,
        context_length: int,
        phase: InferencePhase,
        chip_id: str,
        pp_stage: int,
        token_index: int | None = None,
    ) -> float:
        """ä½¿ç”¨ç²¾ç¡®è¯„ä¼°å™¨æ¨¡æ‹Ÿå•å±‚ï¼ˆåŸºäºç®—å­ï¼‰"""

        # æ„å»ºå¹¶è¯„ä¼°å±‚
        layer_wall_start = time.time()
        layer = self._build_layer_for_evaluation(layer_index, num_tokens, context_length, phase)
        build_time = (time.time() - layer_wall_start) * 1000

        # æ ¹æ®è¯„ä¼°ç²’åº¦å†³å®šæ˜¯å¦å±•å¼€æ‰€æœ‰ç®—å­
        gantt_wall_start = time.time()
        if self.config.evaluation_granularity == "fine":
            # æ£€æŸ¥æ˜¯å¦ä¸º MoE å±‚ä¸”å¯ç”¨äº† TBO ä¼˜åŒ–
            from ..layers import MoELayer

            if self.config.enable_tbo and isinstance(layer, MoELayer):
                # TBO æ¨¡å¼: æ ‡è®°è¢«é‡å éšè—çš„é€šä¿¡ç®—å­
                dispatch_lat = layer._get_operator_latency("dispatch")
                combine_lat = layer._get_operator_latency("combine")

                routed_gate_lat = layer._get_operator_latency("routed_gate")
                routed_up_lat = layer._get_operator_latency("routed_up")
                routed_down_lat = layer._get_operator_latency("routed_down")
                routed_allreduce_lat = layer._get_operator_latency("routed_allreduce")
                routed_compute_lat = routed_gate_lat + routed_up_lat + routed_down_lat + routed_allreduce_lat

                shared_gate_lat = layer._get_operator_latency("shared_gate")
                shared_up_lat = layer._get_operator_latency("shared_up")
                shared_down_lat = layer._get_operator_latency("shared_down")
                shared_allreduce_lat = layer._get_operator_latency("shared_allreduce")
                shared_compute_lat = shared_gate_lat + shared_up_lat + shared_down_lat + shared_allreduce_lat

                # è®¡ç®—è¢«éšè—çš„å»¶è¿Ÿ
                dispatch_hidden = min(dispatch_lat, routed_compute_lat)
                if shared_compute_lat > 0:
                    combine_hidden = min(combine_lat, shared_compute_lat)
                else:
                    combine_hidden = min(combine_lat, routed_compute_lat)

                # éå†æ‰€æœ‰è®¡ç®—ç®—å­ (æ­£å¸¸æ·»åŠ )
                for op in layer.comp_ops:
                    task_type = self._map_compute_op_to_task_type(op.op_type, op.name)
                    latency_ms = op.elapse / 1000

                    # æ„é€ è¯¦ç»†ä¿¡æ¯å­—å…¸
                    extra_fields = {
                        "flops": op.flops,
                        "params_bytes": op.param,
                        "dram_occupy_bytes": op.dram_occupy,
                        "dram_traffic_bytes": op.dram_traffic,
                        "compute_time_us": op.comp_elapse,
                        "memory_time_us": op.dma_elapse,
                        "arch_utilization": op.urate,
                        "parallel_config": {
                            "tp": self.parallelism.tp,
                            "dp": self.parallelism.dp,
                            "pp": self.parallelism.pp,
                            "ep": self.parallelism.ep,
                            "sp": self.parallelism.sp,
                        },
                    }

                    # æ·»åŠ  GEMM ä¼˜åŒ–ç»“æœ
                    if op.best_tile is not None:
                        extra_fields["best_tile"] = op.best_tile
                    if op.best_partition is not None:
                        extra_fields["best_partition"] = op.best_partition
                    if hasattr(op, "parallel_params") and op.parallel_params:
                        extra_fields["gemm_shape"] = {
                            "G": op.parallel_params.get("G"),
                            "M": op.parallel_params.get("M"),
                            "K": op.parallel_params.get("K"),
                            "N": op.parallel_params.get("N"),
                        }

                    self.gantt_builder.add_compute_task(task_type, current_time, latency_ms, phase, chip_id, pp_stage, layer_index, token_index, **extra_fields)
                    current_time += latency_ms

                # éå†é€šä¿¡ç®—å­ (åº”ç”¨ TBO é‡å )
                for op in layer.comm_ops:
                    task_type = self._map_comm_op_to_task_type(op.comm_kind)
                    latency_ms = op.comm_elapse / 1000

                    # å¦‚æœæ˜¯ dispatch æˆ– combineï¼Œå‡å»è¢«éšè—çš„éƒ¨åˆ†
                    if op.name.endswith("dispatch") and dispatch_hidden > 0:
                        effective_latency_ms = max(0, latency_ms - dispatch_hidden / 1000)
                    elif op.name.endswith("combine") and combine_hidden > 0:
                        effective_latency_ms = max(0, latency_ms - combine_hidden / 1000)
                    else:
                        effective_latency_ms = latency_ms

                    if effective_latency_ms > 0:
                        # æ¨æ–­é€šä¿¡ç»„å¤§å°
                        comm_group_size = 1
                        if "tp" in op.comm_kind or "allreduce" in op.comm_kind.lower():
                            comm_group_size = self.parallelism.tp
                        elif "dp" in op.comm_kind:
                            comm_group_size = self.parallelism.dp
                        elif "ep" in op.comm_kind or "dispatch" in op.comm_kind or "combine" in op.comm_kind:
                            comm_group_size = self.parallelism.ep
                        elif "sp" in op.comm_kind:
                            comm_group_size = self.parallelism.sp

                        # æ„é€ é€šä¿¡è¯¦ç»†ä¿¡æ¯
                        comm_extra = {
                            "comm_size_bytes": op.comm_size,
                            "comm_time_us": op.comm_elapse,
                            "comm_algorithm": op.parallel_params.get("algorithm", "unknown"),
                            "comm_group_size": comm_group_size,
                            "parallel_config": {
                                "tp": self.parallelism.tp,
                                "dp": self.parallelism.dp,
                                "pp": self.parallelism.pp,
                                "ep": self.parallelism.ep,
                                "sp": self.parallelism.sp,
                            },
                        }

                        self.gantt_builder.add_comm_task(task_type, current_time, effective_latency_ms, phase, chip_id, pp_stage, layer_index, token_index, **comm_extra)
                        current_time += effective_latency_ms
            else:
                # æ ‡å‡†æ¨¡å¼: ç»†ç²’åº¦éå†æ‰€æœ‰ç®—å­
                for op in layer.comp_ops:
                    task_type = self._map_compute_op_to_task_type(op.op_type, op.name)
                    latency_ms = op.elapse / 1000

                    # æ„é€ è¯¦ç»†ä¿¡æ¯å­—å…¸
                    extra_fields = {
                        "flops": op.flops,
                        "params_bytes": op.param,
                        "dram_occupy_bytes": op.dram_occupy,
                        "dram_traffic_bytes": op.dram_traffic,
                        "compute_time_us": op.comp_elapse,
                        "memory_time_us": op.dma_elapse,
                        "arch_utilization": op.urate,
                        "parallel_config": {
                            "tp": self.parallelism.tp,
                            "dp": self.parallelism.dp,
                            "pp": self.parallelism.pp,
                            "ep": self.parallelism.ep,
                            "sp": self.parallelism.sp,
                        },
                    }

                    # æ·»åŠ  GEMM ä¼˜åŒ–ç»“æœ
                    if op.best_tile is not None:
                        extra_fields["best_tile"] = op.best_tile
                    if op.best_partition is not None:
                        extra_fields["best_partition"] = op.best_partition
                    if hasattr(op, "parallel_params") and op.parallel_params:
                        extra_fields["gemm_shape"] = {
                            "G": op.parallel_params.get("G"),
                            "M": op.parallel_params.get("M"),
                            "K": op.parallel_params.get("K"),
                            "N": op.parallel_params.get("N"),
                        }

                    self.gantt_builder.add_compute_task(task_type, current_time, latency_ms, phase, chip_id, pp_stage, layer_index, token_index, **extra_fields)
                    current_time += latency_ms

                # éå†æ‰€æœ‰é€šä¿¡ç®—å­
                for op in layer.comm_ops:
                    task_type = self._map_comm_op_to_task_type(op.comm_kind)
                    latency_ms = op.comm_elapse / 1000

                    # æ¨æ–­é€šä¿¡ç»„å¤§å°
                    comm_group_size = 1
                    if "tp" in op.comm_kind or "allreduce" in op.comm_kind.lower():
                        comm_group_size = self.parallelism.tp
                    elif "dp" in op.comm_kind:
                        comm_group_size = self.parallelism.dp
                    elif "ep" in op.comm_kind or "dispatch" in op.comm_kind or "combine" in op.comm_kind:
                        comm_group_size = self.parallelism.ep
                    elif "sp" in op.comm_kind:
                        comm_group_size = self.parallelism.sp

                    # æ„é€ é€šä¿¡è¯¦ç»†ä¿¡æ¯
                    comm_extra = {
                        "comm_size_bytes": op.comm_size,
                        "comm_time_us": op.comm_elapse,
                        "comm_algorithm": op.parallel_params.get("algorithm", "unknown"),
                        "comm_group_size": comm_group_size,
                        "parallel_config": {
                            "tp": self.parallelism.tp,
                            "dp": self.parallelism.dp,
                            "pp": self.parallelism.pp,
                            "ep": self.parallelism.ep,
                            "sp": self.parallelism.sp,
                        },
                    }

                    self.gantt_builder.add_comm_task(task_type, current_time, latency_ms, phase, chip_id, pp_stage, layer_index, token_index, **comm_extra)

                    # ç´¯åŠ  TP é€šä¿¡æµé‡ï¼ˆAllReduceï¼‰
                    if "tp" in op.comm_kind or "allreduce" in op.comm_kind.lower():
                        data_size_gb = op.comm_size / (1024 ** 3)
                        task_id_comm = f"tp_comm_{phase.value}_layer{layer_index}_token{token_index}_{chip_id}"
                        import logging
                        logger = logging.getLogger(__name__)
                        logger.debug(f"ç´¯åŠ  TP æµé‡: {data_size_gb:.4f} GB, chip={chip_id}")
                        self._accumulate_tp_comm_traffic(
                            chip_id=chip_id,
                            data_size_gb=data_size_gb,
                            task_id=task_id_comm,
                            task_type=task_type,
                        )

                    current_time += latency_ms
        else:
            # ç²—ç²’åº¦ï¼šèšåˆæ•´å±‚
            # æ£€æŸ¥æ˜¯å¦ä¸º MoE å±‚ä¸”å¯ç”¨äº† TBO ä¼˜åŒ–
            from ..layers import MoELayer

            if self.config.enable_tbo and isinstance(layer, MoELayer):
                # ä½¿ç”¨ TBO ä¼˜åŒ–è®¡ç®—å»¶è¿Ÿ
                total_layer_time = layer.calculate_latency_with_tbo() / 1000  # us -> ms

                # æ·»åŠ èšåˆä»»åŠ¡åˆ°ç”˜ç‰¹å›¾
                if total_layer_time > 0:
                    self.gantt_builder.add_compute_task(GanttTaskType.MOE_EXPERT, current_time, total_layer_time, phase, chip_id, pp_stage, layer_index, token_index)
                    current_time += total_layer_time
            else:
                # æ ‡å‡†æ¨¡å¼ï¼šç®€å•æ±‚å’Œ
                total_compute_time = sum(op.elapse for op in layer.comp_ops) / 1000
                total_comm_time = sum(op.comm_elapse for op in layer.comm_ops) / 1000

                if total_compute_time > 0:
                    self.gantt_builder.add_compute_task(GanttTaskType.COMPUTE, current_time, total_compute_time, phase, chip_id, pp_stage, layer_index, token_index)
                    current_time += total_compute_time

                if total_comm_time > 0:
                    self.gantt_builder.add_comm_task(GanttTaskType.TP_COMM, current_time, total_comm_time, phase, chip_id, pp_stage, layer_index, token_index)
                    current_time += total_comm_time

        gantt_time = (time.time() - gantt_wall_start) * 1000

        # ğŸ“Š æ€§èƒ½æ—¥å¿—ï¼ˆæ‰“å°å‰3å±‚çš„è¯¦ç»†timingï¼Œæˆ–decodeç¬¬ä¸€ä¸ªtokençš„æ‰€æœ‰å±‚ï¼‰
        import logging

        logger = logging.getLogger(__name__)

        # æ¡ä»¶1: Prefillé˜¶æ®µçš„å‰3å±‚
        # æ¡ä»¶2: Decodeç¬¬ä¸€ä¸ªtokençš„å‰3å±‚
        # æ¡ä»¶3: å¦‚æœç¯å¢ƒå˜é‡è®¾ç½®äº†è¯¦ç»†æ—¥å¿—ï¼Œæ‰“å°æ‰€æœ‰å±‚
        import os

        verbose_logging = os.environ.get("GEMM_VERBOSE_LOGGING", "0") == "1"

        should_log = False
        if phase == InferencePhase.PREFILL and layer_index < 3:
            should_log = True
        elif phase == InferencePhase.DECODE and token_index == 0 and layer_index < 3:
            should_log = True
        elif verbose_logging:
            should_log = True

        if should_log:
            logger.info(f"      ğŸ”¸ [{phase.value}] å±‚{layer_index}: build={build_time:.2f}ms, gantt={gantt_time:.2f}ms, ops={len(layer.comp_ops)}+{len(layer.comm_ops)}")

        return current_time

    def _simulate_single_layer_coarse(
        self,
        current_time: float,
        layer_index: int,
        num_tokens: int,
        context_length: int,
        phase: InferencePhase,
        chip_id: str,
        pp_stage: int,
        token_index: int | None = None,
    ) -> float:
        """ç²—ç²’åº¦æ¨¡æ‹Ÿå•å±‚ï¼ˆç®€åŒ–å…¬å¼ï¼Œç”¨äº fallbackï¼‰"""

        # ç®€åŒ–è®¡ç®—ï¼šä½¿ç”¨å›ºå®šå…¬å¼ä¼°ç®—
        # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸ç²—ç•¥çš„ä¼°ç®—ï¼Œä»…ä½œä¸º fallback

        bytes_per_elem = get_bytes_per_element(self.model.dtype)
        hidden_size = self.model.hidden_size

        # Attention éƒ¨åˆ†å»¶è¿Ÿä¼°ç®—ï¼ˆç®€åŒ–ï¼‰
        # QKVæŠ•å½± + Scoreè®¡ç®— + OutputæŠ•å½±
        qkv_size = hidden_size * hidden_size * 3
        qkv_flops = 2 * num_tokens * qkv_size
        attn_score_flops = 2 * num_tokens * context_length * hidden_size
        compute_tflops = self.hardware.compute_tflops_bf16 * 1e12
        attn_latency_ms = (qkv_flops + attn_score_flops) / compute_tflops * 1000

        # FFN éƒ¨åˆ†å»¶è¿Ÿä¼°ç®—
        intermediate_size = self.model.intermediate_size
        ffn_flops = 2 * num_tokens * hidden_size * intermediate_size * 3  # gate, up, down
        ffn_latency_ms = ffn_flops / compute_tflops * 1000

        total_compute_ms = attn_latency_ms + ffn_latency_ms

        self.gantt_builder.add_compute_task(GanttTaskType.COMPUTE, current_time, total_compute_ms, phase, chip_id, pp_stage, layer_index, token_index)
        current_time += total_compute_ms

        # TP é€šä¿¡
        if self.parallelism.tp > 1:
            tp_comm_latency = self._calc_tp_allreduce_latency(num_tokens)
            self.gantt_builder.add_comm_task(GanttTaskType.TP_COMM, current_time, tp_comm_latency, phase, chip_id, pp_stage, layer_index, token_index)

            # ç´¯åŠ  TP é€šä¿¡æµé‡
            bytes_per_elem = get_bytes_per_element(self.model.dtype)
            data_size_bytes = self.inference.batch_size * num_tokens * self.model.hidden_size * bytes_per_elem
            data_size_gb = data_size_bytes / (1024 ** 3)
            task_id_tp = f"tp_comm_coarse_{phase.value}_layer{layer_index}_token{token_index}_{chip_id}"
            self._accumulate_tp_comm_traffic(
                chip_id=chip_id,
                data_size_gb=data_size_gb,
                task_id=task_id_tp,
                task_type=GanttTaskType.TP_COMM,
            )

            current_time += tp_comm_latency

        return current_time

    def _calc_tp_allreduce_latency(self, num_tokens: int) -> float:
        """è®¡ç®— TP AllReduce å»¶è¿Ÿï¼ˆRing AllReduce ç®—æ³•ï¼‰"""
        bytes_per_elem = get_bytes_per_element(self.model.dtype)
        data_size_gb = (self.inference.batch_size * num_tokens * self.model.hidden_size * bytes_per_elem) / (1024**3)

        # Ring AllReduce: 2 * (N-1) / N * data_size / bandwidth + latency
        tp = self.parallelism.tp
        if tp <= 1:
            return 0.0

        transfer_time = 2 * (tp - 1) / tp * data_size_gb / self.tp_bandwidth * 1000  # ms
        latency_overhead = self.tp_latency / 1000  # us -> ms
        return transfer_time + latency_overhead

    def _calc_pp_comm_latency(self, num_tokens: int) -> float:
        """è®¡ç®— PP P2P é€šä¿¡å»¶è¿Ÿ"""
        bytes_per_elem = get_bytes_per_element(self.model.dtype)
        data_size_gb = (self.inference.batch_size * num_tokens * self.model.hidden_size * bytes_per_elem) / (1024**3)

        # P2P: data_size / bandwidth + latency
        transfer_time = data_size_gb / self.pp_bandwidth * 1000  # ms
        latency_overhead = self.pp_latency / 1000  # us -> ms
        return transfer_time + latency_overhead

    def _calc_sp_allgather_latency(self, num_tokens: int) -> float:
        """è®¡ç®— SP AllGather å»¶è¿Ÿ"""
        if self.parallelism.sp <= 1:
            return 0.0

        # è®¡ç®—æ•°æ®é‡
        bytes_per_elem = get_bytes_per_element(self.model.dtype)
        data_size_gb = (self.inference.batch_size * num_tokens * self.model.hidden_size * bytes_per_elem) / (1024**3)

        # AllGather: (N-1) / N * data_size / bandwidth + latency
        sp = self.parallelism.sp
        transfer_time = (sp - 1) / sp * data_size_gb / self.tp_bandwidth * 1000
        latency_overhead = self.tp_latency / 1000
        return transfer_time + latency_overhead

    def _calc_sp_reduce_scatter_latency(self, num_tokens: int) -> float:
        """è®¡ç®— SP ReduceScatter å»¶è¿Ÿ"""
        if self.parallelism.sp <= 1:
            return 0.0

        # è®¡ç®—æ•°æ®é‡
        bytes_per_elem = get_bytes_per_element(self.model.dtype)
        data_size_gb = (self.inference.batch_size * num_tokens * self.model.hidden_size * bytes_per_elem) / (1024**3)

        # ReduceScatter: (N-1) / N * data_size / bandwidth + latency
        sp = self.parallelism.sp
        transfer_time = (sp - 1) / sp * data_size_gb / self.tp_bandwidth * 1000
        latency_overhead = self.tp_latency / 1000
        return transfer_time + latency_overhead

    def _get_chip_for_stage(self, pp_stage: int) -> str:
        """è·å–æŒ‡å®š PP stage çš„ç¬¬ä¸€ä¸ªèŠ¯ç‰‡ID"""
        for assignment in self.group_assignment.assignments:
            if assignment.pp_rank == pp_stage:
                return assignment.chip_id
        raise ValueError(f"æ‰¾ä¸åˆ° PP stage {pp_stage} çš„èŠ¯ç‰‡")

    def _compute_stats(self, total_time: float) -> SimulationStats:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        # TTFT = Prefill æ€»æ—¶é—´
        ttft = self.prefill_stats.total_time

        # å¹³å‡ TPOT
        num_decode_tokens = min(self.config.max_simulated_tokens, self.inference.output_seq_length)
        avg_tpot = self.decode_stats.total_time / num_decode_tokens if num_decode_tokens > 0 else 0.0

        # è®¡ç®— MFU (ç®€åŒ–ç‰ˆæœ¬)
        bytes_per_elem = get_bytes_per_element(self.model.dtype)

        # Prefill é˜¶æ®µ MFU
        # MFU = å®é™… FLOPs/s / å³°å€¼ FLOPs/s
        # æ³¨æ„: prefill_flops æ˜¯å•ä¸ª DP å‰¯æœ¬çš„ FLOPs (ä¸éœ€è¦ä¹˜ DP)
        # peak_tflops åº”è¯¥æ˜¯å•ä¸ª DP å‰¯æœ¬ä½¿ç”¨çš„èŠ¯ç‰‡æ€»ç®—åŠ› (tp * pp)
        prefill_flops = self._calc_total_flops(self.inference.input_seq_length)
        prefill_mfu = 0.0
        if self.prefill_stats.total_time > 0:
            # æ—¶é—´å•ä½: ms -> s
            time_s = self.prefill_stats.total_time / 1000
            achieved_tflops = (prefill_flops / 1e12) / time_s

            # å• DP å‰¯æœ¬çš„å³°å€¼ç®—åŠ› (tp * pp ä¸ªèŠ¯ç‰‡)
            # æ³¨æ„: ä¸ä¹˜ dpï¼Œå› ä¸ºæ¯ä¸ª dp å‰¯æœ¬ç‹¬ç«‹è®¡ç®—ç›¸åŒ FLOPs
            chips_per_replica = self.parallelism.tp * self.parallelism.pp
            peak_tflops = self.hardware.compute_tflops_bf16 * chips_per_replica

            prefill_mfu = achieved_tflops / peak_tflops

        # Decode é˜¶æ®µ MBU (å†…å­˜å¸¦å®½åˆ©ç”¨ç‡)
        # MBU = å®é™…å¸¦å®½éœ€æ±‚ / å³°å€¼å¸¦å®½
        # å®é™…å¸¦å®½éœ€æ±‚ = (æ¨¡å‹æƒé‡ + KV Cache) / TPOT
        decode_mbu = 0.0
        if num_decode_tokens > 0 and avg_tpot > 0:
            # æ¨¡å‹æƒé‡å¤§å°
            model_size_gb = self._calc_model_size_gb()

            # KV Cache å¤§å° (å¹³å‡ context é•¿åº¦)
            avg_context = self.inference.input_seq_length + num_decode_tokens // 2
            kv_cache_gb = self._calc_kv_cache_size_gb(avg_context)

            # æ€»æ•°æ®é‡
            data_read_gb = model_size_gb + kv_cache_gb

            # å®é™…å¸¦å®½éœ€æ±‚ (GB/s)
            required_bandwidth = data_read_gb / (avg_tpot / 1000)

            # å³°å€¼å¸¦å®½ (è€ƒè™‘ HBM æ•ˆç‡ 85%)
            peak_bandwidth = self.hardware.memory_bandwidth_gbps * self.hardware.memory_bandwidth_utilization
            decode_mbu = required_bandwidth / peak_bandwidth

        return SimulationStats(
            prefill=self.prefill_stats,
            decode=self.decode_stats,
            total_run_time=total_time,
            simulated_tokens=num_decode_tokens,
            ttft=ttft,
            avg_tpot=avg_tpot,
            dynamic_mfu=min(prefill_mfu, 1.0),
            dynamic_mbu=min(decode_mbu, 1.0),
            max_pp_bubble_ratio=0.0,  # TODO: è®¡ç®—æ°”æ³¡æ¯”
            total_events=len(self.gantt_builder.tasks),
            prefill_flops=prefill_flops,
        )

    def _calc_total_flops(self, seq_length: int) -> float:
        """
        è®¡ç®—æ€» FLOPs

        æ ‡å‡† Transformer FLOPs è®¡ç®—:
        - QKV Projection: 2 * B * S * H * (H + 2 * kv_heads * head_dim)  (è€ƒè™‘ GQA)
        - Attention Score: 2 * B * n_heads * S * S * head_dim
        - Attention Output: 2 * B * n_heads * S * S * head_dim + 2 * B * S * H * H
        - FFN: 3 * 2 * B * S * H * I (gate, up, down)
        - LM Head: 2 * B * S * H * V

        ç®€åŒ–å…¬å¼: çº¦ç­‰äº 2 * num_params * seq_length
        """
        B = self.inference.batch_size
        S = seq_length
        H = self.model.hidden_size
        L = self.model.num_layers
        I = self.model.intermediate_size
        V = self.model.vocab_size
        n_heads = self.model.num_attention_heads
        kv_heads = self.model.num_kv_heads
        head_dim = H // n_heads

        # QKV Projection (è€ƒè™‘ GQA)
        qkv_flops = 2 * B * S * H * (H + 2 * kv_heads * head_dim) * L

        # Attention Score: Q @ K^T
        score_flops = 2 * B * n_heads * S * S * head_dim * L

        # Attention Output: Softmax @ V + Output Projection
        output_flops = (2 * B * n_heads * S * S * head_dim + 2 * B * S * H * H) * L

        # FFN: gate, up, down
        ffn_flops = 2 * B * S * H * I * 3 * L

        # LM Head
        lm_head_flops = 2 * B * S * H * V

        return qkv_flops + score_flops + output_flops + ffn_flops + lm_head_flops

    def _calc_model_size_gb(self) -> float:
        """è®¡ç®—æ¨¡å‹å¤§å° (GB)

        æ”¯æŒ:
        - MLA (Multi-head Latent Attention) vs æ ‡å‡† Attention
        - MoE (Mixture of Experts) vs Dense FFN
        """
        bytes_per_elem = get_bytes_per_element(self.model.dtype)
        H = self.model.hidden_size
        L = self.model.num_layers
        I = self.model.intermediate_size
        V = self.model.vocab_size
        num_heads = self.model.num_attention_heads
        num_kv_heads = self.model.num_kv_heads

        # === Attention å‚æ•° ===
        if self.model.mla_config is not None:
            # MLA å‚æ•° (DeepSeek-V3)
            mla = self.model.mla_config
            head_dim = mla.qk_nope_head_dim + mla.qk_rope_head_dim

            # Q path: W_DQ (H Ã— q_lora_rank) + W_UQ (q_lora_rank Ã— num_heads Ã— head_dim)
            # + W_QR (q_lora_rank Ã— qk_rope_head_dim Ã— num_heads)
            q_down_params = H * mla.q_lora_rank
            q_up_params = mla.q_lora_rank * num_heads * head_dim
            q_rope_params = mla.q_lora_rank * mla.qk_rope_head_dim * num_heads

            # KV path: W_DKV (H Ã— kv_lora_rank) + W_UK (kv_lora_rank Ã— num_heads Ã— head_dim)
            # + W_UV (kv_lora_rank Ã— num_heads Ã— v_head_dim) + W_KR (H Ã— qk_rope_head_dim)
            kv_down_params = H * mla.kv_lora_rank
            k_up_params = mla.kv_lora_rank * num_heads * mla.qk_nope_head_dim
            v_up_params = mla.kv_lora_rank * num_heads * mla.v_head_dim
            k_rope_params = H * mla.qk_rope_head_dim

            # Output: W_O (num_heads Ã— v_head_dim Ã— H)
            o_params = num_heads * mla.v_head_dim * H

            attn_params_per_layer = q_down_params + q_up_params + q_rope_params + kv_down_params + k_up_params + v_up_params + k_rope_params + o_params
            attn_params = attn_params_per_layer * L
        else:
            # æ ‡å‡† Attention: Q + K + V + O
            head_dim = H // num_heads
            q_params = H * H  # Q projection
            k_params = H * num_kv_heads * head_dim  # K projection (GQA)
            v_params = H * num_kv_heads * head_dim  # V projection (GQA)
            o_params = H * H  # Output projection
            attn_params = (q_params + k_params + v_params + o_params) * L

        # === FFN å‚æ•° ===
        if self.model.model_type == "moe" and self.model.moe_config is not None:
            # MoE æ¨¡å‹
            moe = self.model.moe_config
            expert_I = moe.expert_intermediate_size if moe.expert_intermediate_size > 0 else I

            # Dense å±‚ (å‰ first_k_dense_replace å±‚)
            dense_layers = moe.first_k_dense_replace
            dense_ffn_params = 3 * H * I * dense_layers

            # MoE å±‚
            moe_layers = L - dense_layers
            # è·¯ç”±ä¸“å®¶: num_experts Ã— (gate + up + down)
            routed_expert_params = moe.num_experts * 3 * H * expert_I * moe_layers
            # å…±äº«ä¸“å®¶
            shared_expert_params = moe.num_shared_experts * 3 * H * expert_I * moe_layers
            # Gate ç½‘ç»œ: H Ã— num_experts
            gate_params = H * moe.num_experts * moe_layers

            ffn_params = dense_ffn_params + routed_expert_params + shared_expert_params + gate_params
        else:
            # Dense FFN: (gate, up, down) per layer
            ffn_params = 3 * H * I * L

        # === Embedding (LM Head é€šå¸¸ä¸ Embedding å…±äº«æƒé‡) ===
        embed_params = V * H

        total_params = attn_params + ffn_params + embed_params
        return (total_params * bytes_per_elem) / (1024**3)

    def _calc_kv_cache_size_gb(self, context_length: int) -> float:
        """è®¡ç®— KV Cache å¤§å° (GB)

        æ ¹æ® DeepSeek-V3 è®ºæ–‡ (arXiv:2412.19437):
        "for MLA, only c_t^KV and k_t^R need to be cached during generation"
        - c_t^KV: å‹ç¼©åçš„ KV æ½œåœ¨å‘é‡ï¼Œç»´åº¦ = kv_lora_rank
        - k_t^R: RoPE è§£è€¦ keyï¼Œç»´åº¦ = qk_rope_head_dim

        MLA KV Cache ç»´åº¦ = kv_lora_rank + qk_rope_head_dim (å¦‚ 512 + 64 = 576)
        """
        bytes_per_elem = get_bytes_per_element(self.model.dtype)
        B = self.inference.batch_size
        L = self.model.num_layers

        if self.model.mla_config is not None:
            # MLA: åªç¼“å­˜ c_t^KV + k_t^R
            mla = self.model.mla_config
            kv_cache_dim = mla.kv_lora_rank + mla.qk_rope_head_dim
            kv_cache_bytes = B * context_length * kv_cache_dim * L * bytes_per_elem
        else:
            # æ ‡å‡† Attention: 2 (K+V) Ã— batch Ã— context Ã— kv_heads Ã— head_dim Ã— layers
            H = self.model.hidden_size
            num_heads = self.model.num_attention_heads
            num_kv_heads = self.model.num_kv_heads
            head_dim = H // num_heads
            kv_cache_bytes = 2 * B * context_length * num_kv_heads * head_dim * L * bytes_per_elem

        return kv_cache_bytes / (1024**3)


def run_simulation(
    topology_dict: dict[str, Any],
    model_dict: dict[str, Any],
    inference_dict: dict[str, Any],
    parallelism_dict: dict[str, Any],
    hardware_dict: dict[str, Any],
    config_dict: dict[str, Any] | None = None,
    progress_callback: callable | None = None,
    enable_tile_search: bool = True,
    enable_partition_search: bool = False,
    max_simulated_tokens: int = 4,
    max_gemm_processes: Optional[int] = None,
) -> dict[str, Any]:
    """
    è¿è¡Œæ¨¡æ‹Ÿçš„å…¥å£å‡½æ•°

    Args:
        topology_dict: æ‹“æ‰‘é…ç½®
        model_dict: æ¨¡å‹é…ç½®
        inference_dict: æ¨ç†é…ç½®
        parallelism_dict: å¹¶è¡Œç­–ç•¥
        hardware_dict: ç¡¬ä»¶é…ç½®
        config_dict: æ¨¡æ‹Ÿé…ç½®
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (percent: float, message: str) -> None

    Returns:
        æ¨¡æ‹Ÿç»“æœå­—å…¸
    """
    # éªŒè¯é…ç½®
    validate_model_config(model_dict)
    validate_hardware_config(hardware_dict)
    validate_parallelism_config(parallelism_dict, model_dict)

    # è§£æå¹¶éªŒè¯ MLA é…ç½® (DeepSeek V3/R1)
    mla_config = None
    mla_dict = model_dict.get("mla_config")
    if mla_dict:
        mla_config = validate_mla_config(mla_dict)

    # è§£æå¹¶éªŒè¯ MoE é…ç½® (DeepSeek, Mixtral, Qwen-MoE)
    moe_config = None
    moe_dict = model_dict.get("moe_config")
    if moe_dict:
        moe_config = validate_moe_config(moe_dict)

    # è§£æé…ç½®
    model = LLMModelConfig(
        model_name=model_dict.get("model_name", "Unknown"),
        model_type=model_dict.get("model_type", "dense"),
        hidden_size=model_dict["hidden_size"],
        num_layers=model_dict["num_layers"],
        num_attention_heads=model_dict["num_attention_heads"],
        num_kv_heads=model_dict.get("num_kv_heads", model_dict["num_attention_heads"]),
        intermediate_size=model_dict["intermediate_size"],
        vocab_size=model_dict.get("vocab_size", 32000),
        dtype=model_dict.get("dtype", "fp16"),
        max_seq_length=model_dict.get("max_seq_length", 4096),
        attention_type=model_dict.get("attention_type", "gqa"),
        mla_config=mla_config,
        moe_config=moe_config,
    )

    inference = InferenceConfig(
        batch_size=inference_dict["batch_size"],
        input_seq_length=inference_dict["input_seq_length"],
        output_seq_length=inference_dict["output_seq_length"],
        max_seq_length=inference_dict.get("max_seq_length", 4096),
    )

    parallelism = ParallelismStrategy(
        dp=parallelism_dict.get("dp", 1),
        tp=parallelism_dict.get("tp", 1),
        pp=parallelism_dict.get("pp", 1),
        ep=parallelism_dict.get("ep", 1),
        sp=parallelism_dict.get("sp", 1),
    )

    # è·å– MoE ç›¸å…³çš„ moe_tp å‚æ•°ï¼ˆä» parallelism_dict ä¸­è·å–ï¼‰
    moe_tp = parallelism_dict.get("moe_tp")

    # ä» hardware_dict è·å–èŠ¯ç‰‡å‚æ•°ï¼ˆé¡¶å±‚ chips å­—å…¸ï¼‰
    chips_dict = hardware_dict.get("chips", {})
    if not chips_dict:
        raise ValueError("ç¡¬ä»¶é…ç½®ç¼ºå°‘ 'chips' å­—æ®µï¼Œè¯·ç¡®ä¿ä½¿ç”¨æ–°æ ¼å¼é…ç½®")
    first_chip_name = next(iter(chips_dict))
    chip_hw = chips_dict[first_chip_name]

    # ========== äº’è”å‚æ•°è·å–ï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼‰ ==========
    # æ ¼å¼1: topology_dict.interconnect.links (YAML é…ç½®æ–‡ä»¶æ ¼å¼)
    # æ ¼å¼2: hardware_dict ä¸­çš„ chip/board/rack/pod (å‰ç«¯ä¼ å…¥æ ¼å¼)
    interconnect = topology_dict.get("interconnect", {}).get("links", {})
    c2c_config = interconnect.get("c2c", {})
    b2b_config = interconnect.get("b2b", {})
    r2r_config = interconnect.get("r2r", {})
    p2p_config = interconnect.get("p2p", {})

    # å‰ç«¯ä¼ å…¥çš„ç¡¬ä»¶é…ç½®ï¼ˆå¤‡ç”¨æ¥æºï¼‰
    board_hw = hardware_dict.get("board", {})
    rack_hw = hardware_dict.get("rack", {})
    pod_hw = hardware_dict.get("pod", {})

    # ========== ä¸¥æ ¼å‚æ•°éªŒè¯ï¼ˆä¸ä½¿ç”¨é»˜è®¤å€¼ï¼‰ ==========
    def _require_field(config: dict, field: str, config_name: str) -> Any:
        """è¦æ±‚å­—æ®µå¿…é¡»å­˜åœ¨ï¼Œå¦åˆ™æŠ›å‡ºé”™è¯¯"""
        if field not in config:
            raise ValueError(f"{config_name} ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        return config[field]

    def _require_positive(value: float, field_name: str) -> float:
        """è¦æ±‚å€¼å¿…é¡»ä¸ºæ­£æ•°"""
        if value <= 0:
            raise ValueError(f"{field_name} å¿…é¡»ä¸ºæ­£æ•°ï¼Œå½“å‰å€¼: {value}")
        return value

    def _get_interconnect_param(
        yaml_config: dict, yaml_field: str,
        frontend_config: dict, frontend_field: str,
        param_name: str
    ) -> float:
        """
        ä»ä¸¤ç§æ ¼å¼ä¸­è·å–äº’è”å‚æ•°ï¼ˆä¼˜å…ˆ YAML æ ¼å¼ï¼Œå¤‡ç”¨å‰ç«¯æ ¼å¼ï¼‰

        Args:
            yaml_config: YAML æ ¼å¼çš„é…ç½®ï¼ˆå¦‚ c2c_configï¼‰
            yaml_field: YAML æ ¼å¼çš„å­—æ®µåï¼ˆå¦‚ "bandwidth_gbps"ï¼‰
            frontend_config: å‰ç«¯æ ¼å¼çš„é…ç½®ï¼ˆå¦‚ chip_hw æˆ– board_hwï¼‰
            frontend_field: å‰ç«¯æ ¼å¼çš„å­—æ®µåï¼ˆå¦‚ "c2c_bandwidth_gbps"ï¼‰
            param_name: å‚æ•°åç§°ï¼ˆç”¨äºé”™è¯¯ä¿¡æ¯ï¼‰

        Returns:
            å‚æ•°å€¼
        """
        # ä¼˜å…ˆä» YAML æ ¼å¼è·å–
        if yaml_field in yaml_config:
            return yaml_config[yaml_field]
        # å¤‡ç”¨ï¼šä»å‰ç«¯æ ¼å¼è·å–
        if frontend_field in frontend_config:
            return frontend_config[frontend_field]
        # éƒ½æ²¡æœ‰åˆ™æŠ¥é”™
        raise ValueError(f"äº’è”é…ç½®ç¼ºå°‘å¿…éœ€å­—æ®µ: {param_name}ï¼ˆæ”¯æŒæ ¼å¼ï¼štopology.interconnect.links.*.{yaml_field} æˆ– hardware.*.{frontend_field}ï¼‰")

    # éªŒè¯èŠ¯ç‰‡å¿…éœ€å‚æ•°
    chip_type = _require_field(chip_hw, "name", "èŠ¯ç‰‡é…ç½®")
    num_cores = _require_positive(_require_field(chip_hw, "num_cores", "èŠ¯ç‰‡é…ç½®"), "num_cores")
    compute_tflops_bf16 = _require_positive(_require_field(chip_hw, "compute_tflops_bf16", "èŠ¯ç‰‡é…ç½®"), "compute_tflops_bf16")
    memory_capacity_gb = _require_positive(_require_field(chip_hw, "memory_capacity_gb", "èŠ¯ç‰‡é…ç½®"), "memory_capacity_gb")
    memory_bandwidth_gbps = _require_positive(_require_field(chip_hw, "memory_bandwidth_gbps", "èŠ¯ç‰‡é…ç½®"), "memory_bandwidth_gbps")

    # éªŒè¯äº’è”å¿…éœ€å‚æ•°ï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼‰
    c2c_bandwidth_gbps = _require_positive(
        _get_interconnect_param(c2c_config, "bandwidth_gbps", chip_hw, "c2c_bandwidth_gbps", "c2c_bandwidth"),
        "c2c_bandwidth_gbps"
    )
    c2c_latency_us = _get_interconnect_param(c2c_config, "latency_us", chip_hw, "c2c_latency_us", "c2c_latency")
    b2b_bandwidth_gbps = _require_positive(
        _get_interconnect_param(b2b_config, "bandwidth_gbps", board_hw, "b2b_bandwidth_gbps", "b2b_bandwidth"),
        "b2b_bandwidth_gbps"
    )
    b2b_latency_us = _get_interconnect_param(b2b_config, "latency_us", board_hw, "b2b_latency_us", "b2b_latency")
    r2r_bandwidth_gbps = _require_positive(
        _get_interconnect_param(r2r_config, "bandwidth_gbps", rack_hw, "r2r_bandwidth_gbps", "r2r_bandwidth"),
        "r2r_bandwidth_gbps"
    )
    r2r_latency_us = _get_interconnect_param(r2r_config, "latency_us", rack_hw, "r2r_latency_us", "r2r_latency")
    p2p_bandwidth_gbps = _require_positive(
        _get_interconnect_param(p2p_config, "bandwidth_gbps", pod_hw, "p2p_bandwidth_gbps", "p2p_bandwidth"),
        "p2p_bandwidth_gbps"
    )
    p2p_latency_us = _get_interconnect_param(p2p_config, "latency_us", pod_hw, "p2p_latency_us", "p2p_latency")

    # æ„å»ºè¿è¡Œæ—¶ç¡¬ä»¶å‚æ•°ï¼ˆæ‰€æœ‰å¿…éœ€å‚æ•°å·²éªŒè¯ï¼‰
    hardware = RuntimeHardwareParams(
        # èŠ¯ç‰‡å‚æ•°ï¼ˆå¿…éœ€ï¼‰
        chip_type=chip_type,
        num_cores=num_cores,
        compute_tflops_fp8=chip_hw.get("compute_tflops_fp8", compute_tflops_bf16 * 2),  # FP8 é»˜è®¤ä¸º BF16 çš„ 2 å€
        compute_tflops_bf16=compute_tflops_bf16,
        memory_capacity_gb=memory_capacity_gb,
        memory_bandwidth_gbps=memory_bandwidth_gbps,
        memory_bandwidth_utilization=chip_hw.get("memory_bandwidth_utilization", 0.85),
        lmem_capacity_mb=chip_hw.get("lmem_capacity_mb", 0.0),
        lmem_bandwidth_gbps=chip_hw.get("lmem_bandwidth_gbps", 0.0),
        c2c_bandwidth_gbps=c2c_bandwidth_gbps,
        c2c_latency_us=c2c_latency_us,
        # å¾®æ¶æ„å‚æ•°ï¼ˆå¯é€‰ï¼‰
        cube_m=chip_hw.get("cube_m"),
        cube_k=chip_hw.get("cube_k"),
        cube_n=chip_hw.get("cube_n"),
        sram_size_kb=chip_hw.get("sram_size_kb"),
        sram_utilization=chip_hw.get("sram_utilization"),
        lane_num=chip_hw.get("lane_num"),
        align_bytes=chip_hw.get("align_bytes"),
        compute_dma_overlap_rate=chip_hw.get("compute_dma_overlap_rate"),
        # äº’è”å‚æ•°ï¼ˆå¿…éœ€ï¼‰
        b2b_bandwidth_gbps=b2b_bandwidth_gbps,
        b2b_latency_us=b2b_latency_us,
        r2r_bandwidth_gbps=r2r_bandwidth_gbps,
        r2r_latency_us=r2r_latency_us,
        p2p_bandwidth_gbps=p2p_bandwidth_gbps,
        p2p_latency_us=p2p_latency_us,
    )

    config = SimulationConfig(
        max_simulated_tokens=max_simulated_tokens,  # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
        enable_data_transfer=config_dict.get("enableDataTransferSimulation", True) if config_dict else True,
        enable_detailed_ops=config_dict.get("enableDetailedTransformerOps", True) if config_dict else True,
        enable_kv_cache=config_dict.get("enableKVCacheAccessSimulation", True) if config_dict else True,
    )

    # ä»æ‹“æ‰‘é…ç½®ä¸­æå–é€šä¿¡å»¶è¿Ÿé…ç½® (interconnect.comm_params)
    comm_latency_config = topology_dict.get("interconnect", {}).get("comm_params")

    # è¿è¡Œæ¨¡æ‹Ÿ
    simulator = LLMInferenceSimulator(
        topology_dict=topology_dict,
        model=model,
        inference=inference,
        parallelism=parallelism,
        hardware=hardware,
        config=config,
        comm_latency_config=comm_latency_config,
        progress_callback=progress_callback,
        enable_tile_search=enable_tile_search,
        enable_partition_search=enable_partition_search,
        max_gemm_processes=max_gemm_processes,
        moe_tp=moe_tp,
    )

    result = simulator.simulate()

    # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
    from .gantt import convert_to_frontend_format

    # è®¡ç®—ååé‡æŒ‡æ ‡ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„èŠ¯ç‰‡è®¡ç®—å‡½æ•°ï¼‰
    from ..tasks.deployment import calculate_required_chips
    total_chips = calculate_required_chips(parallelism_dict, model_dict)

    # TPOT è½¬æ¢ï¼šå¾®ç§’ -> æ¯«ç§’
    tpot_ms = result.stats.avg_tpot / 1000.0 if result.stats.avg_tpot > 0 else 0.0

    # TPS per Batch: å•ä¸ªè¯·æ±‚æ¯ç§’ç”Ÿæˆçš„tokenæ•° (ç”¨æˆ·ä½“éªŒæŒ‡æ ‡)
    # å…¬å¼: 1000ms/s / TPOT(ms/token) = tokens/s per request
    tps_per_batch = 1000.0 / tpot_ms if tpot_ms > 0 else 0.0

    # TPS per Chip: å•èŠ¯ç‰‡ï¼ˆå•DP rankï¼‰æ¯ç§’å¤„ç†çš„æ€»tokenæ•° (æˆæœ¬æ•ˆç›ŠæŒ‡æ ‡)
    # å…¬å¼: TPS_batch Ã— batch_size = tokens/s per chip
    tps_per_chip = tps_per_batch * inference.batch_size

    # Total TPS: é›†ç¾¤æ€»ååé‡ (tokens/s)
    # å…¬å¼: TPS_chip Ã— DP = total tokens/s (DPçº¿æ€§æ‰©å±•åå)
    tokens_per_second = tps_per_chip * parallelism.dp

    # ç†è®ºå³°å€¼ååé‡ï¼ˆåŸºäºç¡¬ä»¶ç®—åŠ›ï¼Œä»…ä½œå‚è€ƒï¼‰
    theoretical_max_tps = tokens_per_second / max(result.stats.dynamic_mfu, 0.01) if result.stats.dynamic_mfu > 0 else 0.0

    # Requests per second: æ¯ç§’å¤„ç†çš„è¯·æ±‚æ•°
    # åœ¨æŒç»­decodeåœºæ™¯ä¸‹ï¼Œæ¯ä¸ªè¯·æ±‚å ç”¨ä¸€ä¸ªbatch slot
    requests_per_second = tokens_per_second / inference.output_seq_length if inference.output_seq_length > 0 else 0.0

    # è½¬æ¢é“¾è·¯æµé‡ç»Ÿè®¡ä¸ºå‰ç«¯æ ¼å¼ï¼ˆå°† snake_case è½¬æ¢ä¸º camelCaseï¼‰
    from dataclasses import asdict
    link_traffic_stats_dict = []
    for stat in result.link_traffic_stats:
        link_traffic_stats_dict.append({
            "source": stat.source,
            "target": stat.target,
            "trafficMb": stat.traffic_mb,
            "bandwidthGbps": stat.bandwidth_gbps,
            "latencyUs": stat.latency_us,
            "utilizationPercent": stat.utilization_percent,
            "linkType": stat.link_type,
            "contributingTasks": stat.contributing_tasks,
            "taskTypeBreakdown": stat.task_type_breakdown,
        })

    return {
        "ganttChart": convert_to_frontend_format(result.gantt_chart),
        "stats": {
            "prefill": {
                "computeTime": result.stats.prefill.compute_time,
                "commTime": result.stats.prefill.comm_time,
                "bubbleTime": result.stats.prefill.bubble_time,
                "overlapTime": result.stats.prefill.overlap_time,
                "totalTime": result.stats.prefill.total_time,
                "computeEfficiency": result.stats.prefill.compute_efficiency,
            },
            "decode": {
                "computeTime": result.stats.decode.compute_time,
                "commTime": result.stats.decode.comm_time,
                "bubbleTime": result.stats.decode.bubble_time,
                "overlapTime": result.stats.decode.overlap_time,
                "totalTime": result.stats.decode.total_time,
                "computeEfficiency": result.stats.decode.compute_efficiency,
            },
            "totalRunTime": result.stats.total_run_time,
            "simulatedTokens": result.stats.simulated_tokens,
            "ttft": result.stats.ttft,
            "avgTpot": result.stats.avg_tpot,
            "dynamicMfu": result.stats.dynamic_mfu,
            "dynamicMbu": result.stats.dynamic_mbu,
            "maxPPBubbleRatio": result.stats.max_pp_bubble_ratio,
            "totalEvents": result.stats.total_events,
            "totalChips": total_chips,
            "linkTrafficStats": link_traffic_stats_dict,  # æ–°å¢ï¼šé“¾è·¯æµé‡ç»Ÿè®¡
        },
        # ååé‡æŒ‡æ ‡ï¼ˆç‹¬ç«‹å¯¹è±¡ï¼Œä¸å‰ç«¯ ThroughputAnalysis å¯¹åº”ï¼‰
        "throughput": {
            "tokens_per_second": tokens_per_second,           # é›†ç¾¤æ€»åå (tokens/s)
            "tps_per_batch": tps_per_batch,                   # å•è¯·æ±‚TPS (tokens/s per request) - ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
            "tps_per_chip": tps_per_chip,                     # å•èŠ¯ç‰‡TPS (tokens/s per chip) - æˆæœ¬æ•ˆç›ŠæŒ‡æ ‡
            "requests_per_second": requests_per_second,       # è¯·æ±‚åå (requests/s)
            "model_flops_utilization": result.stats.dynamic_mfu,  # MFU (0-1)
            "memory_bandwidth_utilization": result.stats.dynamic_mbu,  # MBU (0-1)
            "theoretical_max_throughput": theoretical_max_tps,  # ç†è®ºå³°å€¼åå (tokens/s)
        },
        "timestamp": result.timestamp,
    }
