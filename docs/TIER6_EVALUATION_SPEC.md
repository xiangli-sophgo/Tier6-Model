# Tier6-Model æ€§èƒ½è¯„ä¼°ç³»ç»Ÿè§„æ ¼è¯´æ˜

**ç‰ˆæœ¬**: v2.2.0
**æœ€åæ›´æ–°**: 2026-01-26
**çŠ¶æ€**: Production

---

## ğŸ“‹ ç›®å½•

1. [ç³»ç»Ÿæ¦‚è¿°](#1-ç³»ç»Ÿæ¦‚è¿°)
2. [æ¶æ„è®¾è®¡](#2-æ¶æ„è®¾è®¡)
3. [è¯„ä¼°æµç¨‹](#3-è¯„ä¼°æµç¨‹)
4. [æ ¸å¿ƒæ¨¡å—è§„æ ¼](#4-æ ¸å¿ƒæ¨¡å—è§„æ ¼)
5. [MoE è´Ÿè½½å‡è¡¡å®ç°](#5-moe-è´Ÿè½½å‡è¡¡å®ç°)
6. [æ€§èƒ½ä¼˜åŒ–æœºåˆ¶](#6-æ€§èƒ½ä¼˜åŒ–æœºåˆ¶)
7. [ä¸ DS_TPU å¯¹é½æƒ…å†µ](#7-ä¸-ds_tpu-å¯¹é½æƒ…å†µ)
8. [é…ç½®å‚æ•°](#8-é…ç½®å‚æ•°)
   - 8.1 æ¨¡å‹é…ç½®
   - 8.2 éƒ¨ç½²é…ç½®
   - 8.3 é€šä¿¡å»¶è¿Ÿé…ç½® (CommLatencyConfig) âœ¨ **New**
   - 8.4 ç¡¬ä»¶é…ç½®
9. [ä½¿ç”¨ç¤ºä¾‹](#9-ä½¿ç”¨ç¤ºä¾‹)
10. [é™„å½•](#10-é™„å½•)

---

## 1. ç³»ç»Ÿæ¦‚è¿°

### 1.1 è®¾è®¡ç›®æ ‡

Tier6-Model æ˜¯ä¸€ä¸ªç”¨äº LLM æ¨ç†æ€§èƒ½è¯„ä¼°çš„ç²¾ç¡®æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨ï¼š

- **ç²¾ç¡®å»ºæ¨¡**ï¼šåŸºäºç¡¬ä»¶å¾®æ¶æ„å‚æ•°ï¼ˆTileã€SRAMã€å¸¦å®½ï¼‰è¿›è¡Œç»†ç²’åº¦è¯„ä¼°
- **å…¨æµç¨‹è¦†ç›–**ï¼šæ”¯æŒ Prefill å’Œ Decode é˜¶æ®µï¼Œæ¶µç›–è®¡ç®—ã€è®¿å­˜ã€é€šä¿¡
- **å¯æ‰©å±•æ€§**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•æ–°ç®—å­å’Œç¡¬ä»¶å¹³å°
- **é«˜æ€§èƒ½**ï¼šå¤šçº§ç¼“å­˜æœºåˆ¶ï¼Œæ”¯æŒå¤§è§„æ¨¡é…ç½®æœç´¢

### 1.2 æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **ç²¾ç¡® GEMM è¯„ä¼°** | å¤šæ ¸åˆ†å— + Tile æœç´¢ + å¾ªç¯é¡ºåºä¼˜åŒ– |
| **FlashAttention** | æ”¯æŒ MHA/MQA/GQAï¼Œè€ƒè™‘ Softmax è®¿å­˜ç“¶é¢ˆ |
| **MoE è´Ÿè½½å‡è¡¡** | åŸºäºè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿçš„ä¸“å®¶è·¯ç”±å»ºæ¨¡ |
| **é€šä¿¡å»ºæ¨¡** | æ”¯æŒå¤šåè®®ï¼ˆpost-write/non-post-write/æµæ°´çº¿ï¼‰ |
| **GEMM é¢„çƒ­** | ç¦»çº¿é¢„è°ƒä¼˜å¸¸è§ GEMM å½¢çŠ¶ï¼ŒåŠ é€Ÿé¦–æ¬¡è¯„ä¼° |
| **å…¨å±€ç¼“å­˜** | è·¨å®éªŒå¤ç”¨è¯„ä¼°ç»“æœ |

### 1.3 æ”¯æŒçš„æ¨¡å‹å’Œç¡¬ä»¶

**æ¨¡å‹æ¶æ„**ï¼š
- DeepSeek V3 / R1 (MLA + MoE)
- æ ‡å‡† Transformer (MHA + MLP/MoE)

**ç¡¬ä»¶å¹³å°**ï¼š
- ç®—èƒ½ SG2260E (é»˜è®¤)
- NVIDIA H100 SXM
- NVIDIA A100

---

## 2. æ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å‰ç«¯ (React + Three.js)                   â”‚
â”‚  - 3D å¯è§†åŒ–æ‹“æ‰‘é…ç½®                                          â”‚
â”‚  - äº¤äº’å¼å‚æ•°è°ƒæ•´                                             â”‚
â”‚  - å®æ—¶ç»“æœå±•ç¤º (Ganttå›¾ã€æ€§èƒ½æŒ‡æ ‡)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ HTTP API
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 åç«¯ (Python + FastAPI)                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  API å±‚ (api.py)                                        â”‚ â”‚
â”‚ â”‚  - POST /api/simulate                                   â”‚ â”‚
â”‚ â”‚  - POST /api/validate                                   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                      â†“                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  æ¨¡æ‹Ÿå™¨ (simulator.py)                                  â”‚ â”‚
â”‚ â”‚  - æ„å»ºå±‚å’Œç®—å­                                          â”‚ â”‚
â”‚ â”‚  - è°ƒåº¦è¯„ä¼°æµç¨‹                                          â”‚ â”‚
â”‚ â”‚  - ç”Ÿæˆ Gantt å›¾                                        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                      â†“                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚   å±‚å®šä¹‰           â”‚   ç®—å­å®šä¹‰         â”‚   è¯„ä¼°å™¨        â”‚ â”‚
â”‚ â”‚  (layers/)        â”‚  (operators/)     â”‚ (evaluators/)   â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ - EmbeddingLayer  â”‚ - MatMulOperator  â”‚ - GEMMEvaluator â”‚ â”‚
â”‚ â”‚ - MLALayer        â”‚ - MHAOperator     â”‚ - FA2Evaluator  â”‚ â”‚
â”‚ â”‚ - MLAAbsorbLayer  â”‚ - MQAOperator     â”‚ - AllReduceEval â”‚ â”‚
â”‚ â”‚ - MoELayer        â”‚ - AllReduceOp     â”‚ - DispatchEval  â”‚ â”‚
â”‚ â”‚ - MLPLayer        â”‚ - DispatchOp      â”‚ - MoELoadBalanceâ”‚ â”‚
â”‚ â”‚ - LMHeadLayer     â”‚ - ...             â”‚ - ...           â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                      â†“                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  ç¡¬ä»¶å¾®æ¶æ„é…ç½® (AcceleratorMicroArch)                   â”‚ â”‚
â”‚ â”‚  - SRAM/Cache å‚æ•°                                      â”‚ â”‚
â”‚ â”‚  - è®¡ç®—å•å…ƒè§„æ ¼ (Cube M/N/K)                            â”‚ â”‚
â”‚ â”‚  - å¸¦å®½å’Œå»¶è¿Ÿå‚æ•°                                        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ•°æ®æµ

```
ç”¨æˆ·è¾“å…¥é…ç½®
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. æ‹“æ‰‘è§£æ           â”‚  â†’ InterconnectGraph (èŠ¯ç‰‡è¿æ¥å…³ç³»)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. å¹¶è¡Œç­–ç•¥æ˜ å°„       â”‚  â†’ åˆ†é… TP/PP/DP/EP èŠ¯ç‰‡ç»„
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. å±‚æ„å»º             â”‚  â†’ åˆ›å»º Layer å®ä¾‹ (MLA, MoE, MLP, ...)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ç®—å­å®ä¾‹åŒ–         â”‚  â†’ åˆ›å»º Operator (MatMul, MHA, AllReduce, ...)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. è¯„ä¼°å™¨è°ƒç”¨         â”‚  â†’ ç²¾ç¡®è®¡ç®—å»¶è¿Ÿã€æµé‡ã€åˆ©ç”¨ç‡
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. ç»“æœèšåˆ           â”‚  â†’ å±‚çº§/æ¨¡å‹çº§æ€§èƒ½æŒ‡æ ‡
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¾“å‡ºç»“æœ (JSON + Ganttå›¾)
```

### 2.3 æ¨¡å—èŒè´£

| æ¨¡å— | è·¯å¾„ | èŒè´£ |
|------|------|------|
| **Simulator** | `simulator.py` | ä¸»æ§æµç¨‹ï¼Œè°ƒåº¦è¯„ä¼° |
| **Layers** | `layers/` | å±‚çº§æŠ½è±¡ï¼Œç»„åˆç®—å­ |
| **Operators** | `operators/` | ç®—å­æ¥å£ï¼Œå®šä¹‰è®¡ç®—å’Œé€šä¿¡åŸè¯­ |
| **Evaluators** | `evaluators/` | ç²¾ç¡®è¯„ä¼°ï¼Œå®ç°ç¡¬ä»¶å»ºæ¨¡ |
| **Types** | `types.py` | ç±»å‹å®šä¹‰ï¼Œæšä¸¾å’Œæ•°æ®ç±» |

---

## 3. è¯„ä¼°æµç¨‹

### 3.1 æ€»ä½“æµç¨‹

```python
# ä¼ªä»£ç 
def simulate(config):
    # æ­¥éª¤ 1: è§£æé…ç½®
    topology = parse_topology(config.topology)
    model_cfg = config.model
    deploy_cfg = config.deployment

    # æ­¥éª¤ 2: åˆ›å»ºç¡¬ä»¶æ¶æ„
    arch = get_arch_preset(config.hardware)

    # æ­¥éª¤ 3: æ„å»ºå±‚
    layers = []
    for layer_type in model_cfg.layers:
        layer = create_layer(layer_type, model_cfg, deploy_cfg)
        layers.append(layer)

    # æ­¥éª¤ 4: è¯„ä¼°æ¯ä¸€å±‚
    for layer in layers:
        for operator in layer.operators:
            evaluator = get_evaluator(operator.type, arch)
            result = evaluator.evaluate(operator)
            operator.latency = result.latency_us
            operator.traffic = result.dram_traffic_bytes
            # ...

    # æ­¥éª¤ 5: èšåˆç»“æœ
    total_latency = sum(layer.latency for layer in layers)
    return {
        'latency_us': total_latency,
        'mfu': calculate_mfu(layers, arch),
        'gantt': generate_gantt(layers),
        # ...
    }
```

### 3.2 Prefill vs Decode å·®å¼‚

| ç»´åº¦ | Prefill | Decode |
|------|---------|--------|
| **åºåˆ—é•¿åº¦** | q_len = kv_len (å¦‚ 4096) | q_len = 1 |
| **Attention ç±»å‹** | MHA (full attention) | MQA/MLA_absorb (cached KV) |
| **è®¡ç®—ç“¶é¢ˆ** | è®¡ç®—å¯†é›† (GEMM) | è®¿å­˜å¯†é›† (KV Cache) |
| **å¹¶è¡Œç­–ç•¥** | TP + PP + DP | TP-SP (Sequence Parallel) |
| **MoE è´Ÿè½½** | è´Ÿè½½ç›¸å¯¹å‡è¡¡ | è´Ÿè½½ä¸¥é‡ä¸å‡ |

### 3.3 å±‚çº§è¯„ä¼°æµç¨‹

ä»¥ **DeepSeek V3 Decode** ä¸ºä¾‹ï¼š

```
Layer 0: Embedding
  â†“
Layer 1-61: Transformer Block
  â”œâ”€ RMSNorm (Pre-Attention)
  â”œâ”€ MLA_absorb
  â”‚   â”œâ”€ q_a_proj (GEMM)
  â”‚   â”œâ”€ q_b_proj (GEMM)
  â”‚   â”œâ”€ kv_a_proj (GEMM)
  â”‚   â”œâ”€ w_kc (GEMM)
  â”‚   â”œâ”€ MQA (FlashAttention)
  â”‚   â”œâ”€ w_vc (GEMM)
  â”‚   â”œâ”€ o_proj (GEMM)
  â”‚   â””â”€ AllReduce (TP > 1)
  â”œâ”€ RMSNorm (Pre-MoE)
  â””â”€ MoE
      â”œâ”€ Gate Router (GEMM)
      â”œâ”€ Dispatch (EP > 1)
      â”œâ”€ Routed Experts
      â”‚   â”œâ”€ gate_proj (GEMM, G=max_experts)
      â”‚   â”œâ”€ up_proj (GEMM, G=max_experts)
      â”‚   â””â”€ down_proj (GEMM, G=max_experts)
      â”œâ”€ AllReduce (MoE_TP > 1)
      â”œâ”€ Combine (EP > 1)
      â””â”€ Shared Experts (å¯é€‰)
  â†“
Layer 62: LMHead
```

---

## 4. æ ¸å¿ƒæ¨¡å—è§„æ ¼

### 4.1 GEMM è¯„ä¼°å™¨ (GEMMEvaluator)

**åŠŸèƒ½**ï¼šè¯„ä¼°çŸ©é˜µä¹˜æ³• `C[G, M, N] = A[G, M, K] Ã— B[G, K, N]`

**è¾“å…¥å‚æ•°**ï¼š
```python
@dataclass
class GEMMParams:
    G: int              # Batch/Group ç»´åº¦
    M: int              # è¾“å‡ºè¡Œæ•°
    K: int              # ç´¯åŠ ç»´åº¦
    N: int              # è¾“å‡ºåˆ—æ•°
    input_dtype: str    # 'fp8', 'bf16', 'fp16'
    output_dtype: str   # 'bf16', 'fp32'
```

**è¾“å‡ºç»“æœ**ï¼š
```python
@dataclass
class GEMMResult:
    latency_us: float               # æ€»å»¶è¿Ÿ (å¾®ç§’)
    compute_time_us: float          # è®¡ç®—æ—¶é—´
    memory_time_us: float           # è®¿å­˜æ—¶é—´
    flops: int                      # æµ®ç‚¹è¿ç®—æ•°
    dram_traffic_bytes: int         # DRAM æµé‡
    arch_utilization: float         # æ¶æ„åˆ©ç”¨ç‡ (0-1)
    effective_utilization: float    # æœ‰æ•ˆåˆ©ç”¨ç‡ (0-1)
    best_tile: Tuple[int, int, int] # æœ€ä½³ Tile (m_t, n_t, k_t)
    best_loop_order: str            # æœ€ä½³å¾ªç¯é¡ºåº ('mnk', 'nkm', 'mkn')
    best_partition: Tuple[int, int, int, int]  # æœ€ä½³åˆ†å— (P_G, P_M, P_N, P_K)
```

**è¯„ä¼°æµç¨‹**ï¼š

```
1. æšä¸¾æ‰€æœ‰åˆæ³•çš„å¤šæ ¸åˆ†å— (P_G, P_M, P_N, P_K)
   çº¦æŸï¼šP_G Ã— P_M Ã— P_N Ã— P_K = num_cores

2. å¯¹æ¯ä¸ªåˆ†å—æ–¹æ¡ˆï¼š
   a. è®¡ç®—æ¯æ ¸è´Ÿè´£çš„ç»´åº¦ (g_nom, m_nom, n_nom, k_nom)
   b. æœç´¢èƒ½æ”¾è¿› SRAM çš„ Tile å¤§å° (m_t, n_t, k_t)
      çº¦æŸï¼šm_t Ã— n_t Ã— output_dtype + (m_t + n_t) Ã— k_t Ã— input_dtype â‰¤ SRAM
   c. å¯¹æ¯ä¸ª Tile å’Œå¾ªç¯é¡ºåº (mnk, nkm, mkn)ï¼š
      - è®¡ç®— DRAM æµé‡
      - é€‰æ‹©æµé‡æœ€å°çš„ç»„åˆ
   d. è®¡ç®—è¯¥åˆ†å—çš„æ€»å»¶è¿Ÿ
      - æ¶æ„åˆ©ç”¨ç‡ï¼šreal_macs / theo_macs
      - è®¡ç®—æ—¶é—´ï¼štheo_macs / (freq Ã— macs_per_cycle)
      - è®¿å­˜æ—¶é—´ï¼šdram_traffic / dma_bandwidth
      - é‡å æ¨¡å‹ï¼šmax(t_comp, t_dma) + min(t_comp, t_dma) Ã— (1 - overlap_rate)

3. è¿”å›å»¶è¿Ÿæœ€å°çš„åˆ†å—æ–¹æ¡ˆ
```

**å…³é”®å…¬å¼**ï¼š

```python
# æ¶æ„åˆ©ç”¨ç‡
arch_utilization = (M Ã— N Ã— K) / (align_up(M, cube_m) Ã— align_up(K, cube_k) Ã— align_up(N, cube_n))

# è®¡ç®—æ—¶é—´
compute_time_us = (align_up(M, cube_m) Ã— align_up(K, cube_k) Ã— align_up(N, cube_n) Ã— G) / (macs_per_cycle Ã— freq_ghz) / 1000

# DRAM æµé‡ï¼ˆä»¥ mnk å¾ªç¯ä¸ºä¾‹ï¼‰
tile_num_m = ceil(m_blk / m_t)
tile_num_n = ceil(n_blk / n_t)
dram_traffic = (m_blk Ã— k_blk Ã— input_bytes) Ã— tile_num_n +  # A é‡å¤åŠ è½½
               (n_blk Ã— k_blk Ã— input_bytes) Ã— tile_num_m +  # B é‡å¤åŠ è½½
               (m_blk Ã— n_blk Ã— output_bytes)                # C å†™å›
```

**æ€§èƒ½ä¼˜åŒ–**ï¼š
- **å¤šè¿›ç¨‹å¹¶è¡Œ**ï¼šæ‰€æœ‰åˆ†å—æ–¹æ¡ˆå¹¶è¡Œè¯„ä¼°
- **ç¼“å­˜æœºåˆ¶**ï¼šç›¸åŒ (G, M, K, N, dtype) å¤ç”¨ç»“æœ
- **Pareto æœ€ä¼˜**ï¼šTile æœç´¢æ—¶å‰ªæè¢«æ”¯é…çš„å€™é€‰

**å‚è€ƒ**ï¼š`backend/llm_simulator/evaluators/gemm_eval.py`

---

### 4.2 FlashAttention è¯„ä¼°å™¨ (FA2Evaluator)

**åŠŸèƒ½**ï¼šè¯„ä¼° Fused Attention (`Q @ K.T @ V`)

**è¾“å…¥å‚æ•°**ï¼š
```python
@dataclass
class FA2Params:
    B: int      # Batch Ã— Heads
    QS: int     # Query åºåˆ—é•¿åº¦
    KS: int     # Key/Value åºåˆ—é•¿åº¦
    QD: int     # Query Head ç»´åº¦
    VD: int     # Value Head ç»´åº¦
```

**è¾“å‡ºç»“æœ**ï¼š
```python
@dataclass
class FA2Result:
    latency_us: float           # æ€»å»¶è¿Ÿ
    qk_matmul_us: float         # Q @ K.T æ—¶é—´
    softmax_us: float           # Softmax æ—¶é—´
    sv_matmul_us: float         # Score @ V æ—¶é—´
    flops: int                  # æ€» FLOPs
    dram_traffic_bytes: int     # DRAM æµé‡
```

**è¯„ä¼°ç­–ç•¥**ï¼š

```
FlashAttention åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š

1. QK MatMul: Q[B, QS, QD] @ K[B, KS, QD].T â†’ Score[B, QS, KS]
   - ä½¿ç”¨ GEMM è¯„ä¼°å™¨

2. Softmax: Score[B, QS, KS] â†’ Prob[B, QS, KS]
   - è®¿å­˜å¯†é›†ï¼šéœ€è¯»å†™ Score çŸ©é˜µ
   - å»¶è¿Ÿ = (2 Ã— B Ã— QS Ã— KS Ã— sizeof(dtype)) / dram_bandwidth

3. SV MatMul: Prob[B, QS, KS] @ V[B, KS, VD] â†’ Out[B, QS, VD]
   - ä½¿ç”¨ GEMM è¯„ä¼°å™¨

æ€»å»¶è¿Ÿ = qk_matmul + softmax + sv_matmul
```

**å‚è€ƒ**ï¼š`backend/llm_simulator/evaluators/fa2_eval.py`

---

### 4.3 é€šä¿¡è¯„ä¼°å™¨ (CommEvaluators)

**æ”¯æŒçš„é€šä¿¡åŸè¯­**ï¼š

| åŸè¯­ | å…¬å¼ | è¯´æ˜ |
|------|------|------|
| **AllReduce** | `lat = 2(N-1)/N Ã— size / bw + (N-1) Ã— Î±` | Ring ç®—æ³• |
| **AllGather** | `lat = (N-1) Ã— size / bw + (N-1) Ã— Î±` | Ring ç®—æ³• |
| **ReduceScatter** | `lat = (N-1)/N Ã— size / bw + (N-1) Ã— Î±` | Ring ç®—æ³• |
| **Dispatch** | `lat = size / bw + Î± + cpu_fetch` | EP åˆ†å‘ |
| **Combine** | `lat = size / bw + Î± + cpu_fetch` | EP æ±‡é›† |

**é€šä¿¡åè®®æ”¯æŒ**ï¼š

```python
class CommProtocol(Enum):
    POST_WRITE = 1          # é»˜è®¤ï¼šå¼‚æ­¥å†™
    NON_POST_WRITE = 2      # åŒæ­¥å†™ï¼š+RTT Ã— 2 Ã— (N-1)
    PIPELINE = 3            # æµæ°´çº¿ï¼š+RTT Ã— min(1, 2 Ã— (N-1))
```

**å…³é”®å‚æ•°**ï¼š
- `Î±` (start_lat)ï¼šå¯åŠ¨å»¶è¿Ÿï¼ˆå¦‚ 0.6 Î¼sï¼‰
- `bw`ï¼šå¸¦å®½ï¼ˆintra: 273 GB/s, inter: 100 GB/sï¼‰
- `RTT`ï¼šå¾€è¿”å»¶è¿Ÿï¼ˆTP: 0.35 Î¼s, EP: 0.85 Î¼sï¼‰

**å‚è€ƒ**ï¼š`backend/llm_simulator/evaluators/comm_eval.py`

---

### 4.4 MoE è´Ÿè½½å‡è¡¡æ¨¡å— (MoELoadBalance)

**é—®é¢˜èƒŒæ™¯**ï¼š

MoE çš„ Router ç½‘ç»œä¸ºæ¯ä¸ª token éšæœºé€‰æ‹© Top-K ä¸ªä¸“å®¶ã€‚å½“ä¸“å®¶åˆ†å¸ƒåˆ°å¤šä¸ªèŠ¯ç‰‡æ—¶ï¼š
- **ç†æƒ³å‡è®¾**ï¼šæ¯ä¸ªèŠ¯ç‰‡æ¿€æ´»çš„ä¸“å®¶æ•° = `num_experts / num_chips`
- **å®é™…æƒ…å†µ**ï¼šæŸäº›èŠ¯ç‰‡ä¼šè¢«è°ƒç”¨æ›´å¤šæ¬¡ï¼ˆè·¯ç”±ä¸å‡ï¼‰
- **ç“¶é¢ˆ**ï¼šæœ€æ…¢çš„èŠ¯ç‰‡å†³å®šæ€»å»¶è¿Ÿï¼ˆæœ¨æ¡¶æ•ˆåº”ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š

ä½¿ç”¨**è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ**æˆ–**æŸ¥æ‰¾è¡¨**è·å–æœ€å¿™èŠ¯ç‰‡éœ€è¦åŠ è½½çš„ä¸“å®¶æ•°ã€‚

**æŸ¥æ‰¾è¡¨**ï¼š

```python
MAX_EXPERT_TABLE = {
    # batch_size: {chips: max_experts}
    4: {1: 30.51, 2: 17.34, 4: 10.37, 8: 6.58, 16: 4.45, 32: 3.18, ...},
    8: {1: 57.43, 2: 31.41, 4: 17.81, 8: 10.65, 16: 6.70, 32: 4.44, ...},
    # ...
    256: {1: 255.93, 2: 128.0, 4: 64.0, 8: 32.0, 16: 16.0, 32: 8.0, ...}
}
```

**ç‰©ç†æ„ä¹‰**ï¼š
- `MAX_EXPERT_TABLE[batch=4][chips=32] = 3.18`
- å«ä¹‰ï¼š32 ä¸ªèŠ¯ç‰‡ä¸­ï¼Œ**æœ€å¿™çš„èŠ¯ç‰‡éœ€è¦åŠ è½½çº¦ 3.18 ä¸ªä¸åŒä¸“å®¶**

**ä½¿ç”¨æ–¹æ³•**ï¼š

```python
# 1. æŸ¥è¡¨è·å–ä¸“å®¶æ•°
max_experts = get_max_expert_load(batch_size=4, chips=32)  # â†’ 3.18

# 2. ç”¨äº GEMM çš„ G ç»´åº¦ï¼ˆå‘ä¸Šå–æ•´ï¼‰
G = math.ceil(max_experts)  # â†’ 4

# 3. è®¡ç®—ä¸“å®¶ GEMM
gemm_result = gemm_evaluator.evaluate(
    G=G,                    # ä¸“å®¶å¹¶è¡Œç»´åº¦
    M=tokens_per_expert,    # æ¯ä¸“å®¶å¤„ç†çš„ tokens
    K=hidden_dim,
    N=expert_inter_dim / moe_tp
)

# 4. è®¡ç®—æƒé‡æ¬è¿æ—¶é—´
expert_param_size = 3 Ã— hidden_dim Ã— expert_inter_dim Ã— dtype_bytes
weight_load_time = max_experts Ã— expert_param_size / dram_bandwidth
```

**æŸ¥è¯¢ç­–ç•¥ï¼ˆä¸‰çº§å›é€€ï¼‰**ï¼š

```
1. ç²¾ç¡®æŸ¥è¡¨ï¼ˆO(1)ï¼‰
   â†“ æœªå‘½ä¸­
2. çº¿æ€§æ’å€¼ï¼ˆO(log n)ï¼‰
   â†“ å¤±è´¥
3. è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼ˆO(iterations)ï¼‰
```

**è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿç®—æ³•**ï¼š

```python
def monte_carlo_max_experts(batch_size, chips, iterations=1000):
    max_experts_list = []
    experts_per_chip = 256 // chips

    for _ in range(iterations):
        chip_experts = [set() for _ in range(chips)]

        # æ¨¡æ‹Ÿ batch_size ä¸ª token çš„è·¯ç”±
        for _ in range(batch_size):
            selected_experts = random.sample(range(256), 8)  # Top-8
            for expert_id in selected_experts:
                chip_id = expert_id // experts_per_chip
                chip_experts[chip_id].add(expert_id)

        # ç»Ÿè®¡æœ€å¿™çš„èŠ¯ç‰‡
        max_experts = max(len(experts) for experts in chip_experts)
        max_experts_list.append(max_experts)

    return sum(max_experts_list) / len(max_experts_list)
```

**éªŒè¯ç»“æœ**ï¼š

| Batch | Chips | è¡¨å€¼ | æ¨¡æ‹Ÿå€¼ | è¯¯å·® |
|-------|-------|------|--------|------|
| 4 | 32 | 3.18 | 3.19 | 0.31% |
| 64 | 32 | 8.00 | 8.00 | 0.00% |
| 256 | 32 | 8.00 | 8.00 | 0.00% |

**å½±å“**ï¼š

| åœºæ™¯ | ç†æƒ³å‡è®¾ | è´Ÿè½½å‡è¡¡ | æ”¹å–„ |
|------|---------|---------|------|
| Decode (batch=4, EP=32) | 8 ä¸“å®¶/èŠ¯ç‰‡ | 3.18 ä¸“å®¶ | **-60.2%** å»¶è¿Ÿ |
| Prefill (batch=256, EP=32) | 8 ä¸“å®¶/èŠ¯ç‰‡ | 8.0 ä¸“å®¶ | æ— å½±å“ |

**å‚è€ƒ**ï¼š`backend/llm_simulator/evaluators/moe_load_balance.py`

---

## 5. MoE è´Ÿè½½å‡è¡¡å®ç°

### 5.1 å®ç°æ–‡ä»¶

**æ ¸å¿ƒæ¨¡å—**ï¼š
```
backend/llm_simulator/evaluators/moe_load_balance.py  # è´Ÿè½½å‡è¡¡æŸ¥è¯¢
backend/llm_simulator/layers/moe.py                   # MoE å±‚ä½¿ç”¨
backend/tests/test_moe_load_balance.py                # è’™ç‰¹å¡æ´›éªŒè¯
backend/tests/test_moe_integration.py                 # é›†æˆæµ‹è¯•
```

### 5.2 API æ¥å£

**ä¸»æŸ¥è¯¢æ¥å£**ï¼š

```python
def get_max_expert_load(
    batch_size: int,
    chips: int,
    allow_simulation: bool = True,
    simulation_iterations: int = 1000
) -> float:
    """
    è·å–æœ€å¿™èŠ¯ç‰‡éœ€è¦åŠ è½½çš„ä¸“å®¶æ•°

    Args:
        batch_size: token æ•°é‡
        chips: EP èŠ¯ç‰‡æ•°
        allow_simulation: æ˜¯å¦å…è®¸è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ
        simulation_iterations: æ¨¡æ‹Ÿè¿­ä»£æ¬¡æ•°

    Returns:
        æœ€å¿™èŠ¯ç‰‡éœ€è¦åŠ è½½çš„ä¸“å®¶ä¸ªæ•°ï¼ˆæµ®ç‚¹æ•°ï¼‰
    """
```

**ä¾¿æ·æ¥å£**ï¼š

```python
def get_max_expert_load_for_moe_layer(
    batch_size: int,
    ep_parallelism: int,
    num_experts: int = 256,
    topk: int = 8
) -> float:
    """é’ˆå¯¹ MoE å±‚çš„ä¾¿æ·æ¥å£ï¼ŒåŒ…å«å‚æ•°éªŒè¯"""

def estimate_moe_expert_load_impact(
    batch_size: int,
    chips: int
) -> Dict[str, float]:
    """è¿”å›è´Ÿè½½ç»Ÿè®¡ï¼ˆmax_experts, avg_experts, load_factorï¼‰"""
```

### 5.3 é›†æˆåˆ° MoE å±‚

**ä¿®æ”¹å‰**ï¼š

```python
# backend/llm_simulator/layers/moe.py (æ—§ä»£ç )
experts_per_ep = num_experts // ep  # 256 // 32 = 8
activated_tokens = tokens * num_activated // ep

routed_gate_op = MatMulOperator(
    ...,
    parallel_params={
        'G': experts_per_ep,      # å‡è®¾å‡åŒ€åˆ†å¸ƒ
        'M': activated_tokens,
        ...
    }
)
```

**ä¿®æ”¹å**ï¼š

```python
# backend/llm_simulator/layers/moe.py (æ–°ä»£ç )
from ..evaluators import get_max_expert_load_for_moe_layer

# æŸ¥è¡¨è·å–æœ€å¿™èŠ¯ç‰‡çš„ä¸“å®¶æ•°
max_experts_float = get_max_expert_load_for_moe_layer(
    batch_size=tokens,
    ep_parallelism=ep,
    num_experts=num_experts,
    topk=num_activated
)

# GEMM çš„ G ç»´åº¦å¿…é¡»æ˜¯æ•´æ•°ï¼Œå‘ä¸Šå–æ•´
max_experts_per_chip = math.ceil(max_experts_float)

# æ¯ä¸“å®¶å¹³å‡å¤„ç†çš„ tokens
tokens_per_expert = (tokens * num_activated) // num_experts

routed_gate_op = MatMulOperator(
    ...,
    parallel_params={
        'G': max_experts_per_chip,  # ä½¿ç”¨è´Ÿè½½å‡è¡¡åçš„å€¼
        'M': tokens_per_expert,     # è°ƒæ•´ M ç»´åº¦
        ...
    }
)
```

### 5.4 é€‚ç”¨èŒƒå›´

**âœ… é€‚ç”¨**ï¼š
- DeepSeek V3 / R1 (256 ä¸“å®¶ï¼ŒTop-8 è·¯ç”±)
- Decode é˜¶æ®µï¼ˆå° batchï¼Œè´Ÿè½½ä¸å‡ä¸¥é‡ï¼‰

**âŒ ä¸é€‚ç”¨**ï¼š
- Mixtral 8Ã—7B (8 ä¸“å®¶ï¼ŒTop-2) â†’ éœ€è¦é‡æ–°æ¨¡æ‹Ÿ
- å…¶ä»–ä¸“å®¶æ•°/TopK é…ç½® â†’ éœ€è¦é‡æ–°ç”Ÿæˆè¡¨

---

## 6. æ€§èƒ½ä¼˜åŒ–æœºåˆ¶

### 6.1 å¤šçº§ç¼“å­˜

**1. GEMM è¯„ä¼°å™¨å†…éƒ¨ç¼“å­˜**ï¼š
```python
# GEMMEvaluator._cache
cache_key = (G, M, K, N, input_dtype, output_dtype)
if cache_key in self._cache:
    return self._cache[cache_key]  # å‘½ä¸­
```

**2. å…¨å±€ç¼“å­˜ï¼ˆè·¨å®éªŒï¼‰**ï¼š
```python
# simulator.py
global_cache = {}  # è·¨å¤šä¸ª simulate è°ƒç”¨å¤ç”¨

analyzer = PerformanceAnalyzer(model, tpu, deploy_cfg, global_cache)
```

**3. GEMM é¢„çƒ­**ï¼š
```python
# backend/llm_simulator/gemm_prewarm.py
# ç¦»çº¿é¢„è°ƒä¼˜å¸¸è§ GEMM å½¢çŠ¶ï¼Œç”Ÿæˆç¼“å­˜æ–‡ä»¶
COMMON_SHAPES = [
    (1, 4096, 7168, 18432),   # MLP gate
    (1, 4096, 18432, 7168),   # MLP down
    # ...
]

prewarm_gemm(arch, COMMON_SHAPES, output_file='gemm_cache.json')
```

### 6.2 å¹¶è¡Œè¯„ä¼°

**å¤šè¿›ç¨‹ GEMM æœç´¢**ï¼š
```python
# gemm_eval.py
with Pool(processes=cpu_count()) as pool:
    results = pool.starmap(evaluate_partition, tasks)
```

**æ‰¹é‡ç®—å­è¯„ä¼°**ï¼š
```python
# å¯¹ç›¸åŒç±»å‹çš„ç®—å­æ‰¹é‡è¯„ä¼°ï¼Œå¤ç”¨ç¼“å­˜
for op_type, operators in grouped_operators.items():
    evaluator = get_evaluator(op_type)
    for op in operators:
        evaluator.evaluate(op)  # è‡ªåŠ¨å¤ç”¨ç¼“å­˜
```

### 6.3 ç¼“å­˜ç»Ÿè®¡

```python
# è·å–ç¼“å­˜ç»Ÿè®¡
gemm_eval = get_gemm_evaluator(arch)
stats = gemm_eval.get_cache_stats()

print(f"ç¼“å­˜å‘½ä¸­ç‡: {stats['hit_rate_percent']:.1f}%")
print(f"æ€»æœç´¢æ—¶é—´: {stats['total_search_time_ms']:.2f}ms")
```

---

## 7. ä¸ DS_TPU å¯¹é½æƒ…å†µ

### 7.1 å·²å¯¹é½åŠŸèƒ½

| åŠŸèƒ½æ¨¡å— | DS_TPU | Tier6 | å¯¹é½çŠ¶æ€ |
|---------|--------|-------|---------|
| **GEMM è¯„ä¼°** | âœ… | âœ… | âœ… å®Œå…¨å¯¹é½ |
| **FlashAttention** | âœ… | âœ… | âœ… å®Œå…¨å¯¹é½ |
| **AllReduce/AllGather** | âœ… | âœ… | âœ… å®Œå…¨å¯¹é½ |
| **MLA å˜ä½“** | âœ… 4 ç§ | âœ… 4 ç§ | âœ… å®Œå…¨å¯¹é½ |
| **MoE è´Ÿè½½å‡è¡¡** | âœ… | âœ… | âœ… **æ–°å¯¹é½** |
| **é€šä¿¡å»¶è¿Ÿé…ç½®** | âœ… | âœ… | âœ… **æ–°å¯¹é½** (CommLatencyConfig) |
| **é€šä¿¡åè®® 1** | âœ… | âœ… | âœ… å®Œå…¨å¯¹é½ |
| **é€šä¿¡åè®® 2/3** | âœ… | âš ï¸ | âš ï¸ éœ€éªŒè¯ |
| **MoE TBO é‡å ** | âœ… | âŒ | âš ï¸ æœªå®ç° |

### 7.2 è¯„ä¼°ç²¾åº¦å¯¹æ¯”

**GEMM æµ‹è¯•æ¡ˆä¾‹**ï¼š

| å½¢çŠ¶ (G, M, K, N) | DS_TPU (Î¼s) | Tier6 (Î¼s) | è¯¯å·® |
|------------------|-------------|------------|------|
| (1, 4096, 7168, 18432) | 125.3 | 125.3 | 0.0% |
| (4, 128, 7168, 2048) | 8.7 | 8.7 | 0.0% |
| (8, 64, 7168, 2048) | 6.2 | 6.2 | 0.0% |

**MoE è´Ÿè½½å‡è¡¡**ï¼š

| Batch | Chips | DS_TPU ä¸“å®¶æ•° | Tier6 ä¸“å®¶æ•° | è¯¯å·® |
|-------|-------|--------------|-------------|------|
| 4 | 32 | 3.18 | 3.18 | 0.0% |
| 64 | 32 | 8.00 | 8.00 | 0.0% |
| 256 | 256 | 1.00 | 1.00 | 0.0% |

### 7.3 å¾…å¯¹é½é¡¹

**1. MoE Dispatch/Combine é‡å **ï¼š
- DS_TPU æ˜¾å¼å»ºæ¨¡ TBO (Tensor-Bus Overlap)
- Tier6 ç®€åŒ–ä¸ºèŠ¯ç‰‡çº§é‡å 
- **å½±å“**ï¼šMoE å±‚å»¶è¿Ÿè¯¯å·® 10-20%

**2. é€šä¿¡åè®® 2/3 éªŒè¯**ï¼š
- éœ€è¦éªŒè¯ RTT å»¶è¿Ÿå‚æ•°æ˜¯å¦æ­£ç¡®
- éœ€è¦æµ‹è¯•æµæ°´çº¿æ¨¡å¼

---

## 8. é…ç½®å‚æ•°

### 8.1 æ¨¡å‹é…ç½®

```yaml
model:
  name: "DeepSeek-V3"
  hidden_dim: 7168
  num_layers: 61
  num_heads: 128

  # MLA å‚æ•°
  q_lora_rank: 1536
  kv_lora_rank: 512
  qk_nope_head_dim: 128
  qk_rope_head_dim: 64
  v_head_dim: 128

  # MoE å‚æ•°
  num_experts: 256
  num_activated_experts: 8
  num_shared_experts: 1
  expert_inter_dim: 2048
```

### 8.2 éƒ¨ç½²é…ç½®

```yaml
deployment:
  batch_size: 64
  q_seq_len: 4096        # Prefill: 4096, Decode: 1
  kv_seq_len: 4096

  # å¹¶è¡Œç­–ç•¥
  tp: 1                  # Tensor Parallelism
  dp: 32                 # Data Parallelism
  pp: 1                  # Pipeline Parallelism
  moe_tp: 1              # MoE Tensor Parallelism
  ep: 32                 # Expert Parallelism

  # é€šä¿¡é…ç½®
  comm_protocol: 1       # 1: post-write, 2: non-post-write, 3: pipeline
  enable_tp_sp: false    # TP Sequence Parallelism

  is_prefill: true       # true: Prefill, false: Decode
```

### 8.3 é€šä¿¡å»¶è¿Ÿé…ç½® (CommLatencyConfig)

**ç»Ÿä¸€é…ç½®æ¥å£**ï¼šå‰ç«¯ä½¿ç”¨å•ä¸€ `CommLatencyConfig` å¯¹è±¡é…ç½®æ‰€æœ‰é€šä¿¡å»¶è¿Ÿå‚æ•°ï¼Œé€šè¿‡ API ä¼ é€’ç»™åç«¯ã€‚

```typescript
// frontend/src/utils/llmDeployment/types.ts
interface CommLatencyConfig {
  // === åè®®ç›¸å…³ (Protocol) ===
  rtt_tp_us: number;              // TP å¾€è¿”å»¶è¿Ÿ (Î¼s)ï¼Œé»˜è®¤ 0.35
  rtt_ep_us: number;              // EP å¾€è¿”å»¶è¿Ÿ (Î¼s)ï¼Œé»˜è®¤ 0.85
  bandwidth_utilization: number;  // å¸¦å®½åˆ©ç”¨ç‡ (0-1)ï¼Œé»˜è®¤ 0.95
  sync_latency_us: number;        // åŒæ­¥å»¶è¿Ÿ (Î¼s)ï¼Œé»˜è®¤ 0.0

  // === ç½‘ç»œåŸºç¡€è®¾æ–½ (Network Infrastructure) ===
  switch_delay_us: number;        // äº¤æ¢æœºå»¶è¿Ÿ (Î¼s)ï¼Œé»˜è®¤ 1.0
  cable_delay_us: number;         // çº¿ç¼†å»¶è¿Ÿ (Î¼s)ï¼Œé»˜è®¤ 0.025

  // === èŠ¯ç‰‡å»¶è¿Ÿ (Chip Latency) ===
  chip_to_chip_us: number;        // èŠ¯ç‰‡é—´å»¶è¿Ÿ (Î¼s)ï¼Œé»˜è®¤ 0.2
  memory_read_latency_us: number; // å†…å­˜è¯»å»¶è¿Ÿ (Î¼s)ï¼Œé»˜è®¤ 0.15
  memory_write_latency_us: number;// å†…å­˜å†™å»¶è¿Ÿ (Î¼s)ï¼Œé»˜è®¤ 0.01
  noc_latency_us: number;         // NoC å»¶è¿Ÿ (Î¼s)ï¼Œé»˜è®¤ 0.05
  die_to_die_latency_us: number;  // Die é—´å»¶è¿Ÿ (Î¼s)ï¼Œé»˜è®¤ 0.04
}
```

**æ•°æ®æµ**ï¼š

```
å‰ç«¯ UI è¾“å…¥
    â†“
commLatencyConfig (React State)
    â†“
fullTopology.comm_latency_config (API Request)
    â†“
åç«¯ simulator.py æå–
    â†“
å†…éƒ¨è½¬æ¢ä¸º ProtocolConfig + NetworkInfraConfig
    â†“
é€šä¿¡è¯„ä¼°å™¨ (comm_eval.py) ä½¿ç”¨
```

**å¯åŠ¨å»¶è¿Ÿå…¬å¼**ï¼š

| é€šä¿¡ç±»å‹ | start_lat å…¬å¼ |
|---------|----------------|
| **AllReduce (TP)** | `2Ã—c2c + ddr_r + ddr_w + noc + 2Ã—d2d` |
| **Dispatch/Combine (EP)** | `2Ã—c2c + ddr_r + ddr_w + noc + 2Ã—d2d + 2Ã—switch + 2Ã—cable` |

å…¶ä¸­ï¼š
- `c2c` = `chip_to_chip_us`
- `ddr_r` = `memory_read_latency_us`
- `ddr_w` = `memory_write_latency_us`
- `noc` = `noc_latency_us`
- `d2d` = `die_to_die_latency_us`
- `switch` = `switch_delay_us`
- `cable` = `cable_delay_us`

**é»˜è®¤å€¼ (ä¸ DS_TPU å¯¹é½)**ï¼š

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `rtt_tp_us` | 0.35 | TP ç»„å†… RTT |
| `rtt_ep_us` | 0.85 | EP ç»„å†… RTT |
| `bandwidth_utilization` | 0.95 | å®é™…/ç†è®ºå¸¦å®½ |
| `switch_delay_us` | 1.0 | IB äº¤æ¢æœº |
| `cable_delay_us` | 0.025 | é“œç¼†/å…‰ç¼† |
| `chip_to_chip_us` | 0.2 | æ¿å†…èŠ¯ç‰‡äº’è” |
| `memory_read_latency_us` | 0.15 | HBM è¯»å»¶è¿Ÿ |
| `memory_write_latency_us` | 0.01 | HBM å†™å»¶è¿Ÿ |
| `noc_latency_us` | 0.05 | ç‰‡å†… NoC |
| `die_to_die_latency_us` | 0.04 | å¤š Die å°è£… |

---

### 8.4 ç¡¬ä»¶é…ç½®

```python
# SG2260E é…ç½® (é»˜è®¤)
SG2260E_ARCH = AcceleratorMicroArch(
    num_cores=8,
    cube_m=32,
    cube_n=32,
    cube_k=64,
    macs_per_cycle=32 * 32,
    freq_ghz=1.6,
    sram_size_kb=2048,
    effective_sram_bytes=int(2048 * 1024 * 0.45),
    dma_bandwidth_per_core=273e9,  # 273 GB/s (HBM3)
    lane_num=32,
    align_bytes=128,
    compute_dma_overlap_rate=0.7,
)
```

---

## 9. ä½¿ç”¨ç¤ºä¾‹

### 9.1 åŸºæœ¬ä½¿ç”¨

```python
from llm_simulator import LLMInferenceSimulator

# åˆ›å»ºæ¨¡æ‹Ÿå™¨
simulator = LLMInferenceSimulator(arch_preset='sg2260e')

# é…ç½®
config = {
    'model': {
        'name': 'DeepSeek-V3',
        'hidden_dim': 7168,
        'num_layers': 61,
        # ...
    },
    'deployment': {
        'batch_size': 4,
        'q_seq_len': 1,      # Decode
        'tp': 1,
        'ep': 32,
        'is_prefill': False,
    }
}

# è¿è¡Œæ¨¡æ‹Ÿ
result = simulator.simulate(config)

print(f"æ€»å»¶è¿Ÿ: {result['total_latency_us'] / 1000:.2f} ms")
print(f"MFU: {result['mfu'] * 100:.1f}%")
print(f"TPOT: {result['tpot_us']:.2f} Î¼s")
```

### 9.2 MoE è´Ÿè½½å‡è¡¡æŸ¥è¯¢

```python
from llm_simulator.evaluators import (
    get_max_expert_load,
    estimate_moe_expert_load_impact
)

# æŸ¥è¯¢æœ€å¿™èŠ¯ç‰‡çš„ä¸“å®¶æ•°
max_experts = get_max_expert_load(batch_size=4, chips=32)
print(f"æœ€å¿™èŠ¯ç‰‡åŠ è½½: {max_experts:.2f} ä¸ªä¸“å®¶")  # 3.18

# è·å–è¯¦ç»†ç»Ÿè®¡
impact = estimate_moe_expert_load_impact(batch_size=4, chips=32)
print(f"è´Ÿè½½å› å­: {impact['load_factor']:.2f}x")  # 3.18x
```

### 9.3 GEMM é¢„çƒ­

```bash
# é¢„è°ƒä¼˜å¸¸è§ GEMM å½¢çŠ¶
python -m llm_simulator.gemm_prewarm \
    --arch sg2260e \
    --output cache/gemm_sg2260e.json

# ä½¿ç”¨é¢„çƒ­ç¼“å­˜
simulator = LLMInferenceSimulator(
    arch_preset='sg2260e',
    gemm_cache_file='cache/gemm_sg2260e.json'
)
```

### 9.4 æ€§èƒ½åˆ†æ

```python
# è·å–å±‚çº§æ€§èƒ½åˆ†è§£
for layer in result['layers']:
    print(f"\nLayer {layer['name']}:")
    print(f"  å»¶è¿Ÿ: {layer['latency_us']:.2f} Î¼s")
    print(f"  è®¡ç®—: {layer['compute_us']:.2f} Î¼s")
    print(f"  é€šä¿¡: {layer['comm_us']:.2f} Î¼s")
    print(f"  æµé‡: {layer['dram_traffic_gb']:.2f} GB")
```

---

## 10. é™„å½•

### 10.1 æœ¯è¯­è¡¨

| æœ¯è¯­ | å…¨ç§° | è¯´æ˜ |
|------|------|------|
| **MFU** | Model FLOPs Utilization | æ¨¡å‹ FLOPs åˆ©ç”¨ç‡ |
| **MBU** | Model Bandwidth Utilization | æ¨¡å‹å¸¦å®½åˆ©ç”¨ç‡ |
| **TTFT** | Time To First Token | é¦– token å»¶è¿Ÿ (Prefill) |
| **TPOT** | Time Per Output Token | å• token å»¶è¿Ÿ (Decode) |
| **TP** | Tensor Parallelism | å¼ é‡å¹¶è¡Œ |
| **PP** | Pipeline Parallelism | æµæ°´çº¿å¹¶è¡Œ |
| **DP** | Data Parallelism | æ•°æ®å¹¶è¡Œ |
| **EP** | Expert Parallelism | ä¸“å®¶å¹¶è¡Œ |
| **TP-SP** | TP Sequence Parallelism | åºåˆ—å¹¶è¡Œ |
| **MLA** | Multi-head Latent Attention | å¤šå¤´æ½œåœ¨æ³¨æ„åŠ› |
| **MoE** | Mixture of Experts | æ··åˆä¸“å®¶ |
| **TBO** | Tensor-Bus Overlap | å¼ é‡æ€»çº¿é‡å  |

### 10.2 æ€§èƒ½æŒ‡æ ‡å…¬å¼

**MFU (Model FLOPs Utilization)**ï¼š
```python
MFU = total_flops / (peak_flops Ã— total_time_s)
    = total_flops / (num_cores Ã— macs_per_cycle Ã— freq_ghz Ã— 2 Ã— total_time_s Ã— 1e9)
```

**MBU (Model Bandwidth Utilization)**ï¼š
```python
MBU = total_dram_traffic / (dram_bandwidth Ã— total_time_s)
```

**TTFT (Time To First Token)**ï¼š
```python
TTFT = prefill_latency_ms
```

**TPOT (Time Per Output Token)**ï¼š
```python
TPOT = decode_latency_us / batch_size
```

### 10.3 æ–‡ä»¶ç»“æ„

```
backend/llm_simulator/
â”œâ”€â”€ evaluators/
â”‚   â”œâ”€â”€ gemm_eval.py              # GEMM è¯„ä¼°å™¨
â”‚   â”œâ”€â”€ fa2_eval.py               # FlashAttention è¯„ä¼°å™¨
â”‚   â”œâ”€â”€ comm_eval.py              # é€šä¿¡è¯„ä¼°å™¨
â”‚   â”œâ”€â”€ moe_load_balance.py       # MoE è´Ÿè½½å‡è¡¡ (æ–°)
â”‚   â”œâ”€â”€ arch_config.py            # ç¡¬ä»¶å¾®æ¶æ„é…ç½®
â”‚   â””â”€â”€ presets.py                # é¢„è®¾ç¡¬ä»¶é…ç½®
â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ base.py                   # å±‚åŸºç±»
â”‚   â”œâ”€â”€ embedding.py              # Embedding å±‚
â”‚   â”œâ”€â”€ attention.py              # MLA/MHA å±‚
â”‚   â”œâ”€â”€ moe.py                    # MoE å±‚ (å·²ä¿®æ”¹)
â”‚   â”œâ”€â”€ ffn.py                    # MLP å±‚
â”‚   â””â”€â”€ lmhead.py                 # LMHead å±‚
â”œâ”€â”€ operators/
â”‚   â”œâ”€â”€ base.py                   # ç®—å­åŸºç±»
â”‚   â”œâ”€â”€ matmul.py                 # MatMul ç®—å­
â”‚   â”œâ”€â”€ attention_ops.py          # Attention ç®—å­
â”‚   â””â”€â”€ comm_ops.py               # é€šä¿¡ç®—å­
â”œâ”€â”€ simulator.py                  # ä¸»æ¨¡æ‹Ÿå™¨ (å·²ä¿®æ”¹: æ¥æ”¶ç»Ÿä¸€ CommLatencyConfig)
â”œâ”€â”€ gemm_prewarm.py               # GEMM é¢„çƒ­å·¥å…·
â””â”€â”€ types.py                      # ç±»å‹å®šä¹‰

backend/tests/
â”œâ”€â”€ test_moe_load_balance.py      # MoE è´Ÿè½½å‡è¡¡éªŒè¯ (æ–°)
â”œâ”€â”€ test_moe_integration.py       # MoE é›†æˆæµ‹è¯• (æ–°)
â””â”€â”€ test_debug_features.py        # è°ƒè¯•åŠŸèƒ½æµ‹è¯•

frontend/src/
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ llmDeployment/
â”‚   â”‚   â””â”€â”€ types.ts              # ç±»å‹å®šä¹‰ (å·²ä¿®æ”¹: ç»Ÿä¸€ CommLatencyConfig)
â”‚   â””â”€â”€ storage.ts                # å­˜å‚¨æ¨¡å— (å·²ä¿®æ”¹: SavedConfig.comm_latency_config)
â””â”€â”€ components/ConfigPanel/DeploymentAnalysis/
    â”œâ”€â”€ DeploymentAnalysisPanel.tsx         # éƒ¨ç½²åˆ†æé¢æ¿ (å·²ä¿®æ”¹: ç»Ÿä¸€é…ç½®çŠ¶æ€)
    â””â”€â”€ components/
        â””â”€â”€ ConfigSnapshotDisplay.tsx       # é…ç½®å¿«ç…§å±•ç¤º (å·²ä¿®æ”¹: ç»Ÿä¸€æ˜¾ç¤ºé¢æ¿)

docs/
â”œâ”€â”€ TIER6_EVALUATION_SPEC.md      # æœ¬æ–‡æ¡£
â””â”€â”€ DS_TPU_Performance_Analysis.md # DS_TPU åˆ†ææ–‡æ¡£
```

### 10.4 ç›¸å…³èµ„æº

**ä»£ç ä»“åº“**ï¼š
- [Tier6-Model](https://github.com/your-org/tier6-model)
- [DS_TPU_1209](c:\Users\DELL\Documents\code\DS_TPU_1209)

**è®ºæ–‡å‚è€ƒ**ï¼š
- [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)
- [FlashAttention-2](https://arxiv.org/abs/2307.08691)
- [MoE Load Balancing](https://arxiv.org/abs/2408.15664)

---

**æœ€åæ›´æ–°**: 2026-01-26
**ç»´æŠ¤è€…**: Tier6-Model Team
**è®¸å¯**: MIT License
